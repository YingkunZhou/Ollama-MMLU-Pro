{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10009b7d-823c-46ae-8355-4e580c686bf3",
      "metadata": {
        "id": "10009b7d-823c-46ae-8355-4e580c686bf3"
      },
      "source": [
        "```c++\n",
        "common_params params;\n",
        "--> common_params_parse;\n",
        "common_init();\n",
        "llama_backend_init();\n",
        "llama_numa_init(params.numa);\n",
        "// load the model and apply lora adapter, if any\n",
        "--> common_init_from_params;\n",
        "llama_model * model = llama_init.model.get();\n",
        "llama_context * ctx = llama_init.context.get();\n",
        "struct results_perplexity results;\n",
        "--> perplexity;\n",
        "llama_perf_context_print(ctx);\n",
        "--> llama_backend_free --> ggml_quantize_free;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5019f8f4-e92b-49a1-97c0-7da9b098ca40",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "5019f8f4-e92b-49a1-97c0-7da9b098ca40"
      },
      "source": [
        "# [`common_params_parse`](https://github.com/ggml-org/llama.cpp/blob/master/common/arg.cpp#L1183)\n",
        "\n",
        "```c++\n",
        "common_params_parse(argc, argv, params, LLAMA_EXAMPLE_PERPLEXITY);\n",
        "----------\n",
        "bool common_params_parse(int argc, char ** argv, common_params & params, llama_example ex, void(*print_usage)(int, char **)) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac48b2ed-fc26-4326-b2a8-aaecfa12d61a",
      "metadata": {
        "id": "ac48b2ed-fc26-4326-b2a8-aaecfa12d61a"
      },
      "source": [
        "# [`common_init_from_params`](https://github.com/ggml-org/llama.cpp/blob/master/common/common.cpp#L888)\n",
        "\n",
        "```c++\n",
        "common_init_result llama_init = common_init_from_params(params);\n",
        "----------\n",
        "struct common_init_result common_init_from_params(common_params & params) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0cbab3d-c6dc-4363-b124-d687c8808a0a",
      "metadata": {
        "id": "d0cbab3d-c6dc-4363-b124-d687c8808a0a"
      },
      "source": [
        "<details>\n",
        "<summary>struct common_init_result</summary>\n",
        "\n",
        "```c++\n",
        "// note: defines object's lifetime\n",
        "struct common_init_result {\n",
        "    llama_model_ptr   model;\n",
        "    llama_context_ptr context;\n",
        "\n",
        "    std::vector<llama_adapter_lora_ptr> lora;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "common_init_result iparams;\n",
        "\n",
        "auto mparams = common_model_params_to_llama(params);\n",
        "--> llama_model_load_from_file --> llama_model_load_from_file_impl --> llama_model_load --> llama_model * model;\n",
        "auto cparams = common_context_params_to_llama(params);\n",
        "--> llama_init_from_model --> llama_context::llama_context --> llama_context * lctx;\n",
        "llama_set_warmup(lctx, warmup=true); --> cparams.warmup = warmup;\n",
        "std::vector<llama_token> tmp;\n",
        "llama_token bos = llama_vocab_bos(vocab);\n",
        "llama_token eos = llama_vocab_eos(vocab);\n",
        "tmp.push_back(bos);\n",
        "tmp.push_back(eos);\n",
        "--> llama_decode;\n",
        "llama_memory_clear(llama_get_memory(lctx), true);\n",
        "// add the evaluation to the stats\n",
        "llama_synchronize(lctx); --> lctx->synchronize; --> llama_context::synchronize; --> ggml_backend_sched_synchronize;\n",
        "llama_perf_context_reset(lctx);\n",
        "llama_set_warmup(lctx, false);\n",
        "\n",
        "iparams.model.reset(model);\n",
        "iparams.context.reset(lctx);\n",
        "return iparams;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b1fd37-e078-4d43-bbe4-ef923887305b",
      "metadata": {
        "id": "f8b1fd37-e078-4d43-bbe4-ef923887305b"
      },
      "source": [
        "## [`llama_model_load`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama.cpp#L87)\n",
        "\n",
        "```c++\n",
        "llama_model * model = llama_model_load_from_file(params.model.path.c_str(), mparams);\n",
        "----------\n",
        "struct llama_model * llama_model_load_from_file(\n",
        "    const char * path_model,\n",
        "    struct llama_model_params params) {\n",
        "    std::vector<std::string> splits = {};\n",
        "    --> return llama_model_load_from_file_impl(path_model, splits, params);\n",
        "}\n",
        "\n",
        "static struct llama_model * llama_model_load_from_file_impl(\n",
        "        const std::string & path_model,\n",
        "        std::vector<std::string> & splits,\n",
        "        struct llama_model_params params) {\n",
        "    llama_model * model = new llama_model(params);\n",
        "    --> const int status = llama_model_load(path_model, splits, *model, params);\n",
        "    return model;\n",
        "}\n",
        "\n",
        "// Returns 0 on success, -1 on error, and -2 on cancellation via llama_progress_callback\n",
        "static int llama_model_load(const std::string & fname, std::vector<std::string> & splits, llama_model & model, llama_model_params & params) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9d2a10-42f8-4e7d-b13b-70e2ada15840",
      "metadata": {
        "id": "ed9d2a10-42f8-4e7d-b13b-70e2ada15840"
      },
      "source": [
        "<details>\n",
        "<summary>struct llama_model</summary>\n",
        "\n",
        "```c++\n",
        "struct llama_model {\n",
        "    llm_type type = LLM_TYPE_UNKNOWN;\n",
        "    llm_arch arch = LLM_ARCH_UNKNOWN;\n",
        "\n",
        "    std::string name = \"n/a\";\n",
        "\n",
        "    llama_hparams hparams = {};\n",
        "    llama_vocab   vocab;\n",
        "\n",
        "    struct ggml_tensor * tok_embd   = nullptr;\n",
        "    struct ggml_tensor * type_embd  = nullptr;\n",
        "    struct ggml_tensor * pos_embd   = nullptr;\n",
        "    struct ggml_tensor * tok_norm   = nullptr;\n",
        "    struct ggml_tensor * tok_norm_b = nullptr;\n",
        "\n",
        "    struct ggml_tensor * output_norm     = nullptr;\n",
        "    struct ggml_tensor * output_norm_b   = nullptr;\n",
        "    struct ggml_tensor * output          = nullptr;\n",
        "    struct ggml_tensor * output_b        = nullptr;\n",
        "    struct ggml_tensor * output_norm_enc = nullptr;\n",
        "\n",
        "    std::vector<llama_layer> layers;\n",
        "\n",
        "    llama_model_params params;\n",
        "\n",
        "    // gguf metadata\n",
        "    std::unordered_map<std::string, std::string> gguf_kv;\n",
        "\n",
        "    // list of devices used in this model\n",
        "    std::vector<ggml_backend_dev_t> devices;\n",
        "\n",
        "    // for quantize-stats only\n",
        "    std::vector<std::pair<std::string, struct ggml_tensor *>> tensors_by_name;\n",
        "\n",
        "    // note: can mutate `cparams`\n",
        "    // TODO: move this to new llm_arch_model_i interface\n",
        "    llama_memory_i * create_memory(const llama_memory_params & params, llama_cparams & cparams) const;\n",
        "\n",
        "    // TODO: move this to new llm_arch_model_i interface\n",
        "    llm_graph_result_ptr build_graph(\n",
        "            const llm_graph_params & params,\n",
        "                       ggml_cgraph * gf,\n",
        "                    llm_graph_type   type) const;\n",
        "\n",
        "private:\n",
        "    struct impl;\n",
        "    std::unique_ptr<impl> pimpl;\n",
        "};\n",
        "\n",
        "```\n",
        "    \n",
        "</details>\n",
        "\n",
        "```c++\n",
        "--> llama_model_loader::llama_model_loader --> llama_model_loader ml;\n",
        "ml.print_info();\n",
        "model.load_arch(ml);\n",
        "--> llama_model::load_hparams;\n",
        "--> llama_model::load_vocab --> llama_vocab::load --> llama_vocab::impl::load;\n",
        "model.load_stats(ml);\n",
        "model.print_info();\n",
        "--> llama_model::load_tensors;\n",
        "return 0;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23b35917-d3da-45b2-996e-cf9b9e26cfb9",
      "metadata": {
        "id": "23b35917-d3da-45b2-996e-cf9b9e26cfb9"
      },
      "source": [
        "### [`llama_model_loader::llama_model_loader`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#468)\n",
        "\n",
        "```c++\n",
        "llama_model_loader ml(fname, splits, params.use_mmap, params.check_tensors, params.kv_overrides, params.tensor_buft_overrides);\n",
        "----------\n",
        "llama_model_loader::llama_model_loader(\n",
        "        const std::string & fname,\n",
        "        std::vector<std::string> & splits,\n",
        "        bool use_mmap,\n",
        "        bool check_tensors,\n",
        "        const llama_model_kv_override * param_overrides_p,\n",
        "        const llama_model_tensor_buft_override * param_tensor_buft_overrides_p) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45bf2a36-2b0c-4ce8-a29a-11121839b801",
      "metadata": {
        "id": "45bf2a36-2b0c-4ce8-a29a-11121839b801"
      },
      "source": [
        "```c++\n",
        "// Load the main GGUF\n",
        "struct ggml_context * ctx = NULL;\n",
        "struct gguf_init_params params = {\n",
        "    /*.no_alloc = */ true,\n",
        "    /*.ctx      = */ &ctx,\n",
        "};\n",
        "\n",
        "llama_model_loader:: gguf_context_ptr meta;\n",
        "--> gguf_init_from_file --> gguf_init_from_file_impl --> meta;\n",
        "\n",
        "files.emplace_back(new llama_file(fname.c_str(), \"rb\"));\n",
        "contexts.emplace_back(ctx);\n",
        "\n",
        "for (ggml_tensor * cur = ggml_get_first_tensor(ctx); cur; cur = ggml_get_next_tensor(ctx, cur)) {\n",
        "    std::string tensor_name = std::string(cur->name);\n",
        "    n_elements += ggml_nelements(cur);\n",
        "    n_bytes    += ggml_nbytes(cur);\n",
        "    weights_map.emplace(tensor_name, llama_tensor_weight(files.back().get(), 0, meta.get(), cur));\n",
        "}\n",
        "\n",
        "n_kv      = gguf_get_n_kv(meta.get());\n",
        "n_tensors = weights_map.size();\n",
        "\n",
        "fver = (enum llama_fver) gguf_get_version(meta.get());\n",
        "\n",
        "this->use_mmap = use_mmap;\n",
        "this->check_tensors = check_tensors;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad88911-3e43-461f-bcb1-8599dccb1fe6",
      "metadata": {
        "id": "9ad88911-3e43-461f-bcb1-8599dccb1fe6"
      },
      "source": [
        "#### [`gguf_init_from_file_impl`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/gguf.cpp#319)\n",
        "\n",
        "```c++\n",
        "meta.reset(gguf_init_from_file(fname.c_str(), params));\n",
        "\n",
        "struct gguf_context * gguf_init_from_file(const char * fname, struct gguf_init_params params) {\n",
        "    FILE * file = ggml_fopen(fname, \"rb\");\n",
        "    --> struct gguf_context * result = gguf_init_from_file_impl(file, params);\n",
        "    fclose(file);\n",
        "    return result;\n",
        "}\n",
        "\n",
        "struct gguf_context * gguf_init_from_file_impl(FILE * file, struct gguf_init_params params) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ed93c7-14ac-4787-82b3-d6f8c7ff1fc2",
      "metadata": {
        "id": "00ed93c7-14ac-4787-82b3-d6f8c7ff1fc2"
      },
      "source": [
        "<details>\n",
        "<summary>struct gguf_context</summary>\n",
        "\n",
        "```c++\n",
        "struct gguf_context {\n",
        "    uint32_t version = GGUF_VERSION;\n",
        "\n",
        "    std::vector<struct gguf_kv> kv;\n",
        "    std::vector<struct gguf_tensor_info> info;\n",
        "\n",
        "    size_t alignment = GGUF_DEFAULT_ALIGNMENT;\n",
        "    size_t offset    = 0; // offset of `data` from beginning of file\n",
        "    size_t size      = 0; // size of `data` in bytes\n",
        "\n",
        "    void * data = nullptr;\n",
        "};\n",
        "```\n",
        "    \n",
        "</details>\n",
        "\n",
        "```c++\n",
        "const struct gguf_reader gr(file);\n",
        "struct gguf_context * ctx = new gguf_context;\n",
        "// file magic\n",
        "gr.read(magic, 4);\n",
        "// header\n",
        "gr.read(ctx->version);\n",
        "gr.read(n_tensors);\n",
        "gr.read(n_kv);\n",
        "// KV pairs\n",
        "for (int64_t i = 0; ok && i < n_kv; ++i)\n",
        "    gr.read(key);\n",
        "    gr.read(type);\n",
        "    is_array = true; gr.read(type); gr.read(n);\n",
        "    gguf_read_emplace_helper<xxx>    (gr, ctx->kv, key, is_array, n);\n",
        "const int alignment_idx = gguf_find_key(ctx, GGUF_KEY_GENERAL_ALIGNMENT);\n",
        "ctx->alignment = alignment_idx == -1 ? GGUF_DEFAULT_ALIGNMENT : gguf_get_val_u32(ctx, alignment_idx);\n",
        "\n",
        "// read the tensor info\n",
        "for (int64_t i = 0; ok && i < n_tensors; ++i)\n",
        "    struct gguf_tensor_info info;\n",
        "    std::string name; gr.read(name); ggml_set_name(&info.t, name.c_str()); // tensor name\n",
        "    uint32_t n_dims = -1; gr.read(n_dims); gr.read(info.t.ne); // tensor shape\n",
        "    gr.read(info.t.type); // tensor type\n",
        "    // calculate byte offsets given the tensor shape and type\n",
        "    const size_t  type_size = ggml_type_size(info.t.type);\n",
        "    const int64_t blck_size = ggml_blck_size(info.t.type);\n",
        "    info.t.nb;\n",
        "    gr.read(info.offset); // tensor data offset within buffer\n",
        "    ctx->info.push_back(info);\n",
        "\n",
        "// store the current file offset - this is where the data section starts\n",
        "ctx->offset = ftell(file);\n",
        "\n",
        "// compute the total size of the data section, taking into account the alignment\n",
        "ctx->size = 0;\n",
        "for (size_t i = 0; i < ctx->info.size(); ++i)\n",
        "    const gguf_tensor_info & ti = ctx->info[i];\n",
        "    ctx->size += GGML_PAD(ggml_nbytes(&ti.t), ctx->alignment);\n",
        "\n",
        "\n",
        "// load the tensor data only if requested\n",
        "// compute the exact size needed for the new ggml_context\n",
        "const size_t mem_size = n_tensors * ggml_tensor_overhead();\n",
        "\n",
        "struct ggml_init_params pdata = {\n",
        "    /*mem_size   =*/ mem_size,\n",
        "    /*mem_buffer =*/ nullptr,\n",
        "    /*no_alloc   =*/ params.no_alloc,\n",
        "};\n",
        "--> ggml_init --> *params.ctx;\n",
        "struct ggml_context * ctx_data = *params.ctx;\n",
        "\n",
        "// create the tensors\n",
        "for (size_t i = 0; i < ctx->info.size(); ++i) {\n",
        "    const struct gguf_tensor_info & info = ctx->info[i];\n",
        "    --> ggml_new_tensor --> ggml_new_tensor_impl --> struct ggml_tensor * cur;\n",
        "    ggml_set_name(cur, info.t.name);\n",
        "}\n",
        "return ctx;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "186d1c0b-b609-4be9-8132-b8a912a71e7f",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "186d1c0b-b609-4be9-8132-b8a912a71e7f"
      },
      "source": [
        "##### [`ggml_init`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1420)\n",
        "\n",
        "```c++\n",
        "*params.ctx = ggml_init(pdata);\n",
        "\n",
        "struct ggml_context * ggml_init(struct ggml_init_params params) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dddffe5c-6aea-4666-b676-87a6dd835057",
      "metadata": {
        "id": "dddffe5c-6aea-4666-b676-87a6dd835057"
      },
      "source": [
        "<details>\n",
        "<summary>struct ggml_context</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_context {\n",
        "    size_t mem_size;\n",
        "    void * mem_buffer;\n",
        "    bool   mem_buffer_owned;\n",
        "    bool   no_alloc;\n",
        "\n",
        "    int    n_objects;\n",
        "\n",
        "    struct ggml_object * objects_begin;\n",
        "    struct ggml_object * objects_end;\n",
        "};\n",
        "\n",
        "size_t ggml_tensor_overhead(void) {\n",
        "    return GGML_OBJECT_SIZE + GGML_TENSOR_SIZE;\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "```c++\n",
        "struct ggml_context * ctx = GGML_MALLOC(sizeof(struct ggml_context));\n",
        "\n",
        "// allow to call ggml_init with 0 size\n",
        "if (params.mem_size == 0) {\n",
        "    params.mem_size = GGML_MEM_ALIGN;\n",
        "}\n",
        "\n",
        "const size_t mem_size = params.mem_buffer ? params.mem_size : GGML_PAD(params.mem_size, GGML_MEM_ALIGN);\n",
        "\n",
        "*ctx = (struct ggml_context) {\n",
        "    /*.mem_size           =*/ mem_size,\n",
        "    /*.mem_buffer         =*/ params.mem_buffer ? params.mem_buffer : ggml_aligned_malloc(mem_size),\n",
        "    /*.mem_buffer_owned   =*/ params.mem_buffer ? false : true,\n",
        "    /*.no_alloc           =*/ params.no_alloc,\n",
        "    /*.n_objects          =*/ 0,\n",
        "    /*.objects_begin      =*/ NULL,\n",
        "    /*.objects_end        =*/ NULL,\n",
        "};\n",
        "\n",
        "return ctx;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6458a4-f1d0-4a4f-8ef3-e8ba3d78e344",
      "metadata": {
        "id": "7f6458a4-f1d0-4a4f-8ef3-e8ba3d78e344"
      },
      "source": [
        "##### [`ggml_new_tensor_impl`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1647)\n",
        "\n",
        "```c++\n",
        "struct ggml_tensor * cur = ggml_new_tensor(ctx_data, info.t.type, GGML_MAX_DIMS, info.t.ne);\n",
        "\n",
        "struct ggml_tensor * ggml_new_tensor(\n",
        "        struct ggml_context * ctx,\n",
        "        enum   ggml_type      type,\n",
        "        int                   n_dims,\n",
        "        const int64_t       * ne) {\n",
        "    --> return ggml_new_tensor_impl(ctx, type, n_dims, ne, NULL, 0);\n",
        "}\n",
        "\n",
        "static struct ggml_tensor * ggml_new_tensor_impl(\n",
        "        struct ggml_context * ctx,\n",
        "        enum   ggml_type      type,\n",
        "        int                   n_dims,\n",
        "        const int64_t       * ne,\n",
        "        struct ggml_tensor  * view_src,\n",
        "        size_t                view_offs) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "867d12c4-9416-4598-8809-05b5ee817c3b",
      "metadata": {
        "id": "867d12c4-9416-4598-8809-05b5ee817c3b"
      },
      "source": [
        "<details>\n",
        "<summary>struct ggml_tensor</summary>\n",
        "\n",
        "```c++\n",
        "// n-dimensional tensor\n",
        "struct ggml_tensor {\n",
        "    enum ggml_type type;\n",
        "\n",
        "    struct ggml_backend_buffer * buffer;\n",
        "\n",
        "    int64_t ne[GGML_MAX_DIMS]; // number of elements\n",
        "    size_t  nb[GGML_MAX_DIMS]; // stride in bytes:\n",
        "                               // nb[0] = ggml_type_size(type)\n",
        "                               // nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding\n",
        "                               // nb[i] = nb[i-1] * ne[i-1]\n",
        "\n",
        "    // compute data\n",
        "    enum ggml_op op;\n",
        "\n",
        "    // op params - allocated as int32_t for alignment\n",
        "    int32_t op_params[GGML_MAX_OP_PARAMS / sizeof(int32_t)];\n",
        "\n",
        "    int32_t flags;\n",
        "\n",
        "    struct ggml_tensor * src[GGML_MAX_SRC];\n",
        "\n",
        "    // source tensor and offset for views\n",
        "    struct ggml_tensor * view_src;\n",
        "    size_t               view_offs;\n",
        "\n",
        "    void * data;\n",
        "\n",
        "    char name[GGML_MAX_NAME];\n",
        "\n",
        "    void * extra; // extra things e.g. for ggml-cuda.cu\n",
        "\n",
        "    char padding[8];\n",
        "};\n",
        "\n",
        "static const size_t GGML_TENSOR_SIZE = sizeof(struct ggml_tensor);\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "```c++\n",
        "--> ggml_new_object --> obj_new;\n",
        "struct ggml_tensor * const result = (struct ggml_tensor *)((char *)ctx->mem_buffer + obj_new->offs);\n",
        "*result = (struct ggml_tensor) {\n",
        "    /*.type         =*/ type,\n",
        "    /*.buffer       =*/ NULL,\n",
        "    /*.ne           =*/ { 1, 1, 1, 1 },\n",
        "    /*.nb           =*/ { 0, 0, 0, 0 },\n",
        "    /*.op           =*/ GGML_OP_NONE,\n",
        "    /*.op_params    =*/ { 0 },\n",
        "    /*.flags        =*/ 0,\n",
        "    /*.src          =*/ { NULL },\n",
        "    /*.view_src     =*/ view_src,\n",
        "    /*.view_offs    =*/ view_offs,\n",
        "    /*.data         =*/ view_src != NULL ? view_src->data + view_offs : NULL,\n",
        "    /*.name         =*/ { 0 },\n",
        "    /*.extra        =*/ NULL,\n",
        "    /*.padding      =*/ { 0 },\n",
        "};\n",
        "\n",
        "result->ne;\n",
        "result->nb;\n",
        "ctx->n_objects++;\n",
        "return result;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b57f77f5-8623-49cc-8aa9-d6cc788469af",
      "metadata": {
        "id": "b57f77f5-8623-49cc-8aa9-d6cc788469af"
      },
      "source": [
        "[`ggml_new_object`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1525)\n",
        "\n",
        "```c++\n",
        "struct ggml_object * const obj_new = ggml_new_object(ctx, GGML_OBJECT_TYPE_TENSOR, GGML_TENSOR_SIZE);\n",
        "\n",
        "static struct ggml_object * ggml_new_object(struct ggml_context * ctx, enum ggml_object_type type, size_t size) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a747dcb-8a63-4695-8d73-68a3009ab9d1",
      "metadata": {
        "id": "2a747dcb-8a63-4695-8d73-68a3009ab9d1"
      },
      "source": [
        "<details>\n",
        "<summary>struct ggml_object</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_object {\n",
        "    size_t offs;\n",
        "    size_t size;\n",
        "\n",
        "    struct ggml_object * next;\n",
        "\n",
        "    enum ggml_object_type type;\n",
        "\n",
        "    char padding[4];\n",
        "};\n",
        "\n",
        "static const size_t GGML_OBJECT_SIZE = sizeof(struct ggml_object);\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "```c++\n",
        "// always insert objects at the end of the context's memory pool\n",
        "struct ggml_object * obj_cur = ctx->objects_end;\n",
        "\n",
        "const size_t cur_offs = obj_cur == NULL ? 0 : obj_cur->offs;\n",
        "const size_t cur_size = obj_cur == NULL ? 0 : obj_cur->size;\n",
        "const size_t cur_end  = cur_offs + cur_size;\n",
        "\n",
        "// align to GGML_MEM_ALIGN\n",
        "size_t size_needed = GGML_PAD(size, GGML_MEM_ALIGN);\n",
        "\n",
        "char * const mem_buffer = ctx->mem_buffer;\n",
        "struct ggml_object * const obj_new = (struct ggml_object *)(mem_buffer + cur_end);\n",
        "\n",
        "*obj_new = (struct ggml_object) {\n",
        "    .offs = cur_end + GGML_OBJECT_SIZE,\n",
        "    .size = size_needed,\n",
        "    .next = NULL,\n",
        "    .type = type,\n",
        "};\n",
        "\n",
        "GGML_ASSERT_ALIGNED(mem_buffer + obj_new->offs);\n",
        "\n",
        "if (obj_cur != NULL) {\n",
        "    obj_cur->next = obj_new;\n",
        "} else {\n",
        "    // this is the first object in this context\n",
        "    ctx->objects_begin = obj_new;\n",
        "}\n",
        "\n",
        "ctx->objects_end = obj_new;\n",
        "return obj_new;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4cdb26-bd29-4915-96b6-9573765ce679",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "7d4cdb26-bd29-4915-96b6-9573765ce679"
      },
      "source": [
        "### [`llama_model::load_hparams`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#423)\n",
        "\n",
        "```c++\n",
        "model.load_hparams(ml);\n",
        "----------\n",
        "void llama_model::load_hparams(llama_model_loader & ml) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0018b980-85ef-4e0f-a684-e88a10f1ae01",
      "metadata": {
        "id": "0018b980-85ef-4e0f-a684-e88a10f1ae01"
      },
      "source": [
        "```c++\n",
        "const gguf_context * ctx = ml.meta.get();\n",
        "// get metadata as string\n",
        "// gguf metadata\n",
        "llama_model:: std::unordered_map<std::string, std::string> gguf_kv;\n",
        "gguf_kv.emplace(name, value);\n",
        "hparams.xxx = xxx; // via ml.get_key\n",
        "pimpl->n_bytes = ml.n_bytes;\n",
        "pimpl->desc_str = arch_name() + \" \" + type_name() + \" \" + ml.ftype_name();\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1729941-5b75-45f6-9598-a0800fb9e442",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "f1729941-5b75-45f6-9598-a0800fb9e442"
      },
      "source": [
        "### [`llama_vocab::impl::load`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-vocab.cpp#1372)\n",
        "\n",
        "```c++\n",
        "model.load_vocab(ml);\n",
        "----------\n",
        "void llama_model::load_vocab(llama_model_loader & ml) {\n",
        "    const auto kv = LLM_KV(arch);\n",
        "    --> vocab.load(ml, kv);\n",
        "}\n",
        "\n",
        "void llama_vocab::load(llama_model_loader & ml, const LLM_KV & kv) {\n",
        "    --> pimpl->load(ml, kv);\n",
        "}\n",
        "\n",
        "void llama_vocab::impl::load(llama_model_loader & ml, const LLM_KV & kv) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f22f50-c727-4d33-8bac-95a2825460a2",
      "metadata": {
        "id": "88f22f50-c727-4d33-8bac-95a2825460a2"
      },
      "source": [
        "```c++\n",
        "struct gguf_context * ctx = ml.meta.get();\n",
        "// determine vocab type\n",
        "ml.get_key(LLM_KV_TOKENIZER_MODEL, tokenizer_model);\n",
        "ml.get_key(LLM_KV_TOKENIZER_PRE,   tokenizer_pre, false);\n",
        "ml.get_key(LLM_KV_TOKENIZER_TOKEN_TYPE_COUNT, n_token_types, false);\n",
        "// for now, only BPE models have pre-tokenizers\n",
        "llama_vocab::impl::\n",
        "    std::unordered_map<std::string, llama_token> token_to_id;\n",
        "    std::vector<token_data> id_to_token;\n",
        "--> init_tokenizer(type); --> tokenizer = std::make_unique<llm_tokenizer_bpe>(vocab);\n",
        "// determine the newline token: LLaMA \"<0x0A>\" == 10 == '\\n', Falcon 193 == '\\n\n",
        "// special tokens\n",
        "    std::set<llama_token> special_eog_ids; // set of all tokens that cause \"end of generation\"\n",
        "// build special tokens cache\n",
        "    std::vector<llama_token> cache_special_tokens;\n",
        "// build token to piece cache\n",
        "    std::vector<std::string> cache_token_to_piece; // llama_token_to_piece(special = true);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c62852-51d9-4b64-9244-73cb6e2081d8",
      "metadata": {
        "id": "e0c62852-51d9-4b64-9244-73cb6e2081d8"
      },
      "source": [
        "### [`llama_model::load_tensors`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#1467)\n",
        "\n",
        "```c++\n",
        "model.load_tensors(ml);\n",
        "----------\n",
        "bool llama_model::load_tensors(llama_model_loader & ml) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3073523b-3048-4b55-bfb4-01af5ddc4195",
      "metadata": {
        "id": "3073523b-3048-4b55-bfb4-01af5ddc4195"
      },
      "source": [
        "```c++\n",
        "// build a list of buffer types for the CPU and GPU devices\n",
        "pimpl->cpu_buft_list = make_cpu_buft_list(devices);\n",
        "// calculate the split points\n",
        "std::vector<float> splits(n_devices());\n",
        "std::copy(tensor_split, tensor_split + n_devices(), splits.begin());\n",
        "// sum and normalize the splits to get the split points\n",
        "ggml_backend_dev_t cpu_dev = ggml_backend_dev_by_type(GGML_BACKEND_DEVICE_TYPE_CPU);\n",
        "auto get_layer_buft_list = [&](int il) -> llama_model::impl::layer_dev {}\n",
        "// assign the input layer, there is very little benefit to offloading the input layer, so always keep it on the CPU\n",
        "pimpl->dev_input = { cpu_dev, &pimpl->cpu_buft_list };\n",
        "// assign the repeating layers to the devices according to the splits\n",
        "pimpl->dev_layer.resize(n_layer);\n",
        "for (int il = 0; il < n_layer; ++il) pimpl->dev_layer[il] = get_layer_buft_list(il);\n",
        "// assign the output layer\n",
        "pimpl->dev_output = get_layer_buft_list(n_layer);\n",
        "// one ggml context per buffer type\n",
        "int max_n_tensors = ml.n_tensors;\n",
        "max_n_tensors += 1;         // duplicated output tensor\n",
        "max_n_tensors += n_layer*2; // duplicated rope freq tensors\n",
        "const size_t ctx_size = ggml_tensor_overhead()*max_n_tensors;\n",
        "std::map<ggml_backend_buffer_type_t, ggml_context *> ctx_map;\n",
        "auto ctx_for_buft = [&](ggml_backend_buffer_type_t buft) -> ggml_context * {\n",
        "    auto it = ctx_map.find(buft);\n",
        "    if (it == ctx_map.end()) {\n",
        "        ggml_init_params params = {\n",
        "            /*.mem_size   =*/ ctx_size,\n",
        "            /*.mem_buffer =*/ NULL,\n",
        "            /*.no_alloc   =*/ true,\n",
        "        };\n",
        "        ggml_context * ctx = ggml_init(params);\n",
        "        ctx_map[buft] = ctx;\n",
        "        pimpl->ctxs.emplace_back(ctx);\n",
        "        return ctx;\n",
        "    }\n",
        "    return it->second;\n",
        "};\n",
        "// create tensors for the weights\n",
        "auto create_tensor = [&](const LLM_TN_IMPL & tn, const std::initializer_list<int64_t> & ne, int flags) -> ggml_tensor * {\n",
        "    ggml_tensor * t_meta = ml.get_tensor_meta(tn.str().c_str());\n",
        "    llm_tensor_info info = llm_tensor_info_for(tn_tensor);\n",
        "    // select the buffer type for this tensor\n",
        "    switch (info.layer) {}\n",
        "    --> select_weight_buft --> weight_buft_supported --> buft;\n",
        "    ggml_context * ctx = ctx_for_buft(buft);\n",
        "    --> return llama_model_loader::create_tensor;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cdd8350-9ed4-4b32-9a89-c73cd160f294",
      "metadata": {
        "id": "2cdd8350-9ed4-4b32-9a89-c73cd160f294"
      },
      "source": [
        "```c++\n",
        "layers.resize(n_layer);\n",
        "// TODO: move to a separate function\n",
        "const auto tn = LLM_TN(arch);\n",
        "\n",
        "tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, 0);\n",
        "\n",
        "// output\n",
        "output_norm = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd}, 0);\n",
        "output      = create_tensor(tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, TENSOR_NOT_REQUIRED);\n",
        "\n",
        "// if output is NULL, init from the input tok embed\n",
        "if (output == NULL)\n",
        "    output = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, TENSOR_DUPLICATED);\n",
        "\n",
        "for (int i = 0; i < n_layer; ++i)\n",
        "    auto & layer = layers[i];\n",
        "\n",
        "    layer.attn_norm = create_tensor(tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), {n_embd}, 0);\n",
        "\n",
        "    layer.wq = create_tensor(tn(LLM_TENSOR_ATTN_Q,   \"weight\", i), {n_embd, n_embd_head_k * n_head}, 0);\n",
        "    layer.wk = create_tensor(tn(LLM_TENSOR_ATTN_K,   \"weight\", i), {n_embd, n_embd_k_gqa}, 0);\n",
        "    layer.wv = create_tensor(tn(LLM_TENSOR_ATTN_V,   \"weight\", i), {n_embd, n_embd_v_gqa}, 0);\n",
        "    layer.wo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd_head_k * n_head, n_embd}, 0);\n",
        "\n",
        "    // optional bias tensors\n",
        "    layer.bq = create_tensor(tn(LLM_TENSOR_ATTN_Q,   \"bias\", i), {n_embd},     TENSOR_NOT_REQUIRED);\n",
        "    layer.bk = create_tensor(tn(LLM_TENSOR_ATTN_K,   \"bias\", i), {n_embd_gqa}, TENSOR_NOT_REQUIRED);\n",
        "    layer.bv = create_tensor(tn(LLM_TENSOR_ATTN_V,   \"bias\", i), {n_embd_gqa}, TENSOR_NOT_REQUIRED);\n",
        "    layer.bo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"bias\", i), {n_embd},     TENSOR_NOT_REQUIRED);\n",
        "\n",
        "    layer.ffn_norm = create_tensor(tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd}, 0);\n",
        "\n",
        "    layer.ffn_gate = create_tensor(tn(LLM_TENSOR_FFN_GATE, \"weight\", i), {n_embd,   n_ff}, 0);\n",
        "    layer.ffn_down = create_tensor(tn(LLM_TENSOR_FFN_DOWN, \"weight\", i), {  n_ff, n_embd}, 0);\n",
        "    layer.ffn_up   = create_tensor(tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff}, 0);\n",
        "\n",
        "    // optional MLP bias\n",
        "    layer.ffn_gate_b = create_tensor(tn(LLM_TENSOR_FFN_GATE, \"bias\", i), {n_ff}, TENSOR_NOT_REQUIRED);\n",
        "    layer.ffn_down_b = create_tensor(tn(LLM_TENSOR_FFN_DOWN, \"bias\", i), {n_embd}, TENSOR_NOT_REQUIRED);\n",
        "    layer.ffn_up_b   = create_tensor(tn(LLM_TENSOR_FFN_UP,   \"bias\", i), {n_ff}, TENSOR_NOT_REQUIRED);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1038a45e-ce5b-4c44-b79b-eb423abf15b6",
      "metadata": {
        "id": "1038a45e-ce5b-4c44-b79b-eb423abf15b6"
      },
      "source": [
        "```c++\n",
        "ml.done_getting_tensors();\n",
        "--> llama_model_loader::init_mappings --> ml.mappings;\n",
        "pimpl->mappings.reserve(ml.mappings.size());\n",
        "for (auto & mapping : ml.mappings)\n",
        "    pimpl->mappings.emplace_back(std::move(mapping));\n",
        "\n",
        "// create the backend buffers\n",
        "using llama_buf_map = std::unordered_map<uint32_t, ggml_backend_buffer_t>;\n",
        "std::vector<std::pair<ggml_context *, llama_buf_map>> ctx_bufs;\n",
        "ctx_bufs.reserve(ctx_map.size());\n",
        "\n",
        "// Ensure we have enough capacity for the maximum backend buffer we will potentially create\n",
        "pimpl->bufs.reserve(ctx_map.size());\n",
        "\n",
        "for (auto & it : ctx_map)\n",
        "    ggml_context * ctx = it.second;\n",
        "    llama_buf_map buf_map;\n",
        "    buf_map.reserve(1); // ml.files.size()\n",
        "    //if (ml.use_mmap && use_mmap_buffer && buffer_from_host_ptr_supported && is_default_buft)\n",
        "    //for (uint32_t idx = 0; idx < ml.files.size(); idx++)\n",
        "    // only the mmap region containing the tensors in the model is mapped to the backend buffer\n",
        "    // this is important for metal with apple silicon: if the entire model could be mapped to a metal buffer, then we could just use metal for all layers\n",
        "    // this allows using partial offloading when the model size exceeds the metal buffer size, but not the RAM size\n",
        "    void * addr = nullptr;\n",
        "    size_t first, last; // NOLINT\n",
        "    --> llama_model_loader::get_mapping_range;\n",
        "    --> ggml_backend_dev_buffer_from_host_ptr --> ggml_backend_cpu_buffer_from_ptr --> ggml_backend_buffer_t buf;\n",
        "    // indicate that this buffer contains weights, this is used by ggml_backend_sched to improve op scheduling: ops that use a weight are preferably scheduled to the backend that contains the weight\n",
        "    ggml_backend_buffer_set_usage(buf, GGML_BACKEND_BUFFER_USAGE_WEIGHTS);\n",
        "    pimpl->bufs.emplace_back(buf);\n",
        "    buf_map.emplace(0, buf);\n",
        "\n",
        "    ctx_bufs.emplace_back(ctx, buf_map);\n",
        "\n",
        "// populate tensors_by_name\n",
        "for (auto & ctx : pimpl->ctxs) for (auto * cur = ggml_get_first_tensor(ctx.get()); cur != NULL; cur = ggml_get_next_tensor(ctx.get(), cur)) tensors_by_name.emplace_back(ggml_get_name(cur), cur);\n",
        "\n",
        "// load tensor data\n",
        "for (auto & it : ctx_bufs)\n",
        "    ggml_context * ctx = it.first;\n",
        "    auto & bufs = it.second;\n",
        "    --> llama_model_loader::load_all_data;\n",
        "\n",
        "return true;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca22f4be-1f51-437c-ba6e-32f1caed638b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "ca22f4be-1f51-437c-ba6e-32f1caed638b"
      },
      "source": [
        "#### [`weight_buft_supported`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#L138)\n",
        "\n",
        "```c++\n",
        "ggml_backend_buffer_type_t buft = select_weight_buft(hparams, t_meta, op, *buft_list);\n",
        "----------\n",
        "// find the first buffer type in the list that can use the tensor\n",
        "static ggml_backend_buffer_type_t select_weight_buft(const llama_hparams & hparams, ggml_tensor * tensor, ggml_op op, const buft_list_t & buft_list) {\n",
        "    for (const auto & cur : buft_list)\n",
        "        --> if (weight_buft_supported(hparams, tensor, op, cur_buft = cur.second, cur_dev = cur.first)) {\n",
        "            return cur_buft;\n",
        "}\n",
        "\n",
        "// checks if the weight tensor can be used with the specified buffer type and device\n",
        "static bool weight_buft_supported(const llama_hparams & hparams, ggml_tensor * w, ggml_op op, ggml_backend_buffer_type_t buft, ggml_backend_dev_t dev) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94fbb3d1-24e1-47e1-8308-ec9de9ebe01f",
      "metadata": {
        "id": "94fbb3d1-24e1-47e1-8308-ec9de9ebe01f"
      },
      "source": [
        "```c++\n",
        "ggml_init_params params = {\n",
        "    /*.mem_size   =*/ ggml_tensor_overhead()*8,\n",
        "    /*.mem_buffer =*/ NULL,\n",
        "    /*.no_alloc   =*/ true,\n",
        "};\n",
        "ggml_context_ptr ctx_ptr { ggml_init(params) };\n",
        "\n",
        "ggml_context * ctx = ctx_ptr.get();\n",
        "ggml_tensor * op_tensor = nullptr;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "325a5303-6f78-4266-a722-206ea33d8d92",
      "metadata": {
        "id": "325a5303-6f78-4266-a722-206ea33d8d92"
      },
      "source": [
        "```c++\n",
        "switch (op) {\n",
        "    case GGML_OP_GET_ROWS:\n",
        "        {\n",
        "            ggml_tensor * b = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 512);\n",
        "            op_tensor = ggml_get_rows(ctx, w, b);\n",
        "        } break;\n",
        "    case GGML_OP_MUL_MAT:\n",
        "        {\n",
        "            ggml_tensor * b = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, w->ne[0], 512, w->ne[2], w->ne[3]);\n",
        "            op_tensor = ggml_mul_mat(ctx, w, b);\n",
        "        } break;\n",
        "    case GGML_OP_ADD:\n",
        "        {\n",
        "            ggml_tensor * a = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, w->ne[0], w->ne[1], w->ne[2], w->ne[3]);\n",
        "            op_tensor = ggml_add(ctx, a, w);\n",
        "        } break;\n",
        "    case GGML_OP_MUL:\n",
        "        {\n",
        "            ggml_tensor * a = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, w->ne[0], w->ne[1], w->ne[2], w->ne[3]);\n",
        "            op_tensor = ggml_mul(ctx, a, w);\n",
        "        } break;\n",
        "    case GGML_OP_DIV:\n",
        "        {\n",
        "            ggml_tensor * a = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, w->ne[0]);\n",
        "            op_tensor = ggml_div(ctx, a, w);\n",
        "        } break;\n",
        "    case GGML_OP_ROPE:\n",
        "        {\n",
        "            int n_embd_head = hparams.n_embd_head_v;\n",
        "            int n_head = hparams.n_head();\n",
        "            ggml_tensor * a = ggml_new_tensor_3d(ctx, GGML_TYPE_F32, n_embd_head, n_head, 512);\n",
        "            ggml_tensor * b = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 512);\n",
        "            op_tensor = ggml_rope_ext(\n",
        "                ctx, a, b, w,\n",
        "                0, 0, 0, 0, 0,\n",
        "                0, 0, 0, 0\n",
        "            );\n",
        "        } break;\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary>ggml_new_tensor</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_tensor * ggml_new_tensor_1d()\n",
        "    return ggml_new_tensor(ctx, type, 1, &ne0);\n",
        "\n",
        "struct ggml_tensor * ggml_new_tensor_2d()\n",
        "    const int64_t ne[2] = { ne0, ne1};\n",
        "    return ggml_new_tensor(ctx, type, 2, ne);\n",
        "\n",
        "struct ggml_tensor * ggml_new_tensor_3d()\n",
        "    const int64_t ne[3] = { ne0, ne1, ne2 };\n",
        "    return ggml_new_tensor(ctx, type, 3, ne);\n",
        "\n",
        "struct ggml_tensor * ggml_new_tensor_4d()\n",
        "    const int64_t ne[4] = { ne0, ne1, ne2, ne3 };\n",
        "    return ggml_new_tensor(ctx, type, 4, ne);\n",
        "\n",
        "struct ggml_tensor * ggml_get_rows()\n",
        "    struct ggml_tensor * result = ggml_new_tensor_4d(ctx, type, a->ne[0], b->ne[0], b->ne[1], b->ne[2]);\n",
        "    result->op     = GGML_OP_GET_ROWS;\n",
        "    result->src[0] = a;\n",
        "    result->src[1] = b;\n",
        "    return result;\n",
        "\n",
        "static struct ggml_tensor * ggml_rope_impl()\n",
        "    int sections[4] = {0, 0, 0, 0};\n",
        "\n",
        "    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n",
        "\n",
        "    int32_t params[15] = { /*n_past*/ 0, n_dims, mode, /*n_ctx*/ 0, n_ctx_orig };\n",
        "    memcpy(params +  5, &freq_base,    sizeof(float));\n",
        "    memcpy(params +  6, &freq_scale,   sizeof(float));\n",
        "    memcpy(params +  7, &ext_factor,   sizeof(float));\n",
        "    memcpy(params +  8, &attn_factor,  sizeof(float));\n",
        "    memcpy(params +  9, &beta_fast,    sizeof(float));\n",
        "    memcpy(params + 10, &beta_slow,    sizeof(float));\n",
        "    memcpy(params + 11, &sections,     sizeof(int)*4);\n",
        "    ggml_set_op_params(result, params, sizeof(params));\n",
        "\n",
        "    result->op     = GGML_OP_ROPE;\n",
        "    result->src[0] = a;\n",
        "    result->src[1] = b;\n",
        "    result->src[2] = c;\n",
        "\n",
        "    return result;\n",
        "```\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d53c9982-bda8-41f4-8741-18241262b8b0",
      "metadata": {
        "id": "d53c9982-bda8-41f4-8741-18241262b8b0"
      },
      "source": [
        "```c++\n",
        "w->buffer = ggml_backend_buft_alloc_buffer(buft, 0);\n",
        "bool op_supported = ggml_backend_dev_supports_op(dev, op_tensor);\n",
        "ggml_backend_buffer_free(w->buffer);\n",
        "return op_supported;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c727af-2ca8-44e5-828f-09c3d2ec4abc",
      "metadata": {
        "id": "b1c727af-2ca8-44e5-828f-09c3d2ec4abc"
      },
      "source": [
        "#### [`llama_model_loader::create_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L789)\n",
        "\n",
        "```c++\n",
        "return ml.create_tensor(ctx, tn, ne, flags);\n",
        "----------\n",
        "struct ggml_tensor * llama_model_loader::create_tensor(struct ggml_context * ctx, const std::string & name, const std::initializer_list<int64_t> & ne, int flags) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb77bfe6-68d8-442e-85b9-7b9913d6e15f",
      "metadata": {
        "id": "bb77bfe6-68d8-442e-85b9-7b9913d6e15f"
      },
      "source": [
        "```c++\n",
        "--> check_tensor_dims --> cur;\n",
        "bool duplicated = flags & TENSOR_DUPLICATED;\n",
        "--> ggml_dup_tensor --> tensor;\n",
        "ggml_set_name(tensor, ggml_get_name(cur));\n",
        "if (duplicated) {\n",
        "    size_data += ggml_nbytes(cur);\n",
        "} else {\n",
        "    n_created++;\n",
        "}\n",
        "return tensor;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "146c992f-40c0-4e93-85fa-ab915b5ee0dc",
      "metadata": {
        "id": "146c992f-40c0-4e93-85fa-ab915b5ee0dc"
      },
      "source": [
        "[`llama_model_loader::check_tensor_dims`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L759)\n",
        "\n",
        "```c++\n",
        "const struct ggml_tensor * cur = check_tensor_dims(name, ne, !(flags & TENSOR_NOT_REQUIRED));\n",
        "----------\n",
        "const struct ggml_tensor * llama_model_loader::check_tensor_dims(const std::string & name, const std::vector<int64_t> & ne, bool required) const {\n",
        "    const struct ggml_tensor * cur = get_tensor_meta(name.c_str());\n",
        "    return cur;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31b327e9-86e6-4308-9eb6-aa39a0812b6b",
      "metadata": {
        "id": "31b327e9-86e6-4308-9eb6-aa39a0812b6b"
      },
      "source": [
        "[`ggml_dup_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1698)\n",
        "\n",
        "```c++\n",
        "struct ggml_tensor * tensor = ggml_dup_tensor(ctx, cur);\n",
        "----------\n",
        "struct ggml_tensor * ggml_dup_tensor(struct ggml_context * ctx, const struct ggml_tensor * src) {\n",
        "    return ggml_new_tensor(ctx, src->type, GGML_MAX_DIMS, src->ne);\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e54ddae-ac49-42fd-badb-fe9410f808d3",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8e54ddae-ac49-42fd-badb-fe9410f808d3"
      },
      "source": [
        "#### [`llama_model_loader::init_mappings`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L845)\n",
        "\n",
        "```c++\n",
        "ml.init_mappings(true, nullptr);\n",
        "----------\n",
        "void llama_model_loader::init_mappings(bool prefetch, llama_mlocks * mlock_mmaps) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60c5c1a2-b9aa-4269-ac36-93b676d6909e",
      "metadata": {
        "id": "60c5c1a2-b9aa-4269-ac36-93b676d6909e"
      },
      "source": [
        "```c++\n",
        "--> llama_mmap::llama_mmap --> llama_mmap::impl::impl --> mapping;\n",
        "llama_model_loader:: std::vector<std::pair<size_t, size_t>> mmaps_used;\n",
        "mmaps_used.emplace_back(mapping->size(), 0);\n",
        "\n",
        "using llama_mmaps  = std::vector<std::unique_ptr<llama_mmap>>;\n",
        "llama_model_loader:: llama_mmaps mappings;\n",
        "mappings.emplace_back(std::move(mapping));\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c5504b-b640-44c4-bce7-ec63d58875f9",
      "metadata": {
        "id": "25c5504b-b640-44c4-bce7-ec63d58875f9"
      },
      "source": [
        "[`llama_mmap::llama_mmap`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-mmap.cpp#L441)\n",
        "\n",
        "```c++\n",
        "std::unique_ptr<llama_mmap> mapping = std::make_unique<llama_mmap>(file.get(), prefetch ? -1 : 0, is_numa);\n",
        "----------\n",
        "llama_mmap::llama_mmap(struct llama_file * file, size_t prefetch, bool numa) : pimpl(std::make_unique<impl>(file, prefetch, numa)) {}\n",
        "struct llama_mmap::impl {\n",
        "    std::vector<std::pair<size_t, size_t>> mapped_fragments;\n",
        "    impl(struct llama_file * file, size_t prefetch, bool numa) {\n",
        "        size = file->size();\n",
        "        int fd = file->file_id();\n",
        "        int flags = MAP_SHARED;\n",
        "        if (prefetch) { flags |= MAP_POPULATE; }\n",
        "        --> addr = mmap(NULL, file->size(), PROT_READ, flags, fd, 0);\n",
        "        if (prefetch > 0) {\n",
        "            if (posix_madvise(addr, std::min(file->size(), prefetch), POSIX_MADV_WILLNEED)) {\n",
        "                LLAMA_LOG_WARN(\"warning: posix_madvise(.., POSIX_MADV_WILLNEED) failed: %s\\n\",\n",
        "                        strerror(errno));\n",
        "            }\n",
        "        }\n",
        "        mapped_fragments.emplace_back(0, file->size());\n",
        "    }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edea6ed7-7a50-44f2-8e68-37447c81f32a",
      "metadata": {
        "id": "edea6ed7-7a50-44f2-8e68-37447c81f32a"
      },
      "source": [
        "#### [`llama_model_loader::get_mapping_range`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L878)\n",
        "\n",
        "```c++\n",
        "ml.get_mapping_range(&first, &last, &addr, idx=0, ctx);\n",
        "----------\n",
        "void llama_model_loader::get_mapping_range(size_t * first, size_t * last, void ** addr, int idx, ggml_context * ctx) const {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66b56a65-03b8-4a4d-9574-caa9e8d79942",
      "metadata": {
        "id": "66b56a65-03b8-4a4d-9574-caa9e8d79942"
      },
      "source": [
        "```c++\n",
        "const auto & mapping = mappings.at(idx);\n",
        "\n",
        "*first = mapping->size();\n",
        "*last  = 0;\n",
        "*addr = mapping->addr();\n",
        "for (ggml_tensor * tensor = ggml_get_first_tensor(ctx); tensor; tensor = ggml_get_next_tensor(ctx, tensor)) {\n",
        "    const auto * weight = get_weight(ggml_get_name(tensor));\n",
        "    if (!weight || weight->idx != idx) {\n",
        "        continue;\n",
        "    }\n",
        "    *first = std::min(*first, weight->offs);\n",
        "    *last  = std::max(*last,  weight->offs + ggml_nbytes(tensor));\n",
        "}\n",
        "```\n",
        "\n",
        "<details>\n",
        "<summary>struct llama_model_loader::llama_tensor_weight</summary>\n",
        "\n",
        "```c++\n",
        "// Holds information on a model weight\n",
        "struct llama_tensor_weight {\n",
        "    uint16_t  idx; // source file index\n",
        "    size_t   offs; // tensor data offset in the original file\n",
        "    ggml_tensor * tensor;\n",
        "}\n",
        "\n",
        "const llama_model_loader::llama_tensor_weight * llama_model_loader::get_weight(const char * name) const {\n",
        "    auto pos = weights_map.find(name);\n",
        "    if (pos != weights_map.end()) {\n",
        "        return &pos->second;\n",
        "    }\n",
        "\n",
        "    return nullptr;\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3ed2404-8fdc-48b2-8c24-173fd6321bda",
      "metadata": {
        "id": "b3ed2404-8fdc-48b2-8c24-173fd6321bda"
      },
      "source": [
        "#### [`ggml_backend_cpu_buffer_from_ptr`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L2013)\n",
        "\n",
        "```c++\n",
        "ggml_backend_buffer_t buf = ggml_backend_dev_buffer_from_host_ptr(dev, (char *) addr + first, last - first, ggml_get_max_tensor_size(ctx));\n",
        "----------\n",
        "ggml_backend_buffer_t ggml_backend_dev_buffer_from_host_ptr(ggml_backend_dev_t device, void * ptr, size_t size, size_t max_tensor_size) {\n",
        "    --> return device->iface.buffer_from_host_ptr(device, ptr, size, max_tensor_size);\n",
        "}\n",
        "\n",
        "static ggml_backend_buffer_t ggml_backend_cpu_device_buffer_from_host_ptr(ggml_backend_dev_t dev, void * ptr, size_t size, size_t max_tensor_size) {\n",
        "    --> return ggml_backend_cpu_buffer_from_ptr(ptr, size);\n",
        "}\n",
        "\n",
        "ggml_backend_buffer_t ggml_backend_cpu_buffer_from_ptr(void * ptr, size_t size) {\n",
        "    --> return ggml_backend_buffer_init(ggml_backend_cpu_buffer_from_ptr_type(), ggml_backend_cpu_buffer_from_ptr_i, ptr, size);\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f0ba812-14e6-4642-a5bf-6eb4509254b2",
      "metadata": {
        "id": "6f0ba812-14e6-4642-a5bf-6eb4509254b2"
      },
      "source": [
        "[`ggml_backend_buffer_init`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L82)\n",
        "\n",
        "<details>\n",
        "<summary>struct ggml_backend_buffer</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_backend_buffer {\n",
        "    struct ggml_backend_buffer_i  iface;\n",
        "    ggml_backend_buffer_type_t    buft;\n",
        "    void * context;\n",
        "    size_t size;\n",
        "    enum ggml_backend_buffer_usage usage;\n",
        "};\n",
        "\n",
        "typedef struct ggml_backend_buffer * ggml_backend_buffer_t;\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "// backend buffer\n",
        "\n",
        "ggml_backend_buffer_t ggml_backend_buffer_init(\n",
        "               ggml_backend_buffer_type_t buft,\n",
        "        struct ggml_backend_buffer_i      iface,\n",
        "               void *                     context,\n",
        "               size_t                     size) {\n",
        "    ggml_backend_buffer_t buffer = new ggml_backend_buffer {\n",
        "        /* .interface = */ iface,\n",
        "        /* .buft      = */ buft,\n",
        "        /* .context   = */ context,\n",
        "        /* .size      = */ size,\n",
        "        /* .usage     = */ GGML_BACKEND_BUFFER_USAGE_ANY\n",
        "    };\n",
        "\n",
        "    return buffer;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_backend_cpu_buffer_from_ptr_type`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1996)\n",
        "\n",
        "<details>\n",
        "<summary>struct ggml_backend_buffer_type</summary>\n",
        "    \n",
        "```c++\n",
        "struct ggml_backend_buffer_type {\n",
        "    struct ggml_backend_buffer_type_i  iface;\n",
        "    ggml_backend_dev_t device;\n",
        "    void * context;\n",
        "};\n",
        "\n",
        "typedef struct ggml_backend_buffer_type * ggml_backend_buffer_type_t;\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "\n",
        "static ggml_backend_buffer_type_t ggml_backend_cpu_buffer_from_ptr_type(void) {\n",
        "    static struct ggml_backend_buffer_type ggml_backend_cpu_buffer_type = {\n",
        "        /* .iface   = */ {\n",
        "            /* .get_name         = */ ggml_backend_cpu_buffer_from_ptr_type_get_name,\n",
        "            /* .alloc_buffer     = */ ggml_backend_cpu_buffer_type_alloc_buffer,\n",
        "            /* .get_alignment    = */ ggml_backend_cpu_buffer_type_get_alignment,\n",
        "            /* .get_max_size     = */ NULL, // defaults to SIZE_MAX\n",
        "            /* .get_alloc_size   = */ NULL, // defaults to ggml_nbytes\n",
        "            /* .is_host          = */ ggml_backend_cpu_buffer_type_is_host,\n",
        "        },\n",
        "        /* .device  = */ NULL, // FIXME ggml_backend_reg_dev_get(ggml_backend_cpu_reg(), 0),\n",
        "        /* .context = */ NULL,\n",
        "    };\n",
        "\n",
        "    return &ggml_backend_cpu_buffer_type;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "p-pkzumMj-sQ"
      },
      "id": "p-pkzumMj-sQ"
    },
    {
      "cell_type": "markdown",
      "id": "ded1b402-3b72-4096-9f7a-961391ff4079",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "ded1b402-3b72-4096-9f7a-961391ff4079"
      },
      "source": [
        "#### [`llama_model_loader::load_all_data`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L918)\n",
        "\n",
        "```c++\n",
        "ml.load_all_data(ctx, bufs, use_mlock ? &pimpl->mlock_mmaps : NULL, params.progress_callback, params.progress_callback_user_data);\n",
        "----------\n",
        "bool llama_model_loader::load_all_data(\n",
        "        struct ggml_context * ctx,\n",
        "        llama_buf_map & bufs,\n",
        "        llama_mlocks * lmlocks,\n",
        "        llama_progress_callback progress_callback,\n",
        "        void * progress_callback_user_data) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d3dd224-27e8-467f-aad1-7901c3893ce9",
      "metadata": {
        "id": "3d3dd224-27e8-467f-aad1-7901c3893ce9"
      },
      "source": [
        "```c++\n",
        "for (struct ggml_tensor * cur = ggml_get_first_tensor(ctx); cur != NULL; cur = ggml_get_next_tensor(ctx, cur))\n",
        "    const auto * weight = get_weight(ggml_get_name(cur));\n",
        "    size_t n_size = ggml_nbytes(cur);\n",
        "    const auto & mapping = mappings.at(weight->idx);\n",
        "    ggml_backend_buffer_t buf_mmap = bufs.at(weight->idx);\n",
        "    uint8_t * data = (uint8_t *) mapping->addr() + weight->offs;\n",
        "    --> ggml_backend_tensor_alloc(buffer=buf_mmap, tensor=cur, addr=data);\n",
        "        tensor->buffer = buffer;\n",
        "        tensor->data = addr;\n",
        "        return ggml_backend_buffer_init_tensor(buffer, tensor);\n",
        "    auto & mmap_used = mmaps_used[weight->idx];\n",
        "    mmap_used.first  = std::min(mmap_used.first,  weight->offs);\n",
        "    mmap_used.second = std::max(mmap_used.second, weight->offs + n_size);\n",
        "    size_done += n_size;\n",
        "// check if this is the last call and do final cleanup\n",
        "for (uint32_t idx = 0; idx < mappings.size(); idx++)\n",
        "    const auto & mmap_used = mmaps_used.at(idx);\n",
        "    auto & mapping = mappings.at(idx);\n",
        "    --> mapping->unmap_fragment(0, mmap_used.first);\n",
        "    --> if (mmap_used.second != 0) mapping->unmap_fragment(mmap_used.second, mapping->size());\n",
        "return true;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40142326-2ae9-489a-addc-70ddb31e281b",
      "metadata": {
        "id": "40142326-2ae9-489a-addc-70ddb31e281b"
      },
      "source": [
        "[`llama_mmap::unmap_fragment`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-mmap.cpp#L447)\n",
        "\n",
        "```c++\n",
        "void llama_mmap::unmap_fragment(size_t first, size_t last) { pimpl->unmap_fragment(first, last); }\n",
        "\n",
        "struct llama_mmap::impl {\n",
        "    void unmap_fragment(size_t first, size_t last) {\n",
        "        int page_size = sysconf(_SC_PAGESIZE);\n",
        "        align_range(&first, &last, page_size);\n",
        "        size_t len = last - first;\n",
        "        void * next_page_start = (uint8_t *) addr + first;\n",
        "        munmap(next_page_start, len);\n",
        "        for (const auto & frag : mapped_fragments) new_mapped_fragments.emplace_back(last, frag.second);\n",
        "        mapped_fragments = std::move(new_mapped_fragments);\n",
        "    }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7635e210-5a12-4060-b2ef-1aae86fe9b14",
      "metadata": {
        "id": "7635e210-5a12-4060-b2ef-1aae86fe9b14"
      },
      "source": [
        "## [`llama_context::llama_context`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L18)\n",
        "\n",
        "```c++\n",
        "llama_context * lctx = llama_init_from_model(model, cparams);\n",
        "----------\n",
        "llama_context * llama_init_from_model(\n",
        "                 llama_model * model,\n",
        "        llama_context_params   params) {\n",
        "    --> auto * ctx = new llama_context(*model, params);\n",
        "    return ctx;\n",
        "}\n",
        "\n",
        "llama_context::llama_context(\n",
        "        const llama_model & model,\n",
        "              llama_context_params params) :\n",
        "    model(model),\n",
        "    batch_allocr(std::make_unique<llama_batch_allocr>()) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c42f04f-0568-431f-940f-bf7e71945255",
      "metadata": {
        "id": "0c42f04f-0568-431f-940f-bf7e71945255"
      },
      "source": [
        "<details>\n",
        "<summary>struct llama_context</summary>\n",
        "\n",
        "```c++\n",
        "struct llama_context {\n",
        "private:\n",
        "    const llama_model & model;\n",
        "\n",
        "    llama_cparams       cparams;\n",
        "\n",
        "    std::unique_ptr<llama_memory_i> memory;\n",
        "\n",
        "    // TODO: temporary, until the llama_kv_self_defrag() API is removed\n",
        "    bool memory_force_optimize = false;\n",
        "\n",
        "    // decode output (2-dimensional array: [n_outputs][n_vocab])\n",
        "    size_t  logits_size = 0; // capacity (of floats) for logits\n",
        "    float * logits      = nullptr;\n",
        "\n",
        "    // reuse the batch_allocr to avoid unnecessary memory allocations\n",
        "    std::unique_ptr<llama_batch_allocr> batch_allocr;\n",
        "\n",
        "    int32_t n_outputs     = 0; // number of actually-used outputs in the current ubatch or last logical batch\n",
        "    int32_t n_outputs_max = 0; // capacity (of tokens positions) for the output buffers\n",
        "\n",
        "    std::vector<int32_t> output_ids; // map batch token positions to ids of the logits and embd buffers\n",
        "\n",
        "    ggml_backend_sched_ptr sched;\n",
        "\n",
        "    ggml_backend_t backend_cpu = nullptr;\n",
        "    std::vector<ggml_backend_ptr> backends;\n",
        "\n",
        "    ggml_context_ptr ctx_compute;\n",
        "\n",
        "    // training\n",
        "    ggml_opt_context_t opt_ctx = nullptr;\n",
        "\n",
        "    ggml_threadpool_t threadpool       = nullptr;\n",
        "    ggml_threadpool_t threadpool_batch = nullptr;\n",
        "\n",
        "    ggml_abort_callback abort_callback      = nullptr;\n",
        "    void *              abort_callback_data = nullptr;\n",
        "\n",
        "    std::vector<std::pair<ggml_backend_t, ggml_backend_set_n_threads_t>> set_n_threads_fns;\n",
        "\n",
        "    // buffer types used for the compute buffer of each backend\n",
        "    std::vector<ggml_backend_t>             backend_ptrs;\n",
        "    std::vector<ggml_backend_buffer_type_t> backend_buft;\n",
        "\n",
        "    // memory buffers used to evaluate the model\n",
        "    std::vector<uint8_t> buf_compute_meta;\n",
        "\n",
        "    // host buffer for the model output (logits and embeddings)\n",
        "    ggml_backend_buffer_ptr buf_output;\n",
        "\n",
        "    bool has_evaluated_once = false;\n",
        "\n",
        "    // perf\n",
        "    mutable int64_t t_start_us  = 0;\n",
        "    mutable int64_t t_load_us   = 0;\n",
        "    mutable int64_t t_p_eval_us = 0;\n",
        "    mutable int64_t t_eval_us   = 0;\n",
        "\n",
        "    mutable int64_t t_compute_start_us = 0;\n",
        "    mutable int64_t n_queued_tokens    = 0;\n",
        "\n",
        "    mutable int32_t n_p_eval = 0; // number of tokens in eval calls for the prompt (with batch size > 1)\n",
        "    mutable int32_t n_eval   = 0; // number of eval calls\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "```c++\n",
        "// GPU backends\n",
        "// add ACCEL backends (such as BLAS)\n",
        "// add CPU backend\n",
        "llama_context:: ggml_backend_t backend_cpu = nullptr;\n",
        "--> ggml_backend_init_by_type --> ggml_backend_dev_init --> ggml_backend_cpu_device_init_backend --> ggml_backend_cpu_init --> backend_cpu;\n",
        "backends.emplace_back(backend_cpu);\n",
        "// create a list of the set_n_threads functions in the backends\n",
        "llama_context:: std::vector<std::pair<ggml_backend_t, ggml_backend_set_n_threads_t>>\n",
        "set_n_threads_fns.emplace_back(backend.get(), ggml_backend_set_n_threads_fn);\n",
        "// graph outputs buffer\n",
        "// resized during inference when a batch uses more outputs\n",
        "--> llama_context::output_reserve;\n",
        "\n",
        "// init the memory module\n",
        "llama_memory_params params_mem = {\n",
        "    /*.type_k   =*/ params.type_k,\n",
        "    /*.type_v   =*/ params.type_v,\n",
        "    /*.swa_full =*/ params.swa_full,\n",
        "};\n",
        "llama_context:: std::unique_ptr<llama_memory_i> memory;\n",
        "--> llama_model::create_memory --> llama_context:: memory;\n",
        "\n",
        "// init backends\n",
        "backend_buft.clear();\n",
        "backend_ptrs.clear();\n",
        "//for (auto & backend : backends)\n",
        "auto * buft = ggml_backend_get_default_buffer_type(backend.get());\n",
        "auto backend_type = ggml_backend_dev_type(ggml_backend_get_device(backend.get()));\n",
        "backend_buft.push_back(buft);\n",
        "backend_ptrs.push_back(backend.get());\n",
        "\n",
        "const size_t max_nodes = this->graph_max_nodes();\n",
        "// memory buffers used to evaluate the model\n",
        "llama_context:: std::vector<uint8_t> buf_compute_meta;\n",
        "// buffer used to store the computation graph and the tensor meta data\n",
        "--> ggml_graph_nbytes --> buf_compute_meta;\n",
        "llama_context:: ggml_backend_sched_ptr sched;\n",
        "--> ggml_backend_sched_new --> sched;\n",
        "\n",
        "// reserve worst-case graph\n",
        "const uint32_t n_seqs = cparams.n_seq_max;\n",
        "const uint32_t n_tokens = std::min(cparams.n_ctx, cparams.n_ubatch);\n",
        "// simulate full KV cache\n",
        "--> llama_kv_cache_unified_state::llama_kv_cache_unified_state --> mstate;\n",
        "\n",
        "// reserve pp graph first so that buffers are only allocated once\n",
        "--> llama_context::graph_reserve --> gf;\n",
        "// reserve with tg graph to get the number of splits and nodes\n",
        "--> llama_context::graph_reserve\n",
        "// reserve again with pp graph to avoid ggml-alloc reallocations during inference\n",
        "--> llama_context::graph_reserve\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4844a666-be63-4d77-bfde-6ee25055d585",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "4844a666-be63-4d77-bfde-6ee25055d585"
      },
      "source": [
        "### [`ggml_backend_cpu_init`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.cpp#L196)\n",
        "\n",
        "```c++\n",
        "backend_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);\n",
        "----------\n",
        "ggml_backend_t ggml_backend_init_by_type(enum ggml_backend_dev_type type, const char * params) {\n",
        "    ggml_backend_dev_t dev = ggml_backend_dev_by_type(type);\n",
        "    if (!dev) {\n",
        "        return nullptr;\n",
        "    }\n",
        "    --> return ggml_backend_dev_init(dev, params);\n",
        "}\n",
        "\n",
        "\n",
        "ggml_backend_dev_t ggml_backend_dev_by_type(enum ggml_backend_dev_type type)\n",
        "    for (size_t i = 0; i < ggml_backend_dev_count(); i++) {\n",
        "        ggml_backend_dev_t dev = ggml_backend_dev_get(i);\n",
        "        if (ggml_backend_dev_type(dev) == type) {\n",
        "            return dev;\n",
        "        }\n",
        "    }\n",
        "\n",
        "ggml_backend_t ggml_backend_dev_init(ggml_backend_dev_t device, const char * params)\n",
        "    --> return device->iface.init_backend(device, params);\n",
        "\n",
        "static ggml_backend_t ggml_backend_cpu_device_init_backend(ggml_backend_dev_t dev, const char * params)\n",
        "    --> return ggml_backend_cpu_init();\n",
        "\n",
        "ggml_backend_t ggml_backend_cpu_init(void) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c198c8f7-2d5f-4d39-928d-122e8bb7b8f2",
      "metadata": {
        "id": "c198c8f7-2d5f-4d39-928d-122e8bb7b8f2"
      },
      "source": [
        "<details>\n",
        "<summary>struct ggml_backend</summary>\n",
        "\n",
        "```c++\n",
        "\n",
        "struct ggml_backend {\n",
        "    ggml_guid_t guid;\n",
        "    struct ggml_backend_i iface;\n",
        "    ggml_backend_dev_t device;\n",
        "    void * context;\n",
        "};\n",
        "\n",
        "typedef struct ggml_backend * ggml_backend_t;\n",
        "```\n",
        "    \n",
        "</details>\n",
        "\n",
        "```c++\n",
        "// initialize CPU backend now to avoid slowing the first graph computation\n",
        "ggml_cpu_init();\n",
        "\n",
        "struct ggml_backend_cpu_context * ctx = new ggml_backend_cpu_context;\n",
        "\n",
        "ctx->n_threads           = GGML_DEFAULT_N_THREADS;\n",
        "ctx->threadpool          = NULL;\n",
        "ctx->work_data           = NULL;\n",
        "ctx->work_size           = 0;\n",
        "ctx->abort_callback      = NULL;\n",
        "ctx->abort_callback_data = NULL;\n",
        "\n",
        "ggml_backend_t cpu_backend = new ggml_backend {\n",
        "    /* .guid      = */ ggml_backend_cpu_guid(),\n",
        "    /* .interface = */ ggml_backend_cpu_i,\n",
        "    /* .device    = */ ggml_backend_reg_dev_get(ggml_backend_cpu_reg(), 0),\n",
        "    /* .context   = */ ctx,\n",
        "};\n",
        "\n",
        "return cpu_backend;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6f065a-2f96-4902-8503-06b7e5d3ed02",
      "metadata": {
        "id": "3e6f065a-2f96-4902-8503-06b7e5d3ed02"
      },
      "source": [
        "### [`llama_context::output_reserve`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L1239)\n",
        "\n",
        "```c++\n",
        "output_reserve(params.n_seq_max);\n",
        "----------\n",
        "int32_t llama_context::output_reserve(int32_t n_outputs) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3acbf15b-b5f0-4e74-9322-0f72dcf2c072",
      "metadata": {
        "id": "3acbf15b-b5f0-4e74-9322-0f72dcf2c072"
      },
      "source": [
        "```c++\n",
        "struct llama_context\n",
        "    // decode output (2-dimensional array: [n_outputs][n_vocab])\n",
        "    size_t  logits_size = 0; // capacity (of floats) for logits\n",
        "    float * logits      = nullptr;\n",
        "\n",
        "// map batch token positions to ids of the logits and embd buffers\n",
        "llama_context:: std::vector<int32_t> output_ids;\n",
        "output_ids.resize(n_batch);\n",
        "\n",
        "const int64_t n_outputs_max = std::max<int64_t>(n_outputs, n_seq_max());\n",
        "logits_size = n_vocab*n_outputs_max;\n",
        "const size_t new_size  = (logits_size + embd_size) * sizeof(float);\n",
        "auto * buft = ggml_backend_cpu_buffer_type();\n",
        "// host buffer for the model output (logits and embeddings)\n",
        "llama_context:: ggml_backend_buffer_ptr buf_output;\n",
        "--> ggml_backend_buft_alloc_buffer --> ggml_backend_cpu_buffer_type_alloc_buffer --> buf_output;\n",
        "\n",
        "--> ggml_backend_buffer_get_base --> ggml_backend_cpu_buffer_get_base --> output_base;\n",
        "logits = output_base;\n",
        "// set all ids as invalid (negative)\n",
        "std::fill(output_ids.begin(), output_ids.end(), -1);\n",
        "this->n_outputs = 0;\n",
        "this->n_outputs_max = n_outputs_max;\n",
        "return n_outputs_max;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c477f94d-3156-4dc7-ac70-7d921e4a3f2d",
      "metadata": {
        "id": "c477f94d-3156-4dc7-ac70-7d921e4a3f2d"
      },
      "source": [
        "[`ggml_backend_cpu_buffer_type_alloc_buffer`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1950)\n",
        "\n",
        "```c++\n",
        "ggml_backend_buffer_ptr llama_context:: buf_output.reset(ggml_backend_buft_alloc_buffer(buft, new_size));\n",
        "----------\n",
        "ggml_backend_buffer_t ggml_backend_buft_alloc_buffer(ggml_backend_buffer_type_t buft, size_t size) {\n",
        "    if (size == 0) {\n",
        "        // return a dummy buffer for zero-sized allocations\n",
        "        return ggml_backend_buffer_init(buft, {}, NULL, 0);\n",
        "    }\n",
        "    --> return buft->iface.alloc_buffer(buft, size);\n",
        "}\n",
        "\n",
        "static ggml_backend_buffer_t ggml_backend_cpu_buffer_type_alloc_buffer(ggml_backend_buffer_type_t buft, size_t size) {\n",
        "    void * data = ggml_aligned_malloc(size);\n",
        "    return ggml_backend_buffer_init(buft, ggml_backend_cpu_buffer_i, data, size);\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b348dedc-0e61-4c11-91bf-a825ae29e133",
      "metadata": {
        "id": "b348dedc-0e61-4c11-91bf-a825ae29e133"
      },
      "source": [
        "[`ggml_backend_cpu_buffer_get_base`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1869)\n",
        "\n",
        "```c++\n",
        "float * output_base = (float *) ggml_backend_buffer_get_base(buf_output.get());\n",
        "----------\n",
        "void * ggml_backend_buffer_get_base(ggml_backend_buffer_t buffer) {\n",
        "    // get_base is optional if the buffer is zero-sized\n",
        "    if (buffer->size == 0) {\n",
        "        return NULL;\n",
        "    }\n",
        "    return buffer->iface.get_base(buffer);\n",
        "}\n",
        "\n",
        "static void * ggml_backend_cpu_buffer_get_base(ggml_backend_buffer_t buffer) {\n",
        "    uintptr_t data = (uintptr_t)buffer->context;\n",
        "\n",
        "    // align the buffer\n",
        "    if (data % TENSOR_ALIGNMENT != 0) {\n",
        "        data = GGML_PAD(data, TENSOR_ALIGNMENT);\n",
        "    }\n",
        "\n",
        "    return (void *)data;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26c21825-1ade-484e-a922-f1f009a36f1b",
      "metadata": {
        "id": "26c21825-1ade-484e-a922-f1f009a36f1b"
      },
      "source": [
        "### [`llama_model::create_memory`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#L13206)\n",
        "\n",
        "\n",
        "```c++\n",
        "memory.reset(model.create_memory(params_mem, cparams));\n",
        "----------\n",
        "llama_memory_i * llama_model::create_memory(const llama_memory_params & params, llama_cparams & cparams) const {\n",
        "    llama_memory_i * res;\n",
        "    const auto padding = llama_kv_cache_unified::get_padding(cparams);\n",
        "        --> llama_kv_cache_unified::get_padding(const llama_cparams & cparams)\n",
        "        // the FA kernels require padding to avoid extra runtime boundary checks\n",
        "        return cparams.flash_attn ? 256u : 32u;\n",
        "    cparams.n_ctx = GGML_PAD(cparams.n_ctx, padding);\n",
        "    --> res = new llama_kv_cache_unified(\n",
        "            *this,\n",
        "            nullptr,\n",
        "            params.type_k,\n",
        "            params.type_v,\n",
        "            !cparams.flash_attn,\n",
        "            cparams.offload_kqv,\n",
        "            cparams.n_ctx,\n",
        "            cparams.n_seq_max,\n",
        "            padding,\n",
        "            hparams.n_swa,\n",
        "            hparams.swa_type);\n",
        "    return res;\n",
        "}\n",
        "\n",
        "llama_kv_cache_unified::llama_kv_cache_unified(\n",
        "        const llama_model &  model,\n",
        "          layer_filter_cb && filter,\n",
        "                ggml_type    type_k,\n",
        "                ggml_type    type_v,\n",
        "                     bool    v_trans,\n",
        "                     bool    offload,\n",
        "                 uint32_t    kv_size,\n",
        "                 uint32_t    n_seq_max,\n",
        "                 uint32_t    n_pad,\n",
        "                 uint32_t    n_swa,\n",
        "           llama_swa_type    swa_type) :\n",
        "    model(model), hparams(model.hparams), v_trans(v_trans),\n",
        "    n_seq_max(n_seq_max), n_pad(n_pad), n_swa(n_swa), swa_type(swa_type) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>class llama_kv_cache_unified</summary>\n",
        "\n",
        "```c++\n",
        "class llama_kv_cache_unified : public llama_memory_i {\n",
        "    using ubatch_heads = std::vector<uint32_t>;\n",
        "private:\n",
        "    const llama_model & model;\n",
        "    const llama_hparams & hparams;\n",
        "\n",
        "    struct kv_layer {\n",
        "        // layer index in the model\n",
        "        // note: can be different from the layer index in the KV cache\n",
        "        uint32_t il;\n",
        "\n",
        "        ggml_tensor * k;\n",
        "        ggml_tensor * v;\n",
        "    };\n",
        "\n",
        "    bool v_trans = true;  // the value tensor is transposed\n",
        "\n",
        "    // the current index from where we start searching for a free slot in the ring buffer of KV cells (see find_slot())\n",
        "    // note: this is not part of the KV state and it's only used to speed-up the find_slot() method\n",
        "    uint32_t head = 0;\n",
        "\n",
        "    const uint32_t n_seq_max = 1;\n",
        "\n",
        "    // required padding\n",
        "\n",
        "    const llama_swa_type swa_type = LLAMA_SWA_TYPE_NONE;\n",
        "\n",
        "    std::vector<ggml_context_ptr>        ctxs;\n",
        "    std::vector<ggml_backend_buffer_ptr> bufs;\n",
        "\n",
        "    llama_kv_cells_unified cells;\n",
        "\n",
        "    std::vector<kv_layer> layers;\n",
        "\n",
        "    // model layer id -> KV cache layer id\n",
        "    std::unordered_map<int32_t, int32_t> map_layer_ids;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary>class llama_kv_cells_unified</summary>\n",
        "\n",
        "```c++\n",
        "class llama_kv_cells_unified {\n",
        "private:\n",
        "    bool has_shift = false;\n",
        "\n",
        "    // set of indices of used cells (i.e. pos[i] != -1, allowed to not have any seq_id)\n",
        "    std::set<uint32_t> used;\n",
        "\n",
        "    std::vector<llama_pos> pos;\n",
        "\n",
        "    // this array accumulates any applied shifts to the pos array since the last reset_shift() call\n",
        "    // this is used to queue multiple updates to the pos array, which in the end can be applied in one go:\n",
        "    //\n",
        "    //   cells.pos_add(x, shift_x);\n",
        "    //   cells.pos_div(y, shift_y);\n",
        "    //   ...\n",
        "    //\n",
        "    //   if (cells.has_shift()) {\n",
        "    //      for (int i = 0; i < n; ++i) {\n",
        "    //          auto shift_i = cells.get_shift(i);\n",
        "    //          ...\n",
        "    //      }\n",
        "    //      cells.reset_shift();\n",
        "    //   }\n",
        "    //\n",
        "    std::vector<llama_pos> shift;\n",
        "\n",
        "    using bits_t = std::bitset<LLAMA_MAX_SEQ>;\n",
        "\n",
        "    // the bitset seq[i] tells us which sequences are currently occupying the i-th cell\n",
        "    std::vector<bits_t> seq;\n",
        "\n",
        "    // the set seq_pos[s] tells us which positions are currently present for sequence s\n",
        "    // this way seq_pos[s].begin() and seq_pos[s].rbegin() give us the min/max positions currently in the cache\n",
        "    std::set<llama_pos> seq_pos[LLAMA_MAX_SEQ];\n",
        "};\n",
        "\n",
        "// copy the state of cells [i, i + n) (used for save/restore the state of the cells)\n",
        "llama_kv_cells_unified cp(uint32_t i, uint32_t n) const {\n",
        "    llama_kv_cells_unified res;\n",
        "    res.resize(n);\n",
        "    for (uint32_t j = 0; j < n; ++j) {\n",
        "        res.pos[j] = pos[i + j];\n",
        "        res.seq[j] = seq[i + j];\n",
        "    }\n",
        "    return res;\n",
        "}\n",
        "\n",
        "// set the state of cells [i, i + other.pos.size()) (used for save/restore the state of the cells)\n",
        "void set(uint32_t i, const llama_kv_cells_unified & other) {\n",
        "    for (uint32_t j = 0; j < other.pos.size(); ++j) {\n",
        "        if (pos[i + j] == -1 && other.pos[j] != -1) {\n",
        "            used.insert(i + j);\n",
        "        }\n",
        "\n",
        "        if (pos[i + j] != -1 && other.pos[j] == -1) {\n",
        "            used.erase(i + j);\n",
        "        }\n",
        "\n",
        "        if (pos[i + j] != -1) {\n",
        "            seq_pos_rm(i + j);\n",
        "        }\n",
        "\n",
        "        pos[i + j] = other.pos[j];\n",
        "        seq[i + j] = other.seq[j];\n",
        "\n",
        "        if (pos[i + j] != -1) {\n",
        "            seq_pos_add(i + j);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "    // remove cell i\n",
        "    void seq_pos_rm(uint32_t i) {\n",
        "        for (int s = 0; s < LLAMA_MAX_SEQ; ++s) {\n",
        "            if (seq[i].test(s)) {\n",
        "                seq_pos[s].erase(pos[i]);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // add cell i\n",
        "    void seq_pos_add(uint32_t i) {\n",
        "        for (int s = 0; s < LLAMA_MAX_SEQ; ++s) {\n",
        "            if (seq[i].test(s)) {\n",
        "                seq_pos[s].insert(pos[i]);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // set the position of an empty cell\n",
        "    // does not modify \"has_shift\"\n",
        "    // note: call only if the cell is empty\n",
        "    void pos_set(uint32_t i, llama_pos p) {\n",
        "        pos[i] = p;\n",
        "        used.insert(i);\n",
        "    }\n",
        "\n",
        "    // note: call only if the cell is not empty and the seq_id is not in the cell\n",
        "    void seq_add(uint32_t i, llama_seq_id seq_id) {\n",
        "        seq[i].set(seq_id);\n",
        "        seq_pos[seq_id].insert(pos[i]);\n",
        "    }\n",
        "\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "// create a context for each buffer type\n",
        "std::map<ggml_backend_buffer_type_t, ggml_context *> ctx_map;\n",
        "auto ctx_for_buft = [&](ggml_backend_buffer_type_t buft) -> ggml_context * {\n",
        "    auto it = ctx_map.find(buft);\n",
        "    if (it == ctx_map.end()) {\n",
        "        ggml_init_params params = {\n",
        "            /*.mem_size   =*/ size_t(2u*hparams.n_layer*ggml_tensor_overhead()),\n",
        "            /*.mem_buffer =*/ NULL,\n",
        "            /*.no_alloc   =*/ true,\n",
        "        };\n",
        "        ggml_context * ctx = ggml_init(params);\n",
        "        ctx_map[buft] = ctx;\n",
        "        ctxs.emplace_back(ctx);\n",
        "        return ctx;\n",
        "    }\n",
        "    return it->second;\n",
        "};\n",
        "\n",
        "head = 0;\n",
        "// llama_kv_cache_unified:: llama_kv_cells_unified cells;\n",
        "cells.resize(kv_size);\n",
        "for (uint32_t il = 0; il < hparams.n_layer; il++)\n",
        "    const uint32_t n_embd_k_gqa = hparams.n_embd_k_gqa(il) + hparams.n_embd_k_s();\n",
        "    const uint32_t n_embd_v_gqa = hparams.n_embd_v_gqa(il) + hparams.n_embd_v_s();\n",
        "    auto * dev = model.dev_layer(il);\n",
        "    buft = ggml_backend_dev_buffer_type(dev);\n",
        "    dev_name = ggml_backend_dev_name(dev);\n",
        "    ggml_context * ctx = ctx_for_buft(buft);\n",
        "    ggml_tensor * k = ggml_new_tensor_2d(ctx, type_k, n_embd_k_gqa, kv_size);\n",
        "    ggml_tensor * v = ggml_new_tensor_2d(ctx, type_v, n_embd_v_gqa, kv_size);\n",
        "    ggml_format_name(k, \"cache_k_l%d\", il);\n",
        "    ggml_format_name(v, \"cache_v_l%d\", il);\n",
        "    // model layer id -> KV cache layer id\n",
        "    // llama_kv_cache_unified:: std::unordered_map<int32_t, int32_t> map_layer_ids;\n",
        "    map_layer_ids[il] = layers.size();\n",
        "    // llama_kv_cache_unified:: std::vector<kv_layer> layers;\n",
        "    layers.push_back({ il, k, v });\n",
        "\n",
        "// allocate tensors and initialize the buffers to avoid NaNs in the padding\n",
        "auto * buft = ctx_map[0].first;\n",
        "auto * ctx  = ctx_map[0].second;\n",
        "--> ggml_backend_alloc_ctx_tensors_from_buft -> buf;\n",
        "ggml_backend_buffer_clear(buf, 0);\n",
        "// llama_kv_cache_unified:: std::vector<ggml_backend_buffer_ptr> bufs;\n",
        "bufs.emplace_back(buf);\n",
        "```"
      ],
      "metadata": {
        "id": "CLwdfZ0uodJv"
      },
      "id": "CLwdfZ0uodJv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`ggml_backend_alloc_ctx_tensors_from_buft`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L987)\n",
        "\n",
        "\n",
        "```c++\n",
        "ggml_backend_buffer_t buf = ggml_backend_alloc_ctx_tensors_from_buft(ctx, buft);\n",
        "----------\n",
        "ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors_from_buft(struct ggml_context * ctx, ggml_backend_buffer_type_t buft) {}\n",
        "```\n"
      ],
      "metadata": {
        "id": "inozcLYst-Yw"
      },
      "id": "inozcLYst-Yw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "size_t alignment = ggml_backend_buft_get_alignment(buft);\n",
        "size_t max_size = ggml_backend_buft_get_max_size(buft);\n",
        "size_t cur_buf_size = 0;\n",
        "struct ggml_tensor * first = ggml_get_first_tensor(ctx);\n",
        "for (struct ggml_tensor * t = first; t != NULL; t = ggml_get_next_tensor(ctx, t))\n",
        "    size_t this_size = GGML_PAD(ggml_backend_buft_get_alloc_size(buft, t), alignment); --> ggml_nbytes(tensor)\n",
        "    cur_buf_size += this_size;\n",
        "// allocate remaining tensors\n",
        "--> alloc_tensor_range\n",
        "\n",
        "ggml_backend_buffer_t buffer = buffers[0];\n",
        "free(buffers);\n",
        "return buffer;\n",
        "```"
      ],
      "metadata": {
        "id": "PmO24k6rvpBu"
      },
      "id": "PmO24k6rvpBu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`alloc_tensor_range`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L946)\n",
        "\n",
        "```c++\n",
        "alloc_tensor_range(ctx, first, NULL, buft, cur_buf_size, &buffers, &n_buffers);\n",
        "----------\n",
        "static bool alloc_tensor_range(struct ggml_context * ctx,\n",
        "        struct ggml_tensor * first, struct ggml_tensor * last,\n",
        "        ggml_backend_buffer_type_t buft, size_t size,\n",
        "        ggml_backend_buffer_t ** buffers, size_t * n_buffers) {\n",
        "    // ggml_backend_buffer_t buffer = ggml_backend_buft_alloc_buffer(buft, size);\n",
        "    --> ggml_backend_buft_alloc_buffer --> ggml_backend_buffer_t buffer;\n",
        "    *buffers = realloc(*buffers, sizeof(ggml_backend_buffer_t) * (*n_buffers + 1));\n",
        "    (*buffers)[(*n_buffers)++] = buffer;\n",
        "    --> ggml_tallocr_new --> struct ggml_tallocr tallocr;\n",
        "    for (struct ggml_tensor * t = first; t != last; t = ggml_get_next_tensor(ctx, t))\n",
        "        --> ggml_tallocr_alloc;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "3DLqtWGMwl3m"
      },
      "id": "3DLqtWGMwl3m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_tallocr_new`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L77)\n",
        "\n",
        "<details>\n",
        "<summary>struct ggml_tallocr</summary>\n",
        "\n",
        "```c++\n",
        "// Tensor allocator\n",
        "struct ggml_tallocr {\n",
        "    ggml_backend_buffer_t buffer;\n",
        "    void * base;\n",
        "    size_t alignment;\n",
        "    size_t offset;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "struct ggml_tallocr tallocr = ggml_tallocr_new(buffer);\n",
        "----------\n",
        "struct ggml_tallocr ggml_tallocr_new(ggml_backend_buffer_t buffer) {\n",
        "    void * base = ggml_backend_buffer_get_base(buffer);\n",
        "    size_t align = ggml_backend_buffer_get_alignment(buffer);\n",
        "\n",
        "    assert(align && !(align & (align - 1))); // power of 2\n",
        "\n",
        "    struct ggml_tallocr talloc = (struct ggml_tallocr) {\n",
        "        /*.buffer    = */ buffer,\n",
        "        /*.base      = */ base,\n",
        "        /*.alignment = */ align,\n",
        "        /*.offset    = */ aligned_offset(base, 0, align),\n",
        "    };\n",
        "    return talloc;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "LaPoxENNx0kI"
      },
      "id": "LaPoxENNx0kI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_tallocr_alloc`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L92)\n",
        "\n",
        "```c++\n",
        "status = ggml_tallocr_alloc(&tallocr, t);\n",
        "----------\n",
        "enum ggml_status ggml_tallocr_alloc(struct ggml_tallocr * talloc, struct ggml_tensor * tensor) {\n",
        "    size_t size = ggml_backend_buffer_get_alloc_size(talloc->buffer, tensor);\n",
        "    size = GGML_PAD(size, talloc->alignment);\n",
        "\n",
        "    void * addr = (char *)ggml_backend_buffer_get_base(talloc->buffer) + talloc->offset;\n",
        "    talloc->offset += size;\n",
        "\n",
        "    assert(((uintptr_t)addr % talloc->alignment) == 0);\n",
        "\n",
        "    return ggml_backend_tensor_alloc(talloc->buffer, tensor, addr);\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "ukAvf36unQfN"
      },
      "id": "ukAvf36unQfN"
    },
    {
      "cell_type": "markdown",
      "id": "8c7da1ea-1d80-4200-8a17-00d86ace2407",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8c7da1ea-1d80-4200-8a17-00d86ace2407"
      },
      "source": [
        "### [`ggml_graph_nbytes`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L5961)\n",
        "\n",
        "```c++\n",
        "buf_compute_meta.resize(ggml_tensor_overhead()*max_nodes + ggml_graph_overhead_custom(max_nodes, false));\n",
        "----------\n",
        "size_t ggml_graph_overhead_custom(size_t size, bool grads) {\n",
        "    return GGML_OBJECT_SIZE + GGML_PAD(ggml_graph_nbytes(size, grads), GGML_MEM_ALIGN);\n",
        "}\n",
        "\n",
        "static size_t ggml_graph_nbytes(size_t size, bool grads) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4298b9-4dbc-4a12-9752-2297cb732f4c",
      "metadata": {
        "id": "8e4298b9-4dbc-4a12-9752-2297cb732f4c"
      },
      "source": [
        "```c++\n",
        "size_t hash_size = ggml_hash_size(size * 2);\n",
        "void * p = 0;\n",
        "incr_ptr_aligned(&p, sizeof(struct ggml_cgraph), 1);\n",
        "incr_ptr_aligned(&p, size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)); // nodes\n",
        "incr_ptr_aligned(&p, size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)); // leafs\n",
        "incr_ptr_aligned(&p, hash_size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)); // hash keys\n",
        "incr_ptr_aligned(&p, ggml_bitset_size(hash_size) * sizeof(ggml_bitset_t), sizeof(ggml_bitset_t));\n",
        "\n",
        "size_t nbytes = (size_t) p;\n",
        "return nbytes;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "234e48e9-d085-43ff-bae4-8a10ce230c0f",
      "metadata": {
        "id": "234e48e9-d085-43ff-bae4-8a10ce230c0f"
      },
      "source": [
        "### [`ggml_backend_sched_new`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1455)\n",
        "\n",
        "```c++\n",
        "sched.reset(ggml_backend_sched_new(backend_ptrs.data(), backend_buft.data(), backend_ptrs.size(), max_nodes, pipeline_parallel, cparams.op_offload));\n",
        "----------\n",
        "ggml_backend_sched_t ggml_backend_sched_new(\n",
        "        ggml_backend_t * backends,\n",
        "        ggml_backend_buffer_type_t * bufts,\n",
        "        int n_backends,\n",
        "        size_t graph_size,\n",
        "        bool parallel,\n",
        "        bool op_offload) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>struct ggml_backend_sched</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_backend_sched {\n",
        "    bool is_reset; // true if the scheduler has been reset since the last graph split\n",
        "    bool is_alloc;\n",
        "\n",
        "    int n_backends;\n",
        "\n",
        "    ggml_backend_t backends[GGML_SCHED_MAX_BACKENDS];\n",
        "    ggml_backend_buffer_type_t bufts[GGML_SCHED_MAX_BACKENDS];\n",
        "    ggml_gallocr_t galloc;\n",
        "\n",
        "    // hash map of the nodes in the graph\n",
        "    struct ggml_hash_set  hash_set;\n",
        "    int                 * hv_tensor_backend_ids; // [hash_set.size]\n",
        "    struct ggml_tensor ** hv_tensor_copies;      // [hash_set.size][n_backends][n_copies]\n",
        "\n",
        "    int * node_backend_ids; // [graph_size]\n",
        "    int * leaf_backend_ids; // [graph_size]\n",
        "\n",
        "    int * prev_node_backend_ids; // [graph_size]\n",
        "    int * prev_leaf_backend_ids; // [graph_size]\n",
        "\n",
        "    // copy of the graph with modified inputs\n",
        "    struct ggml_cgraph graph;\n",
        "\n",
        "    // graph splits\n",
        "    struct ggml_backend_sched_split * splits;\n",
        "    int n_splits;\n",
        "    int splits_capacity;\n",
        "\n",
        "    // pipeline parallelism support\n",
        "    int n_copies;\n",
        "    int cur_copy;\n",
        "    ggml_backend_event_t events[GGML_SCHED_MAX_BACKENDS][GGML_SCHED_MAX_COPIES];\n",
        "    struct ggml_tensor * graph_inputs[GGML_SCHED_MAX_SPLIT_INPUTS];\n",
        "    int n_graph_inputs;\n",
        "\n",
        "    struct ggml_context * ctx;\n",
        "\n",
        "    ggml_backend_sched_eval_callback callback_eval;\n",
        "    void * callback_eval_user_data;\n",
        "\n",
        "    char * context_buffer;\n",
        "    size_t context_buffer_size;\n",
        "\n",
        "    bool op_offload;\n",
        "\n",
        "    int debug;\n",
        "};\n",
        "\n",
        "typedef struct ggml_backend_sched * ggml_backend_sched_t;\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "struct ggml_backend_sched * sched = (ggml_backend_sched *) calloc(1, sizeof(struct ggml_backend_sched));\n",
        "sched->n_backends = n_backends;\n",
        "sched->n_copies = parallel ? GGML_SCHED_MAX_COPIES : 1;\n",
        "// initialize hash table\n",
        "// FIXME: needs to be size*2 to account for leafs (do it in graph_split instead)\n",
        "sched->hash_set    = ggml_hash_set_new(graph_size);\n",
        "sched->hv_tensor_backend_ids = (int *) malloc(sched->hash_set.size * sizeof(sched->hv_tensor_backend_ids[0]));\n",
        "sched->hv_tensor_copies      = (ggml_tensor **) malloc(sched->hash_set.size * sched->n_backends * sched->n_copies * sizeof(struct ggml_tensor *));\n",
        "const size_t ggml_sched_max_splits = graph_size; // at most there is one split for each node in the graph\n",
        "const size_t nodes_size = graph_size + ggml_sched_max_splits*GGML_SCHED_MAX_SPLIT_INPUTS*2;\n",
        "sched->node_backend_ids = (int *) calloc(nodes_size, sizeof(sched->node_backend_ids[0]));\n",
        "sched->leaf_backend_ids = (int *) calloc(nodes_size, sizeof(sched->leaf_backend_ids[0]));\n",
        "sched->prev_node_backend_ids = (int *) calloc(nodes_size, sizeof(sched->prev_node_backend_ids[0]));\n",
        "sched->prev_leaf_backend_ids = (int *) calloc(nodes_size, sizeof(sched->prev_leaf_backend_ids[0]));\n",
        "\n",
        "sched->context_buffer_size = ggml_sched_max_splits*GGML_SCHED_MAX_SPLIT_INPUTS*2*sizeof(struct ggml_tensor) + ggml_graph_overhead_custom(graph_size, false);\n",
        "sched->context_buffer = (char *) malloc(sched->context_buffer_size);\n",
        "\n",
        "const int initial_splits_capacity = 16;\n",
        "sched->splits = (ggml_backend_sched_split *) calloc(initial_splits_capacity, sizeof(sched->splits[0]));\n",
        "sched->splits_capacity = initial_splits_capacity;\n",
        "\n",
        "sched->backends[0] = backends[0];\n",
        "sched->bufts[0] = bufts[0];\n",
        "\n",
        "--> ggml_gallocr_new_n --> sched->galloc;\n",
        "sched->op_offload = op_offload;\n",
        "ggml_backend_sched_reset(sched);\n",
        "    // reset state for the next run\n",
        "    if (!sched->is_reset) {\n",
        "        ggml_hash_set_reset(&sched->hash_set);\n",
        "        memset(sched->hv_tensor_backend_ids, -1, sched->hash_set.size * sizeof(sched->hv_tensor_backend_ids[0]));\n",
        "        memset(sched->hv_tensor_copies,       0, sched->hash_set.size * sched->n_backends * sched->n_copies * sizeof(struct ggml_tensor *));\n",
        "        sched->is_reset = true;\n",
        "    }\n",
        "    sched->is_alloc = false;\n",
        "return sched;\n",
        "```\n"
      ],
      "metadata": {
        "id": "4GO0tWJL0zt5"
      },
      "id": "4GO0tWJL0zt5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`ggml_gallocr_new_n`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L380)\n",
        "\n",
        "<details>\n",
        "<summary>struct ggml_gallocr</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_gallocr {\n",
        "    ggml_backend_buffer_type_t * bufts; // [n_buffers]\n",
        "    ggml_backend_buffer_t * buffers; // [n_buffers]\n",
        "    struct ggml_dyn_tallocr ** buf_tallocs; // [n_buffers]\n",
        "    int n_buffers;\n",
        "\n",
        "    struct ggml_hash_set hash_set;\n",
        "    struct hash_node * hash_values; // [hash_set.size]\n",
        "\n",
        "    struct node_alloc * node_allocs; // [n_nodes]\n",
        "    int n_nodes;\n",
        "\n",
        "    struct leaf_alloc * leaf_allocs; // [n_leafs]\n",
        "    int n_leafs;\n",
        "};\n",
        "\n",
        "\n",
        "typedef struct ggml_gallocr * ggml_gallocr_t;\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "```c++\n",
        "sched->galloc = ggml_gallocr_new_n(sched->bufts, n_backends);\n",
        "----------\n",
        "ggml_gallocr_t ggml_gallocr_new_n(ggml_backend_buffer_type_t * bufts, int n_bufs) {\n",
        "    galloc->bufts = calloc(n_bufs, sizeof(ggml_backend_buffer_type_t));\n",
        "    galloc->buffers = calloc(n_bufs, sizeof(ggml_backend_buffer_t));\n",
        "    galloc->buf_tallocs = calloc(n_bufs, sizeof(struct ggml_dyn_tallocr *));\n",
        "    // for (int i = 0; i < n_bufs; i++)\n",
        "    galloc->bufts[0] = bufts[0];\n",
        "    galloc->buffers[0] = NULL;\n",
        "    size_t alignment = ggml_backend_buft_get_alignment(bufts[0]);\n",
        "    --> ggml_dyn_tallocr_new --> galloc->buf_tallocs[0];\n",
        "\n",
        "    galloc->n_buffers = n_bufs;\n",
        "    return galloc;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "ksw-c8aM4zCN"
      },
      "id": "ksw-c8aM4zCN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_dyn_tallocr_new`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L310)\n",
        "\n",
        "<details>\n",
        "<summary>struct ggml_dyn_tallocr</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_dyn_tallocr {\n",
        "    size_t alignment;\n",
        "    int n_free_blocks;\n",
        "    struct free_block free_blocks[MAX_FREE_BLOCKS];\n",
        "    size_t max_size;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "galloc->buf_tallocs[i] = ggml_dyn_tallocr_new(alignment);\n",
        "----------\n",
        "static struct ggml_dyn_tallocr * ggml_dyn_tallocr_new(size_t alignment) {\n",
        "    struct ggml_dyn_tallocr * alloc = (struct ggml_dyn_tallocr *)malloc(sizeof(struct ggml_dyn_tallocr));\n",
        "\n",
        "    *alloc = (struct ggml_dyn_tallocr) {\n",
        "        /*.alignment     = */ alignment,\n",
        "        /*.n_free_blocks = */ 0,\n",
        "        /*.free_blocks   = */ {{0}},\n",
        "        /*.max_size      = */ 0,\n",
        "    };\n",
        "\n",
        "    ggml_dyn_tallocr_reset(alloc);\n",
        "        alloc->n_free_blocks = 1;\n",
        "        alloc->free_blocks[0].offset = 0;\n",
        "        alloc->free_blocks[0].size = SIZE_MAX/2; // restrict maximum size of a measure allocator to half size_t max to avoid overflows\n",
        "        alloc->max_size = 0;\n",
        "    return alloc;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "DAlj_2O_5XTi"
      },
      "id": "DAlj_2O_5XTi"
    },
    {
      "cell_type": "markdown",
      "id": "300c4e74-0310-4b08-98f4-ac271b0818d3",
      "metadata": {
        "id": "300c4e74-0310-4b08-98f4-ac271b0818d3"
      },
      "source": [
        "### [`llama_kv_cache_unified_state::llama_kv_cache_unified_state`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L1668)\n",
        "\n",
        "<details>\n",
        "<summary>class llama_kv_cache_unified_state</summary>\n",
        "\n",
        "```c++\n",
        "class llama_kv_cache_unified_state : public llama_memory_state_i {\n",
        "    using ubatch_heads = llama_kv_cache_unified::ubatch_heads;\n",
        "private:\n",
        "    llama_memory_status status;\n",
        "\n",
        "    llama_kv_cache_unified * kv;\n",
        "    llama_context * lctx;\n",
        "\n",
        "    //\n",
        "    // update state\n",
        "    //\n",
        "\n",
        "    bool do_shift = false;\n",
        "\n",
        "    defrag_info dinfo;\n",
        "\n",
        "    //\n",
        "    // batch processing state\n",
        "    //\n",
        "\n",
        "    llama_sbatch sbatch;\n",
        "\n",
        "    // the index of the next ubatch to process\n",
        "    size_t i_next = 0;\n",
        "\n",
        "    ubatch_heads heads;\n",
        "\n",
        "    std::vector<llama_ubatch> ubatches;\n",
        "\n",
        "    //\n",
        "    // data needed for building the compute graph for the current ubatch:\n",
        "    //\n",
        "\n",
        "    // a heuristic, to avoid attending the full cache if it is not yet utilized\n",
        "    // as the cache gets filled, the benefit from this heuristic disappears\n",
        "    int32_t n_kv;\n",
        "\n",
        "    // the beginning of the current slot in which the ubatch will be inserted\n",
        "    int32_t head;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "const auto mstate = memory->init_full();\n",
        "----------\n",
        "llama_memory_state_ptr llama_kv_cache_unified::init_full() {\n",
        "    --> return std::make_unique<llama_kv_cache_unified_state>(this);\n",
        "}\n",
        "\n",
        "class llama_kv_cache_unified_state : public llama_memory_state_i {\n",
        "public:\n",
        "    // used to create a full-cache state\n",
        "    --> llama_kv_cache_unified_state(llama_kv_cache_unified * kv);\n",
        "}\n",
        "\n",
        "llama_kv_cache_unified_state::llama_kv_cache_unified_state(llama_kv_cache_unified * kv) : status(LLAMA_MEMORY_STATUS_SUCCESS), kv(kv) {\n",
        "    n_kv = kv->get_size();\n",
        "    head = 0;\n",
        "}           \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebdc1a12-1a27-4be5-9673-58f3d51d7c5c",
      "metadata": {
        "id": "ebdc1a12-1a27-4be5-9673-58f3d51d7c5c"
      },
      "source": [
        "### [`llama_context::graph_reserve`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L1305)\n",
        "\n",
        "```c++\n",
        "auto * gf = graph_reserve(n_tokens, n_seqs, n_tokens, mstate.get());\n",
        "auto * gf = graph_reserve(1, 1, 1, mstate.get());\n",
        "\n",
        "ggml_cgraph * llama_context::graph_reserve(uint32_t n_tokens, uint32_t n_seqs, uint32_t n_outputs, const llama_memory_state_i * mstate) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "this->n_outputs = n_outputs;\n",
        "llama_token token = model.vocab.token_bos(); // not actually used by llama_build_graph, but required to choose between token and embedding inputs graph\n",
        "llama_ubatch ubatch = { true, n_tokens, n_tokens / n_seqs, n_seqs, &token, nullptr, nullptr, nullptr, nullptr, nullptr};\n",
        "\n",
        "--> llama_context::graph_init --> ggml_cgraph * gf;\n",
        "--> llama_context::graph_build --> llama_model::build_graph --> llm_build_llama::llm_build_llama --> std::unique_ptr<llm_graph_result> res;\n",
        "\n",
        "ggml_backend_sched_reset(sched.get());\n",
        "\n",
        "// initialize scheduler with the specified graph\n",
        "--> ggml_backend_sched_reserve;\n",
        "return gf;\n",
        "```"
      ],
      "metadata": {
        "id": "PcT9obSo9i6T"
      },
      "id": "PcT9obSo9i6T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`llama_context::graph_init`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L1319)\n",
        "\n",
        "```c++\n",
        "auto * gf = graph_init();\n",
        "----------\n",
        "ggml_cgraph * llama_context::graph_init() {}\n",
        "```"
      ],
      "metadata": {
        "id": "F2yre4rS_tfl"
      },
      "id": "F2yre4rS_tfl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>struct ggml_cgraph</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_cgraph {\n",
        "    int size;    // maximum number of nodes/leafs/grads/grad_accs\n",
        "    int n_nodes; // number of nodes currently in use\n",
        "    int n_leafs; // number of leafs currently in use\n",
        "\n",
        "    struct ggml_tensor ** nodes;     // tensors with data that can change if the graph is evaluated\n",
        "    struct ggml_tensor ** grads;     // the outputs of these tensors are the gradients of the nodes\n",
        "    struct ggml_tensor ** grad_accs; // accumulators for node gradients\n",
        "    struct ggml_tensor ** leafs;     // tensors with constant data\n",
        "\n",
        "    struct ggml_hash_set visited_hash_set;\n",
        "\n",
        "    enum ggml_cgraph_eval_order order;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "ggml_init_params params = {\n",
        "    /*.mem_size   =*/ buf_compute_meta.size(),\n",
        "    /*.mem_buffer =*/ buf_compute_meta.data(),\n",
        "    /*.no_alloc   =*/ true,\n",
        "};\n",
        "// llama_context:: ggml_context_ptr ctx_compute;\n",
        "ctx_compute.reset(ggml_init(params));\n",
        "--> return ggml_new_graph_custom;\n",
        "```"
      ],
      "metadata": {
        "id": "WriaqPmAABfH"
      },
      "id": "WriaqPmAABfH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`ggml_new_graph_custom`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L5986)\n",
        "\n",
        "\n",
        "```c++\n",
        "return ggml_new_graph_custom(ctx_compute.get(), graph_max_nodes(), false);\n",
        "----------\n",
        "struct ggml_cgraph * ggml_new_graph_custom(struct ggml_context * ctx, size_t size, bool grads) {\n",
        "    const size_t obj_size = ggml_graph_nbytes(size, grads);\n",
        "    struct ggml_object * obj = ggml_new_object(ctx, GGML_OBJECT_TYPE_GRAPH, obj_size);\n",
        "    struct ggml_cgraph * cgraph = (struct ggml_cgraph *) ((char *) ctx->mem_buffer + obj->offs);\n",
        "\n",
        "    // the size of the hash table is doubled since it needs to hold both nodes and leafs\n",
        "    size_t hash_size = ggml_hash_size(size * 2);\n",
        "\n",
        "    void * p = cgraph + 1;\n",
        "\n",
        "    struct ggml_tensor ** nodes_ptr     =         incr_ptr_aligned(&p, size      * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *));\n",
        "    struct ggml_tensor ** leafs_ptr     =         incr_ptr_aligned(&p, size      * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *));\n",
        "    struct ggml_tensor ** hash_keys_ptr =         incr_ptr_aligned(&p, hash_size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *));\n",
        "    struct ggml_tensor ** grads_ptr     = grads ? incr_ptr_aligned(&p, hash_size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)) : NULL;\n",
        "    struct ggml_tensor ** grad_accs_ptr = grads ? incr_ptr_aligned(&p, hash_size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)) : NULL;\n",
        "\n",
        "    ggml_bitset_t * hash_used = incr_ptr_aligned(&p, ggml_bitset_size(hash_size) * sizeof(ggml_bitset_t), sizeof(ggml_bitset_t));\n",
        "\n",
        "    *cgraph = (struct ggml_cgraph) {\n",
        "        /*.size         =*/ size,\n",
        "        /*.n_nodes      =*/ 0,\n",
        "        /*.n_leafs      =*/ 0,\n",
        "        /*.nodes        =*/ nodes_ptr,\n",
        "        /*.grads        =*/ grads_ptr,\n",
        "        /*.grad_accs    =*/ grad_accs_ptr,\n",
        "        /*.leafs        =*/ leafs_ptr,\n",
        "        /*.hash_table   =*/ { hash_size, hash_used, hash_keys_ptr },\n",
        "        /*.order        =*/ GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT,\n",
        "    };\n",
        "\n",
        "    ggml_hash_set_reset(&cgraph->visited_hash_set);\n",
        "    return cgraph;\n",
        "}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "mKZYl3SVHSMe"
      },
      "id": "mKZYl3SVHSMe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`llm_build_llama::llm_build_llama`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#L4686)\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary>class llm_graph_result</summary>\n",
        "\n",
        "```c++\n",
        "//\n",
        "// llm_graph_result\n",
        "//\n",
        "\n",
        "// these objects deliver the result from the graph build process back to the llama_context\n",
        "// note that the input tensors created for the graph are referenced here - the goal is to be able to populate their\n",
        "//   specific data, by calling the set_inputs() method\n",
        "// along with the input tensors, the object also provides commonly used outputs tensors, such as logits, embeddings, etc.\n",
        "//   these are used by the llama_context to extact the relevant data, based on the compute parameters\n",
        "\n",
        "class llm_graph_result_i {\n",
        "public:\n",
        "    virtual ~llm_graph_result_i() = default;\n",
        "\n",
        "    virtual ggml_tensor * get_tokens()      = 0;\n",
        "    virtual ggml_tensor * get_logits()      = 0;\n",
        "    virtual ggml_tensor * get_embd()        = 0;\n",
        "    virtual ggml_tensor * get_embd_pooled() = 0;\n",
        "\n",
        "    virtual void set_inputs(const llama_ubatch * ubatch) = 0;\n",
        "};\n",
        "\n",
        "using llm_graph_result_ptr = std::unique_ptr<llm_graph_result_i>;\n",
        "\n",
        "class llm_graph_result : public llm_graph_result_i {\n",
        "public:\n",
        "    // important graph nodes\n",
        "    ggml_tensor * t_tokens      = nullptr;\n",
        "    ggml_tensor * t_logits      = nullptr;\n",
        "    ggml_tensor * t_embd        = nullptr;\n",
        "    ggml_tensor * t_embd_pooled = nullptr;\n",
        "\n",
        "    std::vector<llm_graph_input_ptr> inputs;\n",
        "}\n",
        "\n",
        "    void set_inputs(const llama_ubatch * ubatch) override {\n",
        "        for (auto & input : inputs) {\n",
        "            input->set_input(ubatch);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    llm_graph_input_i * add_input(llm_graph_input_ptr input) {\n",
        "        inputs.emplace_back(std::move(input));\n",
        "        return inputs.back().get();\n",
        "    }\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>struct llm_graph_context</summary>\n",
        "\n",
        "```c++\n",
        "struct llm_graph_context {\n",
        "    const llm_arch arch;\n",
        "\n",
        "    const llama_hparams & hparams;\n",
        "    const llama_cparams & cparams;\n",
        "    const llama_ubatch  & ubatch;\n",
        "\n",
        "    const int64_t n_embd;\n",
        "    const int64_t n_layer;\n",
        "    const int64_t n_rot;\n",
        "    const int64_t n_ctx;       // user-specified context size (can be different from n_ctx_train)\n",
        "    const int64_t n_head;\n",
        "    const int64_t n_head_kv;\n",
        "    const int64_t n_embd_head_k;\n",
        "    const int64_t n_embd_k_gqa;\n",
        "    const int64_t n_embd_head_v;\n",
        "    const int64_t n_embd_v_gqa;\n",
        "    const int64_t n_expert;\n",
        "    const int64_t n_expert_used;\n",
        "\n",
        "    const float freq_base;\n",
        "    const float freq_scale;\n",
        "    const float ext_factor;\n",
        "    const float attn_factor;\n",
        "    const float beta_fast;\n",
        "    const float beta_slow;\n",
        "    const float norm_eps;\n",
        "    const float norm_rms_eps;\n",
        "\n",
        "    const int32_t n_tokens;\n",
        "    const int32_t n_outputs;\n",
        "    const int32_t n_ctx_orig; // yarn\n",
        "\n",
        "    const enum llama_pooling_type pooling_type;\n",
        "    const enum llama_rope_type    rope_type;\n",
        "\n",
        "    ggml_context * ctx0 = nullptr;\n",
        "\n",
        "    ggml_backend_sched_t sched;\n",
        "\n",
        "    ggml_backend_t backend_cpu; // TODO: needed by build_attn_mha, figure out a way to remove?\n",
        "\n",
        "    const llama_adapter_cvec   * cvec;\n",
        "    const llama_adapter_loras  * loras;\n",
        "    const llama_memory_state_i * mstate;\n",
        "    const llama_cross          * cross;\n",
        "\n",
        "    const llm_graph_cb & cb_func;\n",
        "\n",
        "    std::unique_ptr<llm_graph_result> res;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "```c++\n",
        "auto res = graph_build(ctx_compute.get(), gf, ubatch, LLM_GRAPH_TYPE_DEFAULT, mstate);\n",
        "----------\n",
        "llm_graph_result_ptr llama_context::graph_build(\n",
        "                    ggml_context * ctx,\n",
        "                     ggml_cgraph * gf,\n",
        "              const llama_ubatch & ubatch,\n",
        "                  llm_graph_type   gtype,\n",
        "      const llama_memory_state_i * mstate) {\n",
        "      --> return model.build_graph(\n",
        "            {\n",
        "                /*.ctx         =*/ ctx,\n",
        "                /*.arch        =*/ model.arch,\n",
        "                /*.hparams     =*/ model.hparams,\n",
        "                /*.cparams     =*/ cparams,\n",
        "                /*.ubatch      =*/ ubatch,\n",
        "                /*.sched       =*/ sched.get(),\n",
        "                /*.backend_cpu =*/ backend_cpu,\n",
        "                /*.cvec        =*/ &cvec,\n",
        "                /*.loras       =*/ &loras,\n",
        "                /*.mstate      =*/ mstate,\n",
        "                /*.cross       =*/ &cross,\n",
        "                /*.n_outputs   =*/ n_outputs,\n",
        "                /*.cb          =*/ graph_get_cb(),\n",
        "            }, gf, gtype);\n",
        "}\n",
        "\n",
        "llm_graph_result_ptr llama_model::build_graph(\n",
        "        const llm_graph_params & params,\n",
        "                   ggml_cgraph * gf,\n",
        "                llm_graph_type   type) const {\n",
        "    --> std::unique_ptr<llm_graph_context> llm = std::make_unique<llm_build_llama>(*this, params, gf);\n",
        "    return std::move(llm->res);\n",
        "}\n",
        "\n",
        "struct llm_build_llama : public llm_graph_context {\n",
        "    llm_build_llama(const llama_model & model, const llm_graph_params & params, ggml_cgraph * gf) : llm_graph_context(params) {}\n",
        "};\n",
        "```"
      ],
      "metadata": {
        "id": "YkLEyS7AAgLL"
      },
      "id": "YkLEyS7AAgLL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>class llm_graph_input_i</summary>\n",
        "\n",
        "```c++\n",
        "class llm_graph_input_i {\n",
        "public:\n",
        "    virtual ~llm_graph_input_i() = default;\n",
        "\n",
        "    virtual void set_input(const llama_ubatch * ubatch) = 0;\n",
        "};\n",
        "\n",
        "using llm_graph_input_ptr = std::unique_ptr<llm_graph_input_i>;\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "const int64_t n_embd_head = hparams.n_embd_head_v;\n",
        "\n",
        "ggml_tensor * cur;\n",
        "ggml_tensor * inpL;\n",
        "\n",
        "--> build_inp_embd --> inpL;\n",
        "// inp_pos - contains the positions\n",
        "--> build_inp_pos --> ggml_tensor * inp_pos;\n",
        "--> build_attn_inp_kv_unified --> auto * inp_attn;\n",
        "\n",
        "const float kq_scale = hparams.f_attention_scale == 0.0f ? 1.0f/sqrtf(float(n_embd_head)) : hparams.f_attention_scale;\n",
        "\n",
        "for (int il = 0; il < n_layer; ++il) {\n",
        "    ggml_tensor * inpSA = inpL;\n",
        "\n",
        "    // norm\n",
        "    cur = build_norm(inpL,\n",
        "            model.layers[il].attn_norm, NULL,\n",
        "            LLM_NORM_RMS, il);\n",
        "    cb(cur, \"attn_norm\", il);\n",
        "\n",
        "    // self-attention\n",
        "    {\n",
        "        // rope freq factors for llama3; may return nullptr for llama2 and other models\n",
        "        ggml_tensor * rope_factors = model.get_rope_factors(cparams, il);\n",
        "\n",
        "        // compute Q and K and RoPE them\n",
        "        ggml_tensor * Qcur = build_lora_mm(model.layers[il].wq, cur);\n",
        "        cb(Qcur, \"Qcur\", il);\n",
        "        ggml_tensor * Kcur = build_lora_mm(model.layers[il].wk, cur);\n",
        "        cb(Kcur, \"Kcur\", il);\n",
        "        ggml_tensor * Vcur = build_lora_mm(model.layers[il].wv, cur);\n",
        "        cb(Vcur, \"Vcur\", il);\n",
        "        Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);\n",
        "        Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);\n",
        "        Vcur = ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens);\n",
        "        Qcur = ggml_rope_ext(\n",
        "                ctx0, Qcur, inp_pos, rope_factors,\n",
        "                n_rot, rope_type, n_ctx_orig, freq_base, freq_scale,\n",
        "                ext_factor, attn_factor, beta_fast, beta_slow\n",
        "                );\n",
        "        Kcur = ggml_rope_ext(\n",
        "                ctx0, Kcur, inp_pos, rope_factors,\n",
        "                n_rot, rope_type, n_ctx_orig, freq_base, freq_scale,\n",
        "                ext_factor, attn_factor, beta_fast, beta_slow\n",
        "                );\n",
        "        cb(Qcur, \"Qcur\", il);\n",
        "        cb(Kcur, \"Kcur\", il);\n",
        "        cb(Vcur, \"Vcur\", il);\n",
        "        --> cur = build_attn(inp_attn, gf,\n",
        "                model.layers[il].wo, model.layers[il].bo,\n",
        "                Qcur, Kcur, Vcur, nullptr, nullptr, kq_scale, il);\n",
        "        cb(cur, \"attn_out\", il);\n",
        "    }\n",
        "\n",
        "    if (il == n_layer - 1) {\n",
        "        // skip computing output for unused tokens\n",
        "        ggml_tensor * inp_out_ids = build_inp_out_ids();\n",
        "        cur   = ggml_get_rows(ctx0,   cur, inp_out_ids);\n",
        "        inpSA = ggml_get_rows(ctx0, inpSA, inp_out_ids);\n",
        "    }\n",
        "\n",
        "    ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n",
        "    cb(ffn_inp, \"ffn_inp\", il);\n",
        "    // feed-forward network (non-MoE)\n",
        "    cur = build_norm(ffn_inp,\n",
        "            model.layers[il].ffn_norm, NULL,\n",
        "            LLM_NORM_RMS, il);\n",
        "    cb(cur, \"ffn_norm\", il);\n",
        "\n",
        "    cur = build_ffn(cur,\n",
        "            model.layers[il].ffn_up,   model.layers[il].ffn_up_b,   NULL,\n",
        "            model.layers[il].ffn_gate, model.layers[il].ffn_gate_b, NULL,\n",
        "            model.layers[il].ffn_down, model.layers[il].ffn_down_b, NULL,\n",
        "            NULL,\n",
        "            LLM_FFN_SILU, LLM_FFN_PAR, il);\n",
        "    cb(cur, \"ffn_out\", il);\n",
        "    cur = ggml_add(ctx0, cur, ffn_inp);\n",
        "    cb(cur, \"ffn_out\", il);\n",
        "    cur = build_cvec(cur, il);\n",
        "    cb(cur, \"l_out\", il);\n",
        "    // input for next layer\n",
        "    inpL = cur;\n",
        "}\n",
        "\n",
        "cur = inpL;\n",
        "\n",
        "cur = build_norm(cur,\n",
        "        model.output_norm, NULL,\n",
        "        LLM_NORM_RMS, -1);\n",
        "\n",
        "cb(cur, \"result_norm\", -1);\n",
        "res->t_embd = cur;\n",
        "\n",
        "// lm_head\n",
        "cur = build_lora_mm(model.output, cur);\n",
        "\n",
        "cb(cur, \"result_output\", -1);\n",
        "res->t_logits = cur;\n",
        "\n",
        "--> ggml_build_forward_expand --> ggml_build_forward_impl --> ggml_visit_parents;\n",
        "```"
      ],
      "metadata": {
        "id": "0gjc_f94Bhyi"
      },
      "id": "0gjc_f94Bhyi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`llm_graph_context::build_inp_embd`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-graph.cpp#L847)\n",
        "\n",
        "<details>\n",
        "<summary>class llm_graph_input_embd</summary>\n",
        "\n",
        "```c++\n",
        "class llm_graph_input_embd : public llm_graph_input_i {\n",
        "public:\n",
        "    ggml_tensor * tokens = nullptr; // I32 [n_batch]\n",
        "    ggml_tensor * embd   = nullptr; // F32 [n_embd, n_batch]\n",
        "};\n",
        "\n",
        "\n",
        "void llm_graph_input_embd::set_input(const llama_ubatch * ubatch) {\n",
        "    const int64_t n_tokens = ubatch->n_tokens;\n",
        "    ggml_backend_tensor_set(tokens, ubatch->token, 0, n_tokens*ggml_element_size(tokens));\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "inpL = build_inp_embd(model.tok_embd);\n",
        "----------\n",
        "// input embeddings with optional lora\n",
        "ggml_tensor * llm_graph_context::build_inp_embd(ggml_tensor * tok_embd) const {}\n",
        "```"
      ],
      "metadata": {
        "id": "w--wDN_kJMJm"
      },
      "id": "w--wDN_kJMJm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "const int64_t n_embd = hparams.n_embd;\n",
        "auto inp = std::make_unique<llm_graph_input_embd>();\n",
        "inp->tokens = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, ubatch.n_tokens);\n",
        "ggml_set_input(inp->tokens);\n",
        "    tensor->flags |= GGML_TENSOR_FLAG_INPUT;\n",
        "res->t_tokens = inp->tokens;\n",
        "ggml_tensor * cur = ggml_get_rows(ctx0, tok_embd, inp->tokens);\n",
        "cb(cur, \"inp_embd\", -1);\n",
        "res->add_input(std::move(inp));\n",
        "return cur;\n",
        "```"
      ],
      "metadata": {
        "id": "0egbXjv1T0jK"
      },
      "id": "0egbXjv1T0jK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`llm_graph_context::build_inp_pos`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-graph.cpp#L898)\n",
        "\n",
        "<details>\n",
        "<summary>class llm_graph_input_pos</summary>\n",
        "\n",
        "```c++\n",
        "class llm_graph_input_pos : public llm_graph_input_i {\n",
        "public:\n",
        "    ggml_tensor * pos = nullptr; // I32 [n_batch]\n",
        "    const int64_t n_pos_per_embd = 1;\n",
        "};\n",
        "\n",
        "void llm_graph_input_pos::set_input(const llama_ubatch * ubatch) {\n",
        "    // if (ubatch->pos && pos)\n",
        "    const int64_t n_tokens = ubatch->n_tokens;\n",
        "    ggml_backend_tensor_set(pos, ubatch->pos, 0, n_tokens*n_pos_per_embd*ggml_element_size(pos));\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "// inp_pos - contains the positions\n",
        "ggml_tensor * inp_pos = build_inp_pos();\n",
        "----------\n",
        "ggml_tensor * llm_graph_context::build_inp_pos() const {}\n",
        "```"
      ],
      "metadata": {
        "id": "ADE3KY3bLEHM"
      },
      "id": "ADE3KY3bLEHM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "auto inp = std::make_unique<llm_graph_input_pos>(n_pos_per_embd());\n",
        "auto & cur = inp->pos;\n",
        "cur = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, n_tokens*n_pos_per_embd());\n",
        "ggml_set_input(cur);\n",
        "res->add_input(std::move(inp));\n",
        "return cur;\n",
        "```"
      ],
      "metadata": {
        "id": "PwO2B7uhUGTX"
      },
      "id": "PwO2B7uhUGTX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`llm_graph_context::build_attn_inp_kv_unified`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-graph.cpp#L1224)\n",
        "\n",
        "<details>\n",
        "<summary>llm_graph_input_attn_kv_unified</summary>\n",
        "\n",
        "```c++\n",
        "class llm_graph_input_attn_kv_unified : public llm_graph_input_i {\n",
        "public:\n",
        "    ggml_tensor * self_kq_mask     = nullptr; // F32 [n_kv, n_batch]\n",
        "    ggml_tensor * self_kq_mask_cnv = nullptr; //     [n_kv, n_batch]\n",
        "\n",
        "    const llama_hparams & hparams;\n",
        "    const llama_cparams & cparams;\n",
        "\n",
        "    const llama_kv_cache_unified_state * kv_state;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "```c++\n",
        "auto * inp_attn = build_attn_inp_kv_unified();\n",
        "----------\n",
        "llm_graph_input_attn_kv_unified * llm_graph_context::build_attn_inp_kv_unified() const {}\n",
        "```"
      ],
      "metadata": {
        "id": "FAdQS1MRL7_n"
      },
      "id": "FAdQS1MRL7_n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "const auto * kv_state = static_cast<const llama_kv_cache_unified_state *>(mstate);\n",
        "auto inp = std::make_unique<llm_graph_input_attn_kv_unified>(hparams, cparams, kv_state);\n",
        "const auto n_kv = kv_state->get_n_kv();\n",
        "inp->self_kq_mask = ggml_new_tensor_2d(ctx0, GGML_TYPE_F32, n_kv, GGML_PAD(n_tokens, GGML_KQ_MASK_PAD));\n",
        "ggml_set_input(inp->self_kq_mask);\n",
        "inp->self_kq_mask_cnv = cparams.flash_attn ? ggml_cast(ctx0, inp->self_kq_mask, GGML_TYPE_F16) : inp->self_kq_mask;\n",
        "return (llm_graph_input_attn_kv_unified *) res->add_input(std::move(inp));\n",
        "```"
      ],
      "metadata": {
        "id": "uI4t1QuKUX9l"
      },
      "id": "uI4t1QuKUX9l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`llm_graph_context::build_inp_out_ids`](src/llama-graph.cpp#L925)\n",
        "\n",
        "```c++\n",
        "ggml_tensor * inp_out_ids = build_inp_out_ids();\n",
        "----------\n",
        "ggml_tensor * llm_graph_context::build_inp_out_ids() const {}\n",
        "```"
      ],
      "metadata": {
        "id": "nzoZ8n6-fChr"
      },
      "id": "nzoZ8n6-fChr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>class llm_graph_input_out_ids</summary>\n",
        "\n",
        "```c++\n",
        "class llm_graph_input_out_ids : public llm_graph_input_i {\n",
        "public:\n",
        "    ggml_tensor * out_ids; // I32 [n_outputs]\n",
        "\n",
        "    const llama_hparams & hparams;\n",
        "    const llama_cparams & cparams;\n",
        "\n",
        "    const int32_t n_outputs;\n",
        "};\n",
        "\n",
        "\n",
        "void llm_graph_input_out_ids::set_input(const llama_ubatch * ubatch) {\n",
        "    const int64_t n_tokens = ubatch->n_tokens;\n",
        "\n",
        "    int32_t * data = (int32_t *) out_ids->data;\n",
        "\n",
        "    if (n_outputs == n_tokens) {\n",
        "        for (int i = 0; i < n_tokens; ++i) {\n",
        "            data[i] = i;\n",
        "        }\n",
        "    } else if (ubatch->output) {\n",
        "        int32_t n_outputs = 0;\n",
        "        for (int i = 0; i < n_tokens; ++i) {\n",
        "            if (ubatch->output[i]) {\n",
        "                data[n_outputs++] = i;\n",
        "            }\n",
        "        }\n",
        "    } else if (n_outputs == 1) {\n",
        "        // only keep last output\n",
        "        data[0] = n_tokens - 1;\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "auto inp = std::make_unique<llm_graph_input_out_ids>(hparams, cparams, n_outputs);\n",
        "auto & cur = inp->out_ids;\n",
        "cur = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, n_outputs);\n",
        "ggml_set_input(cur);\n",
        "res->add_input(std::move(inp));\n",
        "return cur;\n",
        "```"
      ],
      "metadata": {
        "id": "prMJp3BKfRGE"
      },
      "id": "prMJp3BKfRGE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`llm_graph_context::build_attn`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-graph.cpp#L1255)\n",
        "\n",
        "\n",
        "```c++\n",
        "cur = build_attn(inp_attn, gf,\n",
        "                model.layers[il].wo, model.layers[il].bo,\n",
        "                Qcur, Kcur, Vcur, nullptr, nullptr, kq_scale, il);\n",
        "----------\n",
        "ggml_tensor * llm_graph_context::build_attn(\n",
        "        llm_graph_input_attn_kv_unified * inp,\n",
        "        ggml_cgraph * gf,\n",
        "        ggml_tensor * wo,\n",
        "        ggml_tensor * wo_b,\n",
        "        ggml_tensor * q_cur,\n",
        "        ggml_tensor * k_cur,\n",
        "        ggml_tensor * v_cur,\n",
        "        ggml_tensor * kq_b,\n",
        "        ggml_tensor * v_mla,\n",
        "            float     kq_scale,\n",
        "            int       il) const {}\n",
        "```"
      ],
      "metadata": {
        "id": "2bQB42zMPNdj"
      },
      "id": "2bQB42zMPNdj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "// these nodes are added to the graph together so that they are not reordered\n",
        "// by doing so, the number of splits in the graph is reduced\n",
        "ggml_build_forward_expand(gf, q_cur);\n",
        "ggml_build_forward_expand(gf, k_cur);\n",
        "ggml_build_forward_expand(gf, v_cur);\n",
        "\n",
        "const auto * kv_state = static_cast<const llama_kv_cache_unified_state *>(mstate);\n",
        "\n",
        "// store to KV cache\n",
        "ggml_build_forward_expand(gf, kv_state->cpy_k(ctx0, k_cur, il));\n",
        "ggml_build_forward_expand(gf, kv_state->cpy_v(ctx0, v_cur, il));\n",
        "\n",
        "const auto & kq_mask = inp->get_kq_mask();\n",
        "\n",
        "ggml_tensor * q = q_cur;\n",
        "ggml_tensor * k = kv_state->get_k(ctx0, il);\n",
        "ggml_tensor * v = kv_state->get_v(ctx0, il);\n",
        "\n",
        "--> lm_graph_context::build_attn_mha --> ggml_tensor * cur;\n",
        "cb(cur, \"kqv_out\", il);\n",
        "cur = build_lora_mm(wo, cur);\n",
        "return cur;\n",
        "```"
      ],
      "metadata": {
        "id": "HtLNrXSJVJhh"
      },
      "id": "HtLNrXSJVJhh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [`llama_kv_cache_unified_state::cpy_k`]()\n",
        "- [`llama_kv_cache_unified_state::cpy_v`]()\n",
        "- [`ggml_cpy_impl`]()\n",
        "\n",
        "```c++\n",
        "kv_state->cpy_k(ctx0, k_cur, il);\n",
        "kv_state->cpy_v(ctx0, v_cur, il);\n",
        "----------\n",
        "ggml_tensor * llama_kv_cache_unified_state::cpy_k(ggml_context * ctx, ggml_tensor * k_cur, int32_t il) const\n",
        "    return kv->cpy_k(ctx, k_cur, il, head);\n",
        "\n",
        "ggml_tensor * llama_kv_cache_unified_state::cpy_v(ggml_context * ctx, ggml_tensor * v_cur, int32_t il) const\n",
        "    return kv->cpy_v(ctx, v_cur, il, head);\n",
        "\n",
        "ggml_tensor * llama_kv_cache_unified::cpy_k(ggml_context * ctx, ggml_tensor * k_cur, int32_t il, uint32_t head_cur) const {\n",
        "    const int32_t ikv = map_layer_ids.at(il);\n",
        "\n",
        "    auto * k = layers[ikv].k;\n",
        "\n",
        "    const int64_t n_tokens = k_cur->ne[2];\n",
        "\n",
        "    ggml_tensor * k_view = ggml_view_1d(ctx, k,\n",
        "            n_tokens*hparams.n_embd_k_gqa(il),\n",
        "            ggml_row_size(k->type, hparams.n_embd_k_gqa(il))*head_cur);\n",
        "\n",
        "    --> return ggml_cpy(ctx, k_cur, k_view);\n",
        "}\n",
        "\n",
        "ggml_tensor * llama_kv_cache_unified::cpy_v(ggml_context * ctx, ggml_tensor * v_cur, int32_t il, uint32_t head_cur) const {\n",
        "    const int32_t ikv = map_layer_ids.at(il);\n",
        "\n",
        "    auto * v = layers[ikv].v;\n",
        "\n",
        "    const int64_t n_tokens = v_cur->ne[2];\n",
        "\n",
        "    v_cur = ggml_reshape_2d(ctx, v_cur, hparams.n_embd_v_gqa(il), n_tokens);\n",
        "\n",
        "    ggml_tensor * v_view = nullptr;\n",
        "\n",
        "    if (!v_trans) {\n",
        "        v_view = ggml_view_1d(ctx, v,\n",
        "                n_tokens*hparams.n_embd_v_gqa(il),\n",
        "                ggml_row_size(v->type, hparams.n_embd_v_gqa(il))*head_cur);\n",
        "    } else {\n",
        "        // note: the V cache is transposed when not using flash attention\n",
        "        v_view = ggml_view_2d(ctx, v, n_tokens, hparams.n_embd_v_gqa(il),\n",
        "                (v->ne[1])*ggml_element_size(v),\n",
        "                (head_cur)*ggml_element_size(v));\n",
        "\n",
        "        v_cur = ggml_transpose(ctx, v_cur);\n",
        "    }\n",
        "\n",
        "    --> return ggml_cpy(ctx, v_cur, v_view);\n",
        "}\n",
        "\n",
        "struct ggml_tensor * ggml_cpy(\n",
        "        struct ggml_context * ctx,\n",
        "        struct ggml_tensor * a,\n",
        "        struct ggml_tensor * b) {\n",
        "    --> return ggml_cpy_impl(ctx, a, b);\n",
        "}\n",
        "\n",
        "static struct ggml_tensor * ggml_cpy_impl(\n",
        "        struct ggml_context * ctx,\n",
        "        struct ggml_tensor  * a,\n",
        "        struct ggml_tensor  * b) {\n",
        "    // make a view of the destination\n",
        "    struct ggml_tensor * result = ggml_view_tensor(ctx, b);\n",
        "    if (strlen(b->name) > 0) {\n",
        "        ggml_format_name(result, \"%s (copy of %s)\", b->name, a->name);\n",
        "    } else {\n",
        "        ggml_format_name(result, \"%s (copy)\", a->name);\n",
        "    }\n",
        "\n",
        "    result->op     = GGML_OP_CPY;\n",
        "    result->src[0] = a;\n",
        "    result->src[1] = b;\n",
        "\n",
        "    return result;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "NVM9wnjRqChc"
      },
      "id": "NVM9wnjRqChc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`llm_graph_context::build_attn_mha`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-graph.cpp#L1061)\n",
        "\n",
        "```c++\n",
        "ggml_tensor * cur = build_attn_mha(gf, q, k, v, kq_b, kq_mask, v_mla, kq_scale);\n",
        "----------\n",
        "ggml_tensor * llm_graph_context::build_attn_mha(\n",
        "         ggml_cgraph * gf,\n",
        "         ggml_tensor * q,\n",
        "         ggml_tensor * k,\n",
        "         ggml_tensor * v,\n",
        "         ggml_tensor * kq_b,\n",
        "         ggml_tensor * kq_mask,\n",
        "         ggml_tensor * v_mla,\n",
        "             float     kq_scale) const {\n",
        "    const bool v_trans = v->nb[1] > v->nb[2];\n",
        "\n",
        "    q = ggml_permute(ctx0, q, 0, 2, 1, 3);\n",
        "    k = ggml_permute(ctx0, k, 0, 2, 1, 3);\n",
        "    v = ggml_permute(ctx0, v, 0, 2, 1, 3);\n",
        "\n",
        "    const auto n_tokens = q->ne[1];\n",
        "    const auto n_head   = q->ne[2];\n",
        "    ggml_tensor * kq = ggml_mul_mat(ctx0, k, q);\n",
        "    // note: this op tends to require high floating point range\n",
        "    // while for some models F16 is enough, for others it is not, so we default to F32 here\n",
        "    ggml_mul_mat_set_prec(kq, GGML_PREC_F32);\n",
        "    kq = ggml_soft_max_ext(ctx0, kq, kq_mask, kq_scale, hparams.f_max_alibi_bias);\n",
        "    ggml_tensor * kqv = ggml_mul_mat(ctx0, v, kq);\n",
        "    ggml_tensor * cur = ggml_permute(ctx0, kqv, 0, 2, 1, 3);\n",
        "    cur = ggml_cont_2d(ctx0, cur, cur->ne[0]*n_head, n_tokens);\n",
        "    ggml_build_forward_expand(gf, cur);\n",
        "    return cur;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "wHCp4rkElmB7"
      },
      "id": "wHCp4rkElmB7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`ggml_visit_parents`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L5794)\n",
        "\n",
        "```c++\n",
        "ggml_build_forward_expand(gf, cur);\n",
        "----------\n",
        "void ggml_build_forward_expand(struct ggml_cgraph * cgraph, struct ggml_tensor * tensor) {\n",
        "    --> ggml_build_forward_impl(cgraph, tensor, true);\n",
        "}\n",
        "\n",
        "static void ggml_build_forward_impl(struct ggml_cgraph * cgraph, struct ggml_tensor * tensor, bool expand) {\n",
        "    const int n0 = cgraph->n_nodes;\n",
        "    --> ggml_visit_parents(cgraph, tensor);\n",
        "    const int n_new = cgraph->n_nodes - n0;\n",
        "}\n",
        "\n",
        "static void ggml_visit_parents(struct ggml_cgraph * cgraph, struct ggml_tensor * node) {}\n",
        "```"
      ],
      "metadata": {
        "id": "gJ8MiSGWTSPC"
      },
      "id": "gJ8MiSGWTSPC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>struct ggml_hash_set</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_hash_set {\n",
        "    size_t size;\n",
        "    ggml_bitset_t * used;       // whether or not the keys are in use i.e. set\n",
        "    struct ggml_tensor ** keys; // actual tensors in the set, keys[i] is only defined if ggml_bitset_get(used, i)\n",
        "};\n",
        "\n",
        "static size_t ggml_hash_insert(struct ggml_hash_set * hash_set, struct ggml_tensor * key) {\n",
        "    size_t h = ggml_hash(key) % hash_set->size;\n",
        "\n",
        "    // linear probing\n",
        "    size_t i = h;\n",
        "    do {\n",
        "        if (!ggml_bitset_get(hash_set->used, i)) {\n",
        "            ggml_bitset_set(hash_set->used, i);\n",
        "            hash_set->keys[i] = key;\n",
        "            return i;\n",
        "        }\n",
        "        if (hash_set->keys[i] == key) {\n",
        "            return GGML_HASHSET_ALREADY_EXISTS;\n",
        "        }\n",
        "        i = (i + 1) % hash_set->size;\n",
        "    } while (i != h);\n",
        "\n",
        "    // visited all hash table entries -> not found\n",
        "    GGML_ABORT(\"fatal error\");\n",
        "}\n",
        "\n",
        "static size_t ggml_hash_find_or_insert(struct ggml_hash_set * hash_set, struct ggml_tensor * key) {\n",
        "    ...\n",
        "        if (hash_set->keys[i] == key) {\n",
        "            return i;\n",
        "        }\n",
        "    ...\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "// check if already visited\n",
        "if (ggml_hash_insert(&cgraph->visited_hash_set, node) == GGML_HASHSET_ALREADY_EXISTS) {\n",
        "    return;\n",
        "}\n",
        "\n",
        "for (int i = 0; i < GGML_MAX_SRC; ++i) {\n",
        "    const int k =\n",
        "        (cgraph->order == GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT) ? i :\n",
        "        (cgraph->order == GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT) ? (GGML_MAX_SRC-1-i) :\n",
        "        /* unknown order, just fall back to using i*/ i;\n",
        "    if (node->src[k]) {\n",
        "        ggml_visit_parents(cgraph, node->src[k]);\n",
        "    }\n",
        "}\n",
        "\n",
        "if (node->op == GGML_OP_NONE && !(node->flags & GGML_TENSOR_FLAG_PARAM)) {\n",
        "    // reached a leaf node, not part of the gradient graph (e.g. a constant)\n",
        "    if (strlen(node->name) == 0) {\n",
        "        ggml_format_name(node, \"leaf_%d\", cgraph->n_leafs);\n",
        "    }\n",
        "    cgraph->leafs[cgraph->n_leafs] = node;\n",
        "    cgraph->n_leafs++;\n",
        "} else {\n",
        "    if (strlen(node->name) == 0) {\n",
        "        ggml_format_name(node, \"node_%d\", cgraph->n_nodes);\n",
        "    }\n",
        "    cgraph->nodes[cgraph->n_nodes] = node;\n",
        "    cgraph->n_nodes++;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "KmMaxLhUn9KV"
      },
      "id": "KmMaxLhUn9KV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`ggml_backend_sched_reserve`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1549)\n",
        "\n",
        "```c++\n",
        "ggml_backend_sched_reserve(sched.get(), gf);\n",
        "----------\n",
        "bool ggml_backend_sched_reserve(ggml_backend_sched_t sched, struct ggml_cgraph * measure_graph) {}\n",
        "```"
      ],
      "metadata": {
        "id": "c8GQzND6_PTl"
      },
      "id": "c8GQzND6_PTl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "--> ggml_backend_sched_split_graph;\n",
        "--> ggml_backend_sched_synchronize;\n",
        "--> ggml_gallocr_reserve_n;\n",
        "ggml_backend_sched_reset(sched);\n",
        "return true;\n",
        "```"
      ],
      "metadata": {
        "id": "fCgF3QMosNtv"
      },
      "id": "fCgF3QMosNtv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`ggml_backend_sched_split_graph`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L865)\n",
        "\n",
        "```c++\n",
        "ggml_backend_sched_split_graph(sched, measure_graph);\n",
        "----------\n",
        "// assigns backends to ops and splits the graph into subgraphs that can be computed on the same backend\n",
        "static void ggml_backend_sched_split_graph(ggml_backend_sched_t sched, struct ggml_cgraph * graph) {}\n",
        "```"
      ],
      "metadata": {
        "id": "gNhRm_HisqMZ"
      },
      "id": "gNhRm_HisqMZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>struct ggml_backend_sched_split</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_backend_sched_split {\n",
        "    int backend_id;\n",
        "    int i_start;\n",
        "    int i_end;\n",
        "    struct ggml_tensor * inputs[GGML_SCHED_MAX_SPLIT_INPUTS];\n",
        "    int n_inputs;\n",
        "    // graph view of this split\n",
        "    struct ggml_cgraph graph;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "// reset splits\n",
        "sched->n_splits = 0;\n",
        "sched->n_graph_inputs = 0;\n",
        "sched->is_reset = false;\n",
        "\n",
        "struct ggml_init_params params = {\n",
        "    /* .mem_size =   */ sched->context_buffer_size,\n",
        "    /* .mem_buffer = */ sched->context_buffer,\n",
        "    /* .no_alloc =   */ true\n",
        "};\n",
        "\n",
        "ggml_free(sched->ctx);\n",
        "sched->ctx = ggml_init(params);\n",
        "// pass 1: assign backends to ops with pre-allocated inputs\n",
        "for (int i = 0; i < graph->n_leafs; i++)\n",
        "    if (*leaf_backend_id == -1) *leaf_backend_id = ggml_backend_sched_backend_id_from_cur(sched, leaf);\n",
        "for (int i = 0; i < graph->n_nodes; i++)\n",
        "    if (*node_backend_id == -1) *node_backend_id = ggml_backend_sched_backend_id_from_cur(sched, node);\n",
        "// pass 2: expand current backend assignments\n",
        "// assign the same backend to adjacent nodes\n",
        "// expand gpu backends (i.e. non last prio) up and down, ignoring cpu (the lowest priority backend)\n",
        "// thus, cpu will never be used unless weights are on cpu, or there are no gpu ops between cpu ops\n",
        "// ops unsupported by the backend being expanded will be left unassigned so that they can be assigned later when the locations of its inputs are known\n",
        "// expand gpu down\n",
        "for (int i = 0; i < graph->n_nodes; i++) if (*node_backend_id == sched->n_backends - 1) cur_backend_id = -1; // skip cpu (lowest prio backend)\n",
        "// expand gpu up\n",
        "for (int i = graph->n_nodes - 1; i >= 0; i--) if (*node_backend_id == sched->n_backends - 1) cur_backend_id = -1; // skip cpu (lowest prio backend)\n",
        "// expand rest down\n",
        "for (int i = 0; i < graph->n_nodes; i++)\n",
        "// expand rest up\n",
        "for (int i = graph->n_nodes - 1; i >= 0; i--)\n",
        "    if (cur_backend_id != -1 && *node_backend_id == -1) ggml_backend_sched_set_if_supported(sched, node, cur_backend_id, node_backend_id);\n",
        "// pass 3: upgrade nodes to higher prio backends with compatible buffer types\n",
        "// if the tensor is already in the same buffer type (*) as another higher priority backend, we should move it there\n",
        "// however, we also need to verify that the sources are in compatible buffer types\n",
        "// (*) the actual requirement is more relaxed, the buffer type of the backend should be supported by all the users of this tensor further down the graph\n",
        "// however, this is slow to verify, so we have a more strict requirement that the buffer type is the same\n",
        "// this is not uncommon since multiple backends can use host memory, with the same buffer type (eg. BLAS and CPU)\n",
        "// additionally, set remaining unassigned nodes to the backend with the most supported inputs\n",
        "// only nodes that could not be assigned during expansion due to the backend not supporting the op should be unassigned at this point\n",
        "for (int i = 0; i < graph->n_nodes; i++)\n",
        "    if (*node_backend_id == -1); // unassigned node: find the backend with the most supported inputs\n",
        "    else; // assigned node: upgrade to higher prio backend if possible\n",
        "// pass 4: assign backends to remaining src from dst and view_src\n",
        "for (int i = 0; i < graph->n_nodes; i++)\n",
        "    if (node->view_src != NULL && *cur_backend_id == -1);\n",
        "    // views are always on the same backend as the source\n",
        "// pass 5: split graph, find tensors that need to be copied\n",
        "int i_split = 0;\n",
        "struct ggml_backend_sched_split * split = &sched->splits[0];\n",
        "// find the backend of the first split, skipping view ops\n",
        "int i = 0;\n",
        "for (; i < graph->n_nodes; i++) {\n",
        "    struct ggml_tensor * node = graph->nodes[i];\n",
        "    if (!ggml_is_view_op(node->op)) {\n",
        "        split->backend_id = tensor_backend_id(node);\n",
        "        break;\n",
        "    }\n",
        "}\n",
        "split->i_start = 0;\n",
        "split->n_inputs = 0;\n",
        "int cur_backend_id = split->backend_id;\n",
        "for (; i < graph->n_nodes; i++)\n",
        "    // check if we should start a new split based on the sources of the current node\n",
        "    // find inputs that are not on the same backend\n",
        "split->i_end = graph->n_nodes;\n",
        "sched->n_splits = i_split + 1;\n",
        "// swap node_backend_ids and leaf _backend_ids with prevs\n",
        "// --> for `ggml_backend_sched_alloc_splits` func\n",
        "int graph_size = std::max(graph->n_nodes, graph->n_leafs) + sched->n_splits*GGML_SCHED_MAX_SPLIT_INPUTS*2*sched->n_copies;\n",
        "sched->graph.size = graph_size;\n",
        "sched->graph.nodes = (ggml_tensor **) realloc(sched->graph.nodes, graph_size * sizeof(struct ggml_tensor *));\n",
        "sched->graph.leafs = (ggml_tensor **) realloc(sched->graph.leafs, graph_size * sizeof(struct ggml_tensor *));\n",
        "sched->graph.n_nodes = 0;\n",
        "sched->graph.n_leafs = 0;\n",
        "struct ggml_cgraph * graph_copy = &sched->graph;\n",
        "//--> begin to construct the graph_copy, aka, sched->graph\n",
        "// add leafs from the original graph\n",
        "```"
      ],
      "metadata": {
        "id": "UlW7C9WvvdYd"
      },
      "id": "UlW7C9WvvdYd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`ggml_backend_sched_synchronize`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1599)\n",
        "\n",
        "```c++\n",
        "ggml_backend_sched_synchronize(sched);\n",
        "----------\n",
        "void ggml_backend_sched_synchronize(ggml_backend_sched_t sched) {}\n",
        "```"
      ],
      "metadata": {
        "id": "9udilRb2tF3-"
      },
      "id": "9udilRb2tF3-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "for (int i = 0; i < sched->n_backends; i++) {\n",
        "    --> ggml_backend_synchronize(sched->backends[i]);\n",
        "}\n",
        "if (!sched->is_alloc) {\n",
        "    // if the graph is not already allocated, always use copy 0 after a synchronization\n",
        "    // this ensures that during generation the same copy is used every time,\n",
        "    // which avoids changes in the graph that could cause CUDA or other graphs to be disabled\n",
        "    sched->cur_copy = 0;\n",
        "}\n",
        "\n",
        "void ggml_backend_synchronize(ggml_backend_t backend) {\n",
        "    if (backend->iface.synchronize == NULL) {\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    backend->iface.synchronize(backend);\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "K2LknXny8lHb"
      },
      "id": "K2LknXny8lHb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`ggml_gallocr_reserve_n`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L673)\n",
        "\n",
        "\n",
        "```c++\n",
        "ggml_gallocr_reserve_n(sched->galloc, &sched->graph, sched->node_backend_ids, sched->leaf_backend_ids);\n",
        "----------\n",
        "bool ggml_gallocr_reserve_n(ggml_gallocr_t galloc, struct ggml_cgraph * graph, const int * node_buffer_ids, const int * leaf_buffer_ids) {}\n",
        "```"
      ],
      "metadata": {
        "id": "0jIEKPZbtJGf"
      },
      "id": "0jIEKPZbtJGf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>struct hash_node</summary>\n",
        "\n",
        "```c++\n",
        "struct hash_node {\n",
        "    int n_children;\n",
        "    int n_views;\n",
        "    int buffer_id;\n",
        "    size_t offset; // offset within the buffer\n",
        "    bool allocated;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "size_t min_hash_size = graph->n_nodes + graph->n_leafs;\n",
        "// add 25% margin to avoid hash collisions\n",
        "min_hash_size += min_hash_size / 4;\n",
        "\n",
        "// initialize hash table\n",
        "if (galloc->hash_set.size < min_hash_size) {\n",
        "    ggml_hash_set_free(&galloc->hash_set);\n",
        "    galloc->hash_set = ggml_hash_set_new(min_hash_size);\n",
        "\n",
        "    free(galloc->hash_values);\n",
        "    galloc->hash_values = malloc(sizeof(struct hash_node) * galloc->hash_set.size);\n",
        "}\n",
        "\n",
        "// reset allocators\n",
        "for (int i = 0; i < galloc->n_buffers; i++) {\n",
        "    ggml_dyn_tallocr_reset(galloc->buf_tallocs[i]);\n",
        "}\n",
        "\n",
        "// allocate in hash table\n",
        "--> ggml_gallocr_alloc_graph_impl;\n",
        "```"
      ],
      "metadata": {
        "id": "lftemkBo9bDY"
      },
      "id": "lftemkBo9bDY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>struct tensor_alloc</summary>\n",
        "\n",
        "```c++\n",
        "struct tensor_alloc {\n",
        "    int buffer_id;\n",
        "    size_t offset;\n",
        "    size_t size_max; // 0 = pre-allocated, unused, or view\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary>struct node_alloc</summary>\n",
        "\n",
        "```c++\n",
        "struct node_alloc {\n",
        "    struct tensor_alloc dst;\n",
        "    struct tensor_alloc src[GGML_MAX_SRC];\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "```c++\n",
        "// set the node_allocs from the hash table\n",
        "if (galloc->n_nodes < graph->n_nodes) {\n",
        "    free(galloc->node_allocs);\n",
        "    galloc->node_allocs = calloc(graph->n_nodes, sizeof(struct node_alloc));\n",
        "}\n",
        "galloc->n_nodes = graph->n_nodes;\n",
        "for (int i = 0; i < graph->n_nodes; i++) {\n",
        "    struct ggml_tensor * node = graph->nodes[i];\n",
        "    struct node_alloc * node_alloc = &galloc->node_allocs[i];\n",
        "    if (node->view_src || node->data) {\n",
        "        node_alloc->dst.buffer_id = -1;\n",
        "        node_alloc->dst.offset = SIZE_MAX;\n",
        "        node_alloc->dst.size_max = 0;\n",
        "    } else {\n",
        "        --> ggml_gallocr_hash_get --> struct hash_node * hn;\n",
        "        node_alloc->dst.buffer_id = hn->buffer_id;\n",
        "        node_alloc->dst.offset    = hn->offset;\n",
        "        node_alloc->dst.size_max  = ggml_backend_buft_get_alloc_size(galloc->bufts[hn->buffer_id], node);\n",
        "    }\n",
        "    for (int j = 0; j < GGML_MAX_SRC; j++) {\n",
        "        struct ggml_tensor * src = node->src[j];\n",
        "        if (!src || src->view_src || src->data) {\n",
        "            node_alloc->src[j].buffer_id = -1;\n",
        "            node_alloc->src[j].offset = SIZE_MAX;\n",
        "            node_alloc->src[j].size_max = 0;\n",
        "        } else {\n",
        "            --> ggml_gallocr_hash_get --> struct hash_node * hn;\n",
        "            node_alloc->src[j].buffer_id = hn->buffer_id;\n",
        "            node_alloc->src[j].offset   = hn->offset;\n",
        "            node_alloc->src[j].size_max = ggml_backend_buft_get_alloc_size(galloc->bufts[hn->buffer_id], src);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "yw2vC_LvConF"
      },
      "id": "yw2vC_LvConF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>struct leaf_alloc</summary>\n",
        "\n",
        "```c++\n",
        "struct leaf_alloc {\n",
        "    struct tensor_alloc leaf;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "if (galloc->n_leafs < graph->n_leafs) {\n",
        "    free(galloc->leaf_allocs);\n",
        "    galloc->leaf_allocs = calloc(graph->n_leafs, sizeof(galloc->leaf_allocs[0]));\n",
        "}\n",
        "galloc->n_leafs = graph->n_leafs;\n",
        "for (int i = 0; i < graph->n_leafs; i++) {\n",
        "    struct ggml_tensor * leaf = graph->leafs[i];\n",
        "    --> ggml_gallocr_hash_get --> struct hash_node * hn;\n",
        "    if (leaf->view_src || leaf->data) {\n",
        "        galloc->leaf_allocs[i].leaf.buffer_id = -1;\n",
        "        galloc->leaf_allocs[i].leaf.offset = SIZE_MAX;\n",
        "        galloc->leaf_allocs[i].leaf.size_max = 0;\n",
        "    } else {\n",
        "        galloc->leaf_allocs[i].leaf.buffer_id = hn->buffer_id;\n",
        "        galloc->leaf_allocs[i].leaf.offset = hn->offset;\n",
        "        galloc->leaf_allocs[i].leaf.size_max = ggml_backend_buft_get_alloc_size(galloc->bufts[hn->buffer_id], leaf);\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "ACLCWoE1CvCt"
      },
      "id": "ACLCWoE1CvCt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "// reallocate buffers if needed\n",
        "for (int i = 0; i < galloc->n_buffers; i++) {\n",
        "    // if the buffer type is used multiple times, we reuse the same buffer\n",
        "    for (int j = 0; j < i; j++) {\n",
        "        if (galloc->buf_tallocs[j] == galloc->buf_tallocs[i]) {\n",
        "            galloc->buffers[i] = galloc->buffers[j];\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    size_t cur_size = galloc->buffers[i] ? ggml_backend_buffer_get_size(galloc->buffers[i]) : 0;\n",
        "    size_t new_size = ggml_dyn_tallocr_max_size(galloc->buf_tallocs[i]);\n",
        "\n",
        "    // even if there are no tensors allocated in this buffer, we still need to allocate it to initialize views\n",
        "    if (new_size > cur_size || galloc->buffers[i] == NULL) {\n",
        "        ggml_backend_buffer_free(galloc->buffers[i]);\n",
        "        // galloc->buffers[i] = ggml_backend_buft_alloc_buffer(galloc->bufts[i], new_size);\n",
        "        --> ggml_backend_buft_alloc_buffer --> galloc->buffers[i];\n",
        "        ggml_backend_buffer_set_usage(galloc->buffers[i], GGML_BACKEND_BUFFER_USAGE_COMPUTE);\n",
        "    }\n",
        "}\n",
        "\n",
        "return true;\n",
        "```"
      ],
      "metadata": {
        "id": "oS-ZUSrfC4xv"
      },
      "id": "oS-ZUSrfC4xv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### [`ggml_gallocr_alloc_graph_impl`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L566)\n",
        "\n",
        "\n",
        "```c++\n",
        "// allocate in hash table\n",
        "ggml_gallocr_alloc_graph_impl(galloc, graph, node_buffer_ids, leaf_buffer_ids);\n",
        "----------\n",
        "static void ggml_gallocr_alloc_graph_impl(ggml_gallocr_t galloc, struct ggml_cgraph * graph, const int * node_buffer_ids, const int * leaf_buffer_ids) {}\n",
        "```"
      ],
      "metadata": {
        "id": "YEldWDMpKTxe"
      },
      "id": "YEldWDMpKTxe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "// clear hash tables\n",
        "ggml_hash_set_reset(&galloc->hash_set);\n",
        "memset(galloc->hash_values, 0, sizeof(struct hash_node) * galloc->hash_set.size);\n",
        "\n",
        "// allocate leafs\n",
        "// these may be tensors that the application is not using in the graph, but may still want to allocate for other purposes\n",
        "for (int i = 0; i < graph->n_leafs; i++) {\n",
        "    struct ggml_tensor * leaf = graph->leafs[i];\n",
        "    --> ggml_gallocr_allocate_node(galloc, leaf, get_node_buffer_id(leaf_buffer_ids, i));\n",
        "}\n",
        "\n",
        "// count number of children and views\n",
        "// allocate other graph inputs and leafs first to avoid overwriting them\n",
        "for (int i = 0; i < graph->n_nodes; i++) {\n",
        "    struct ggml_tensor * node = graph->nodes[i];\n",
        "\n",
        "    // TODO: better way to add external dependencies\n",
        "    // GGML_OP_NONE does not appear normally in the graph nodes, but is used by ggml-backend to add dependencies to\n",
        "    // control when some tensors are allocated and freed. in this case, the dependencies are in `src`, but the node\n",
        "    // itself is never used and should not be considered a dependency\n",
        "    if (ggml_is_view(node) && node->op != GGML_OP_NONE) {\n",
        "        struct ggml_tensor * view_src = node->view_src;\n",
        "        --> ggml_gallocr_hash_get(galloc, view_src)->n_views += 1;\n",
        "    }\n",
        "\n",
        "    if (node->flags & GGML_TENSOR_FLAG_INPUT) {\n",
        "        --> ggml_gallocr_allocate_node(galloc, graph->nodes[i], get_node_buffer_id(node_buffer_ids, i));\n",
        "    }\n",
        "\n",
        "    for (int j = 0; j < GGML_MAX_SRC; j++) {\n",
        "        struct ggml_tensor * src = node->src[j];\n",
        "        --> ggml_gallocr_hash_get(galloc, src)->n_children += 1;\n",
        "\n",
        "        // allocate explicit inputs\n",
        "        if (src->flags & GGML_TENSOR_FLAG_INPUT) {\n",
        "            --> ggml_gallocr_allocate_node(galloc, src, get_node_buffer_id(node_buffer_ids, i));\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// allocate tensors\n",
        "for (int i = 0; i < graph->n_nodes; i++) {\n",
        "    struct ggml_tensor * node = graph->nodes[i];\n",
        "    int buffer_id = get_node_buffer_id(node_buffer_ids, i);\n",
        "\n",
        "    // allocate parents (only leafs need to be allocated at this point)\n",
        "    for (int j = 0; j < GGML_MAX_SRC; j++) {\n",
        "        struct ggml_tensor * parent = node->src[j];\n",
        "        --> ggml_gallocr_allocate_node(galloc, parent, buffer_id);\n",
        "    }\n",
        "\n",
        "    // allocate node\n",
        "    --> ggml_gallocr_allocate_node(galloc, node, buffer_id);\n",
        "\n",
        "    // update parents\n",
        "    for (int j = 0; j < GGML_MAX_SRC; j++) {\n",
        "        struct ggml_tensor * parent = node->src[j];\n",
        "        --> struct hash_node * p_hn = ggml_gallocr_hash_get(galloc, parent);\n",
        "        p_hn->n_children -= 1;\n",
        "\n",
        "        if (p_hn->n_children == 0 && p_hn->n_views == 0) {\n",
        "            if (ggml_is_view(parent)) {\n",
        "                struct ggml_tensor * view_src = parent->view_src;\n",
        "                --> struct hash_node * view_src_hn = ggml_gallocr_hash_get(galloc, view_src);\n",
        "                view_src_hn->n_views -= 1;\n",
        "                if (view_src_hn->n_views == 0 && view_src_hn->n_children == 0 && view_src_hn->allocated) {\n",
        "                    --> ggml_gallocr_free_node(galloc, view_src);\n",
        "                }\n",
        "            }\n",
        "            else if (p_hn->allocated) {\n",
        "                --> ggml_gallocr_free_node(galloc, parent);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "5b1WDXi_KrtB"
      },
      "id": "5b1WDXi_KrtB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_gallocr_allocate_node`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L478)\n",
        "\n",
        "```c++\n",
        "static void ggml_gallocr_allocate_node(ggml_gallocr_t galloc, struct ggml_tensor * node, int buffer_id) {\n",
        "    GGML_ASSERT(buffer_id >= 0);\n",
        "    // struct hash_node * hn = ggml_gallocr_hash_get(galloc, node);\n",
        "    --> ggml_gallocr_hash_get --> struct hash_node * hn;\n",
        "\n",
        "    if (!ggml_gallocr_is_allocated(galloc, node) && !ggml_is_view(node)) {\n",
        "        hn->allocated = true;\n",
        "\n",
        "        // try to reuse a parent's buffer (inplace)\n",
        "        if (ggml_op_can_inplace(node->op)) {\n",
        "            for (int i = 0; i < GGML_MAX_SRC; i++) {\n",
        "                struct ggml_tensor * parent = node->src[i];\n",
        "\n",
        "                // if the node's data is external, then we cannot re-use it\n",
        "                if (!ggml_gallocr_is_own(galloc, parent)) continue;\n",
        "\n",
        "                // outputs cannot be reused\n",
        "                if (parent->flags & GGML_TENSOR_FLAG_OUTPUT || (parent->view_src != NULL && parent->view_src->flags & GGML_TENSOR_FLAG_OUTPUT)) continue;\n",
        "\n",
        "                if (!ggml_are_same_layout(node, parent)) continue;\n",
        "\n",
        "                // struct hash_node * p_hn = ggml_gallocr_hash_get(galloc, parent);\n",
        "                --> ggml_gallocr_hash_get --> struct hash_node * p_hn;\n",
        "                if (p_hn->n_children == 1 && p_hn->n_views == 0) {\n",
        "                    if (ggml_is_view(parent)) {\n",
        "                        struct ggml_tensor * view_src = parent->view_src;\n",
        "                        // struct hash_node * view_src_hn = ggml_gallocr_hash_get(galloc, view_src);\n",
        "                        --> ggml_gallocr_hash_get --> struct hash_node * view_src_hn;\n",
        "                        if (view_src_hn->n_views == 1 && view_src_hn->n_children == 0 && view_src->data == parent->data) {\n",
        "                            assert(view_src_hn->offset == p_hn->offset);\n",
        "                            hn->buffer_id = p_hn->buffer_id;\n",
        "                            hn->offset = p_hn->offset;\n",
        "                            p_hn->allocated = false; // avoid freeing the parent\n",
        "                            view_src_hn->allocated = false;\n",
        "                            return;\n",
        "                        }\n",
        "                    } else {\n",
        "                        hn->buffer_id = p_hn->buffer_id;\n",
        "                        hn->offset = p_hn->offset;\n",
        "                        p_hn->allocated = false; // avoid freeing the parent\n",
        "                        return;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        // allocate tensor from the buffer\n",
        "        struct ggml_dyn_tallocr * alloc = galloc->buf_tallocs[buffer_id];\n",
        "        ggml_backend_buffer_type_t buft = galloc->bufts[buffer_id];\n",
        "        size_t size = ggml_backend_buft_get_alloc_size(buft, node);\n",
        "        --> ggml_dyn_tallocr_alloc --> size_t offset;\n",
        "        hn->buffer_id = buffer_id;\n",
        "        hn->offset = offset;\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "lKigIvKeNVR6"
      },
      "id": "lKigIvKeNVR6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_dyn_tallocr_alloc`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L153)\n",
        "\n",
        "```c++\n",
        "size_t offset = ggml_dyn_tallocr_alloc(alloc, size, node);\n",
        "----------\n",
        "static size_t ggml_dyn_tallocr_alloc(struct ggml_dyn_tallocr * alloc, size_t size, const struct ggml_tensor * tensor) {\n",
        "    size = aligned_offset(NULL, size, alloc->alignment);\n",
        "    // find the best fitting free block besides the last block\n",
        "    int best_fit_block = -1;\n",
        "    size_t best_fit_size = SIZE_MAX;\n",
        "    for (int i = 0; i < alloc->n_free_blocks - 1; i++) {\n",
        "        struct free_block * block = &alloc->free_blocks[i];\n",
        "        if (block->size >= size && block->size <= best_fit_size) {\n",
        "            best_fit_block = i;\n",
        "            best_fit_size = block->size;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (best_fit_block == -1) {\n",
        "        // the last block is our last resort\n",
        "        struct free_block * block = &alloc->free_blocks[alloc->n_free_blocks - 1];\n",
        "        // if (block->size >= size) {\n",
        "        best_fit_block = alloc->n_free_blocks - 1;\n",
        "    }\n",
        "\n",
        "    struct free_block * block = &alloc->free_blocks[best_fit_block];\n",
        "    size_t offset = block->offset;\n",
        "    block->offset = offset + size;\n",
        "    block->size -= size;\n",
        "    if (block->size == 0) {\n",
        "        // remove block if empty\n",
        "        alloc->n_free_blocks--;\n",
        "        for (int j = best_fit_block; j < alloc->n_free_blocks; j++) {\n",
        "            alloc->free_blocks[j] = alloc->free_blocks[j+1];\n",
        "        }\n",
        "    }\n",
        "    alloc->max_size = MAX(alloc->max_size, offset + size);\n",
        "    return offset;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "BzqW21nqNVdp"
      },
      "id": "BzqW21nqNVdp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_gallocr_free_node`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L545)\n",
        "\n",
        "```c++\n",
        "static void ggml_gallocr_free_node(ggml_gallocr_t galloc, struct ggml_tensor * node) {\n",
        "    // graph outputs are never freed\n",
        "    if (node->flags & GGML_TENSOR_FLAG_OUTPUT) {\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // struct hash_node * hn = ggml_gallocr_hash_get(galloc, node);\n",
        "    --> ggml_gallocr_hash_get --> struct hash_node * hn;\n",
        "    size_t offset = hn->offset;\n",
        "    int buffer_id = hn->buffer_id;\n",
        "    struct ggml_dyn_tallocr * alloc = galloc->buf_tallocs[buffer_id];\n",
        "    ggml_backend_buffer_type_t buft = galloc->bufts[buffer_id];\n",
        "    size_t size = ggml_backend_buft_get_alloc_size(buft, node);\n",
        "    --> ggml_dyn_tallocr_free_tensor;\n",
        "    hn->allocated = false;\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "mSlLVEbyQFNn"
      },
      "id": "mSlLVEbyQFNn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_dyn_tallocr_free_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L238)\n",
        "\n",
        "```c++\n",
        "ggml_dyn_tallocr_free_tensor(alloc, offset, size, node);\n",
        "----------\n",
        "// this is a very naive implementation, but for our case the number of free blocks should be very small\n",
        "static void ggml_dyn_tallocr_free_tensor(struct ggml_dyn_tallocr * alloc, size_t offset, size_t size, const struct ggml_tensor * tensor) {\n",
        "    size = aligned_offset(NULL, size, alloc->alignment);\n",
        "\n",
        "    // see if we can merge with an existing block\n",
        "    for (int i = 0; i < alloc->n_free_blocks; i++) {\n",
        "        struct free_block * block = &alloc->free_blocks[i];\n",
        "        // check if ptr is at the end of the block\n",
        "        if (block->offset + block->size == offset) {\n",
        "            block->size += size;\n",
        "            // check if we can merge with the next block\n",
        "            if (i < alloc->n_free_blocks - 1 && block->offset + block->size == alloc->free_blocks[i+1].offset) {\n",
        "                block->size += alloc->free_blocks[i+1].size;\n",
        "                alloc->n_free_blocks--;\n",
        "                for (int j = i+1; j < alloc->n_free_blocks; j++) {\n",
        "                    alloc->free_blocks[j] = alloc->free_blocks[j+1];\n",
        "                }\n",
        "            }\n",
        "            return;\n",
        "        }\n",
        "        // check if ptr is at the beginning of the block\n",
        "        if (offset + size == block->offset) {\n",
        "            block->offset = offset;\n",
        "            block->size += size;\n",
        "            // check if we can merge with the previous block\n",
        "            if (i > 0 && alloc->free_blocks[i-1].offset + alloc->free_blocks[i-1].size == block->offset) {\n",
        "                alloc->free_blocks[i-1].size += block->size;\n",
        "                alloc->n_free_blocks--;\n",
        "                for (int j = i; j < alloc->n_free_blocks; j++) {\n",
        "                    alloc->free_blocks[j] = alloc->free_blocks[j+1];\n",
        "                }\n",
        "            }\n",
        "            return;\n",
        "        }\n",
        "    }\n",
        "    // otherwise, add a new block\n",
        "    // insert the new block in the correct position to keep the array sorted by address (to make merging blocks faster)\n",
        "    int insert_pos = 0;\n",
        "    while (insert_pos < alloc->n_free_blocks && alloc->free_blocks[insert_pos].offset < offset) {\n",
        "        insert_pos++;\n",
        "    }\n",
        "    // shift all blocks from insert_pos onward to make room for the new block\n",
        "    for (int i = alloc->n_free_blocks; i > insert_pos; i--) {\n",
        "        alloc->free_blocks[i] = alloc->free_blocks[i-1];\n",
        "    }\n",
        "    // insert the new block\n",
        "    alloc->free_blocks[insert_pos].offset = offset;\n",
        "    alloc->free_blocks[insert_pos].size = size;\n",
        "    alloc->n_free_blocks++;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "cmUXXd9TQUD4"
      },
      "id": "cmUXXd9TQUD4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### [`ggml_gallocr_hash_get`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L465)\n",
        "\n",
        "```c++\n",
        "struct hash_node * hn = ggml_gallocr_hash_get(galloc, node);\n",
        "struct hash_node * hn = ggml_gallocr_hash_get(galloc, src);\n",
        "struct hash_node * hn = ggml_gallocr_hash_get(galloc, leaf);\n",
        "----------\n",
        "static struct hash_node * ggml_gallocr_hash_get(ggml_gallocr_t galloc, struct ggml_tensor * t) {\n",
        "    size_t i = ggml_hash_find_or_insert(&galloc->hash_set, t);\n",
        "    return &galloc->hash_values[i];\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "5On_v364Hd2s"
      },
      "id": "5On_v364Hd2s"
    },
    {
      "cell_type": "markdown",
      "id": "c7034405-372b-4f82-803f-87dca0e81d41",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "c7034405-372b-4f82-803f-87dca0e81d41"
      },
      "source": [
        "## *[`llama_decode`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#886)\n",
        "\n",
        "<details>\n",
        "<summary>struct llama_batch</summary>\n",
        "\n",
        "```c++\n",
        "    // Input data for llama_decode\n",
        "    // A llama_batch object can contain input about one or many sequences\n",
        "    // The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens\n",
        "    //\n",
        "    // - token  : the token ids of the input (used when embd is NULL)\n",
        "    // - embd   : token embeddings (i.e. float vector of size n_embd) (used when token is NULL)\n",
        "    // - pos    : the positions of the respective token in the sequence\n",
        "    //            (if set to NULL, the token position will be tracked automatically by llama_decode)\n",
        "    // - seq_id : the sequence to which the respective token belongs\n",
        "    //            (if set to NULL, the sequence ID will be assumed to be 0)\n",
        "    // - logits : if zero, the logits (and/or the embeddings) for the respective token will not be output\n",
        "    //            (if set to NULL, only the logits for last token will be returned)\n",
        "    //\n",
        "    typedef struct llama_batch {\n",
        "        int32_t n_tokens;\n",
        "\n",
        "        llama_token  *  token;\n",
        "        float        *  embd;\n",
        "        llama_pos    *  pos;\n",
        "        int32_t      *  n_seq_id; // TODO: remove, should belong to only 1 sequence\n",
        "        llama_seq_id ** seq_id;   // TODO: become llama_seq_id * seq_id;\n",
        "        int8_t       *  logits;   // TODO: rename this to \"output\"\n",
        "    } llama_batch;\n",
        "\n",
        "\n",
        "struct llama_batch llama_batch_get_one(\n",
        "             llama_token * tokens,\n",
        "                 int32_t   n_tokens) {\n",
        "    return {\n",
        "        /*n_tokens       =*/ n_tokens,\n",
        "        /*tokens         =*/ tokens,\n",
        "        /*embd           =*/ nullptr,\n",
        "        /*pos            =*/ nullptr,\n",
        "        /*n_seq_id       =*/ nullptr,\n",
        "        /*seq_id         =*/ nullptr,\n",
        "        /*logits         =*/ nullptr,\n",
        "    };\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "llama_decode(lctx, llama_batch_get_one(tmp.data(), std::min(tmp.size(), (size_t) params.n_batch)));\n",
        "----------\n",
        "int32_t llama_decode(\n",
        "        llama_context * ctx,\n",
        "          llama_batch   batch) {\n",
        "    --> const int ret = ctx->decode(batch);\n",
        "    return ret;\n",
        "}\n",
        "\n",
        "int llama_context::decode(llama_batch & inp_batch) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "--> llama_batch_allocr::init;\n",
        "const llama_batch & batch = batch_allocr->get_batch();\n",
        "\n",
        "const auto & vocab   = model.vocab;\n",
        "const auto & hparams = model.hparams;\n",
        "const int32_t n_vocab = vocab.n_tokens();\n",
        "const int64_t n_tokens_all = batch.n_tokens;\n",
        "const uint32_t n_outputs_all = batch_allocr->get_n_outputs();\n",
        "n_queued_tokens += n_tokens_all;\n",
        "bool did_optimize = false;\n",
        "// handle any pending defrags/shifts\n",
        "--> kv_self_update;\n",
        "--> llama_kv_cache_unified::init_batch --> llama_memory_state_ptr mstate;\n",
        "// reserve output buffer\n",
        "--> output_reserve(n_outputs_all)\n",
        "int64_t n_outputs_prev = 0;\n",
        "const auto & ubatch = mstate->get_ubatch();\n",
        "// count the outputs in this u_batch\n",
        "int32_t n_outputs_new = 0;\n",
        "for (uint32_t i = 0; i < ubatch.n_tokens; i++) n_outputs_new += (int32_t) (ubatch.output[i] != 0);\n",
        "// needs to happen before the graph is built\n",
        "// llama_context:: int32_t n_outputs; // number of actually-used outputs in the current ubatch or last logical batch\n",
        "n_outputs = n_outputs_new;\n",
        "ggml_backend_sched_reset(sched.get());\n",
        "\n",
        "--> llama_context::process_ubatch --> const auto res;\n",
        "\n",
        "    // plot the computation graph in dot format (for debugging purposes)\n",
        "    //if (n_past%100 == 0) {\n",
        "    //    ggml_graph_dump_dot(gf, NULL, \"llama.dot\");\n",
        "    //}\n",
        "\n",
        "auto * t_logits = res->get_logits();\n",
        "// extract logits\n",
        "ggml_backend_t backend_res = ggml_backend_sched_get_tensor_backend(sched.get(), t_logits);\n",
        "float * logits_out = logits + n_outputs_prev*n_vocab;\n",
        "--> ggml_backend_tensor_get_async --> logits_out\n",
        "n_outputs_prev += n_outputs;\n",
        "\n",
        "// set to total number of outputs in the batch, for use in llama_get_logits_ith\n",
        "n_outputs = n_outputs_all;\n",
        "// set output mappings\n",
        "auto & out_ids = mstate->out_ids();\n",
        "\n",
        "//llama_context:: std::vector<int32_t> output_ids; // map batch token positions to ids of the logits and embd buffers\n",
        "for (int64_t i = 0; i < n_outputs_all; ++i)\n",
        "    output_ids[out_ids[i]] = i;\n",
        "\n",
        "// wait for the computation to finish (automatically done when obtaining the model output)\n",
        "//synchronize();\n",
        "\n",
        "// Reset state for the next token before backend sync, to allow the CPU activities in the reset to overlap with device computation.\n",
        "ggml_backend_sched_reset(sched.get());\n",
        "```"
      ],
      "metadata": {
        "id": "nG2IP_LzbYQS"
      },
      "id": "nG2IP_LzbYQS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [`llama_batch_allocr::llama_batch_allocr`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L299)\n",
        "\n",
        "<details>\n",
        "<summary>class llama_batch_allocr</summary>\n",
        "\n",
        "```c++\n",
        "typedef int32_t llama_pos;\n",
        "typedef int32_t llama_seq_id;\n",
        "\n",
        "class llama_batch_allocr {\n",
        "    llama_batch batch;\n",
        "\n",
        "    uint32_t n_outputs;\n",
        "\n",
        "    std::array<llama_seq_id, 1> seq_id_0 = { 0 }; // default sequence id\n",
        "\n",
        "    std::vector<llama_pos>      pos;\n",
        "    std::vector<int32_t>        n_seq_id;\n",
        "    std::vector<llama_seq_id *> seq_id;\n",
        "    std::vector<int8_t>         output;\n",
        "\n",
        "    std::vector<std::set<llama_pos>> seq_pos; // seq_pos[s]: the set of positions in sequence s\n",
        "    std::vector<std::vector<bool>>   seq_cpl; // seq_cpl[s0][s1]: if sequence s0 is coupled to sequence s1\n",
        "\n",
        "    int debug;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "batch_allocr->init(batch_inp, model.vocab, memory.get());\n",
        "----------\n",
        "bool llama_batch_allocr::init(\n",
        "        const llama_batch & batch_inp,\n",
        "        const llama_vocab & vocab,\n",
        "        const llama_memory_i * memory) {}\n",
        "```"
      ],
      "metadata": {
        "id": "rcWYjLxghV9z"
      },
      "id": "rcWYjLxghV9z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "--> llama_batch_allocr::clear;\n",
        "batch = batch_inp;\n",
        "// validate input batch\n",
        "if (batch.token) for (int32_t i = 0; i < batch.n_tokens; ++i) if (batch.token[i] < 0 || (uint32_t) batch.token[i] >= vocab.n_tokens()) LLAMA_LOG_ERROR(\"%s: invalid token[%d] = %d\\n\", __func__, i, batch.token[i]);\n",
        "if (batch.seq_id) for (int32_t i = 0; i < batch.n_tokens; ++i) if (batch.seq_id && (batch.seq_id[i][s] < 0 || batch.seq_id[i][s] >= LLAMA_MAX_SEQ)) LLAMA_LOG_ERROR(\"%s: invalid seq_id[%d][%d] = %d > %d\\n\", __func__, i, s, batch.seq_id[i][s], LLAMA_MAX_SEQ);\n",
        "\n",
        "// auto-generate missing fields\n",
        "if (!batch.n_seq_id) {\n",
        "    n_seq_id.resize(batch.n_tokens);\n",
        "    for (int32_t i = 0; i < batch.n_tokens; i++) n_seq_id[i] = seq_id_0.size();\n",
        "    batch.n_seq_id = n_seq_id.data();\n",
        "}\n",
        "if (!batch.seq_id) {\n",
        "    seq_id.resize(batch.n_tokens + 1);\n",
        "    seq_id[batch.n_tokens] = NULL;\n",
        "    for (int32_t i = 0; i < batch.n_tokens; i++) seq_id[i] = seq_id_0.data();\n",
        "    batch.seq_id = seq_id.data();\n",
        "}\n",
        "if (!batch.pos) {\n",
        "    pos.resize(batch.n_tokens);\n",
        "    // initialize the starting position for each sequence based on the positions in the memory\n",
        "    llama_pos p0[LLAMA_MAX_SEQ];\n",
        "    for (int32_t s = 0; s < LLAMA_MAX_SEQ; ++s) p0[s] = memory? memory->seq_pos_max(s) + 1 : 0;\n",
        "    for (int32_t i = 0; i < batch.n_tokens; i++) {\n",
        "        const llama_seq_id seq_id = batch.seq_id[i][0];\n",
        "        pos[i] = p0[seq_id];\n",
        "        for (int32_t s = 0; s < batch.n_seq_id[i]; ++s) p0[batch.seq_id[i][s]] = pos[i] + 1;\n",
        "    }\n",
        "    batch.pos = pos.data();\n",
        "}\n",
        "if (!batch.logits) {\n",
        "    // by default return the output only for the last token\n",
        "    logits.resize(batch.n_tokens);\n",
        "    logits[logits.size() - 1] = true;\n",
        "    batch.logits = logits.data();\n",
        "}\n",
        "\n",
        "// compute stats\n",
        "for (int32_t i = 0; i < batch.n_tokens; ++i) n_outputs += batch.logits[i] != 0;\n",
        "\n",
        "// determine coupled sequences these are pairs of sequences that have at least one token in the input batch that is assigned to both of them\n",
        "for (int32_t i = 0; i < batch.n_tokens; ++i)\n",
        "    for (int32_t s = 0; s < batch.n_seq_id[i]; ++s)\n",
        "        seq_pos[batch.seq_id[i][s]].insert(batch.pos[i]);\n",
        "        // mark that sequence s1 is coupled to s0\n",
        "        if (s > 0) seq_cpl[batch.seq_id[i][s]][batch.seq_id[i][0]] = true;\n",
        "\n",
        "// consistency checks\n",
        "memory && seq_pos_min(s) == memory->seq_pos_max(s) + 1; //sequence %d does start from the last position stored in the memory\n",
        "seq_pos_max(s) - seq_pos_min(s) < (int) seq_pos[s].size(); //sequence %d positions are continuous\n",
        "if (memory) {\n",
        "    for (int32_t s0 = 0; s0 < LLAMA_MAX_SEQ; ++s0)\n",
        "    for (int32_t s1 = 0; s1 < LLAMA_MAX_SEQ; ++s1)\n",
        "        if (seq_cpl[s0][s1])\n",
        "            memory->seq_pos_min(s0) == memory->seq_pos_min(s1) ||\n",
        "            memory->seq_pos_max(s0) == memory->seq_pos_max(s1);\n",
        "            //sequence %d is coupled to %d in the input batch, but have divereged\n",
        "}\n",
        "\n",
        "return true;\n",
        "```"
      ],
      "metadata": {
        "id": "wVbufKl8hw-D"
      },
      "id": "wVbufKl8hw-D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`llama_batch_allocr::clear`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L532)\n",
        "\n",
        "```c++\n",
        "clear();\n",
        "----------\n",
        "void llama_batch_allocr::clear() {\n",
        "    n_outputs = 0;\n",
        "\n",
        "    batch = {};\n",
        "    pos.clear();\n",
        "    n_seq_id.clear();\n",
        "    seq_id.clear();\n",
        "    output.clear();\n",
        "\n",
        "    for (auto & cur : seq_pos) {\n",
        "        cur.clear();\n",
        "    }\n",
        "\n",
        "    for (auto & cur : seq_cpl) {\n",
        "        std::fill(cur.begin(), cur.end(), false);\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "s6pc_kUJPOEn"
      },
      "id": "s6pc_kUJPOEn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [`llama_context::kv_self_update`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L437)\n",
        "\n",
        "\n",
        "```c++\n",
        "kv_self_update(false);\n",
        "----------\n",
        "// deprecated\n",
        "bool llama_context::kv_self_update(bool optimize) {}\n",
        "```"
      ],
      "metadata": {
        "id": "jIqtIvyzqb8m"
      },
      "id": "jIqtIvyzqb8m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "// TODO: remove in the future\n",
        "optimize |= memory_force_optimize;\n",
        "memory_force_optimize = false;\n",
        "\n",
        "--> memory --> llama_kv_cache_unified::init_update --> const auto mstate;\n",
        "switch (mstate->get_status())\n",
        "case LLAMA_MEMORY_STATUS_NO_UPDATE: return false; // no updates need to be performed\n",
        "```"
      ],
      "metadata": {
        "id": "8kJxA9OQhpxV"
      },
      "id": "8kJxA9OQhpxV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`lama_kv_cache_unified::init_update`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L340)\n",
        "\n",
        "```c++\n",
        "const auto mstate = memory->init_update(this, optimize);\n",
        "----------\n",
        "llama_memory_state_ptr llama_kv_cache_unified::init_update(llama_context * lctx, bool optimize) {\n",
        "    bool do_shift = get_has_shift();\n",
        "\n",
        "    defrag_info dinfo;\n",
        "\n",
        "    // see if we need to defrag\n",
        "    {\n",
        "        bool do_defrag = optimize;\n",
        "\n",
        "        const auto thold = lctx->get_cparams().defrag_thold;\n",
        "\n",
        "        if (!do_defrag && thold > 0.0f) {\n",
        "            const auto n_kv = cells.used_max_p1();\n",
        "\n",
        "            // - do not defrag small contexts (i.e. < 2048 tokens)\n",
        "            // - count the padding towards the number of used tokens\n",
        "            const float fragmentation = n_kv >= 2048 ? std::max(0.0f, 1.0f - (float(cells.get_used() + n_pad)/n_kv)) : 0.0f;\n",
        "\n",
        "            if (fragmentation > thold) {\n",
        "                LLAMA_LOG_DEBUG(\"%s: fragmentation: %.2f - requesting defrag\\n\", __func__, fragmentation);\n",
        "\n",
        "                do_defrag = true;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if (do_defrag) {\n",
        "            dinfo = defrag_prepare(lctx->graph_max_nodes());\n",
        "        }\n",
        "    }\n",
        "\n",
        "    --> return std::make_unique<llama_kv_cache_unified_state>(this, lctx, do_shift, std::move(dinfo));\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "lEJTh4tsv5YU"
      },
      "id": "lEJTh4tsv5YU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`llama_kv_cache_unified_state::llama_kv_cache_unified_state`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L1737)\n",
        "\n",
        "```c++\n",
        "llama_kv_cache_unified_state::llama_kv_cache_unified_state(\n",
        "        llama_kv_cache_unified * kv,\n",
        "        llama_context * lctx,\n",
        "        bool do_shift,\n",
        "        defrag_info dinfo) : status(LLAMA_MEMORY_STATUS_SUCCESS), kv(kv), lctx(lctx), do_shift(do_shift), dinfo(std::move(dinfo)) {\n",
        "    if (!do_shift && this->dinfo.empty()) {\n",
        "        status = LLAMA_MEMORY_STATUS_NO_UPDATE;\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "ECEOUK5oxRFn"
      },
      "id": "ECEOUK5oxRFn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [`llama_kv_cache_unified::init_batch`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L310)\n",
        "\n",
        "```c++\n",
        "mstate = memory->init_batch(batch, cparams.n_ubatch, embd_pooled);\n",
        "----------\n",
        "llama_memory_state_ptr llama_kv_cache_unified::init_batch(\n",
        "            const llama_batch & batch,\n",
        "            uint32_t n_ubatch,\n",
        "            bool embd_pooled) {}\n",
        "```"
      ],
      "metadata": {
        "id": "fp94VaBRs6lv"
      },
      "id": "fp94VaBRs6lv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "--> llama_sbatch::llama_sbatch --> llama_sbatch sbatch;\n",
        "--> llama_sbatch::split_simple --> std::vector<llama_ubatch> ubatches.push_back;\n",
        "--> llama_kv_cache_unified::prepare --> llama_kv_cache_unified::ubatch_heads heads;\n",
        "--> return llama_kv_cache_unified_state::llama_kv_cache_unified_state;\n",
        "```"
      ],
      "metadata": {
        "id": "q4PW8HnO6lsL"
      },
      "id": "q4PW8HnO6lsL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`llama_sbatch::llama_sbatch`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L201)\n",
        "\n",
        "```c++\n",
        "auto sbatch = llama_sbatch(batch, hparams.n_embd, true);\n",
        "----------\n",
        "llama_sbatch::llama_sbatch(const llama_batch & batch, size_t n_embd, bool simple_split) {}\n",
        "```"
      ],
      "metadata": {
        "id": "ArRAbKST6rOf"
      },
      "id": "ArRAbKST6rOf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>struct llama_sbatch</summary>\n",
        "\n",
        "```c++\n",
        "struct llama_sbatch {\n",
        "    // tokens left in this batch\n",
        "    size_t n_tokens;\n",
        "\n",
        "    size_t n_embd;\n",
        "\n",
        "    // sorted indices into the batch\n",
        "    std::vector<int64_t> ids;\n",
        "    // batch indices of the output\n",
        "    std::vector<int64_t> out_ids;\n",
        "    std::vector<llama_sbatch_seq> seq;\n",
        "\n",
        "    const llama_batch * batch = nullptr;\n",
        "\n",
        "    // buffers for the ubatches\n",
        "    // TODO: very hacky, this needs a complete rework\n",
        "    struct ubatch_data {\n",
        "        std::vector<llama_token>    token;\n",
        "        std::vector<float>          embd;\n",
        "        std::vector<llama_pos>      pos;\n",
        "        std::vector<int32_t>        n_seq_id;\n",
        "        std::vector<llama_seq_id *> seq_id;\n",
        "        std::vector<int8_t>         output;\n",
        "    };\n",
        "\n",
        "    std::vector<ubatch_data> udatas;\n",
        "\n",
        "    llama_ubatch reserve_ubatch(size_t n_ubatch, bool has_embd = false);\n",
        "\n",
        "    void add_seq_to_ubatch(llama_ubatch & ubatch, llama_sbatch_seq & seq, size_t length);\n",
        "\n",
        "    // simple split, unknown number of sequences of unequal lengths\n",
        "    llama_ubatch split_simple(size_t n_ubatch);\n",
        "\n",
        "    // make batches of equal-length sequences\n",
        "    llama_ubatch split_equal(size_t n_ubatch);\n",
        "\n",
        "    // sequence-wise split\n",
        "    llama_ubatch split_seq(size_t n_ubatch);\n",
        "\n",
        "    llama_sbatch() = default;\n",
        "    llama_sbatch(const llama_batch & batch, size_t n_embd, bool simple_split = false);\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>struct llama_sbatch_seq</summary>\n",
        "\n",
        "```c++\n",
        "struct llama_sbatch_seq {\n",
        "    int32_t n_seq_id;\n",
        "\n",
        "    llama_seq_id * seq_id;\n",
        "\n",
        "    size_t offset;\n",
        "    size_t length;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "this->batch = &batch;\n",
        "this->n_embd = n_embd;\n",
        "n_tokens = batch.n_tokens;\n",
        "ids.resize(n_tokens);\n",
        "out_ids.clear();\n",
        "// TODO: reserve out_ids and seq\n",
        "for (size_t i = 0; i < n_tokens; ++i) ids[i] = i;\n",
        "if (simple_split) {\n",
        "    seq.resize(1);\n",
        "    llama_sbatch_seq & s = seq[0];\n",
        "    s.n_seq_id = 0;\n",
        "    s.seq_id = nullptr;\n",
        "    s.offset = 0;\n",
        "    s.length = n_tokens;\n",
        "    return;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "pO7EvVZE7On-"
      },
      "id": "pO7EvVZE7On-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`llama_sbatch::split_simple`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L149)\n",
        "\n",
        "```c++\n",
        "ubatches.push_back(sbatch.split_simple(n_ubatch));\n",
        "----------\n",
        "llama_ubatch llama_sbatch::split_simple(size_t n_ubatch) {}\n",
        "```"
      ],
      "metadata": {
        "id": "dZBqXhpJ8Cep"
      },
      "id": "dZBqXhpJ8Cep"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "n_ubatch = n_tokens < n_ubatch ? n_tokens : n_ubatch;\n",
        "--> llama_sbatch::reserve_ubatch --> llama_ubatch ubatch;\n",
        "ubatch.equal_seqs = false;\n",
        "llama_sbatch_seq & s = seq[0];\n",
        "size_t length = s.length < n_ubatch ? s.length : n_ubatch;\n",
        "--> llama_sbatch::add_seq_to_ubatch;\n",
        "return ubatch;\n",
        "```"
      ],
      "metadata": {
        "id": "u8jAu9qYEA1p"
      },
      "id": "u8jAu9qYEA1p"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`llama_sbatch::reserve_ubatch`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L13)\n",
        "\n",
        "```c++\n",
        "llama_ubatch ubatch = reserve_ubatch(n_ubatch, /* has_embd */ batch->embd != nullptr);\n",
        "----------\n",
        "llama_ubatch llama_sbatch::reserve_ubatch(size_t n_ubatch, bool has_embd) {}\n",
        "```"
      ],
      "metadata": {
        "id": "IK3nbzQYErlY"
      },
      "id": "IK3nbzQYErlY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>struct llama_ubatch</summary>\n",
        "\n",
        "```c++\n",
        "// very similar to llama_batch,\n",
        "// but has more metadata about sequences\n",
        "struct llama_ubatch {\n",
        "    bool equal_seqs;\n",
        "    // TODO: whole_seqs for embeddings?\n",
        "\n",
        "    uint32_t n_tokens;     // total tokens (n_seq_tokens * n_seqs)\n",
        "    uint32_t n_seq_tokens; // tokens per sequence\n",
        "    uint32_t n_seqs;\n",
        "\n",
        "    llama_token  *  token;    // [n_tokens]\n",
        "    float        *  embd;     // [n_embd, n_tokens]\n",
        "    llama_pos    *  pos;      // [n_tokens]\n",
        "    int32_t      *  n_seq_id; // [n_seqs]\n",
        "    llama_seq_id ** seq_id;   // [n_seqs]\n",
        "    int8_t       *  output;   // [n_tokens]\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "// simple split \n",
        "\n",
        "udatas.push_back({});\n",
        "\n",
        "auto & udata = udatas.back();\n",
        "\n",
        "udata.token.resize(n_ubatch);\n",
        "udata.pos.resize(n_ubatch);\n",
        "udata.n_seq_id.resize(n_ubatch);\n",
        "udata.seq_id.resize(n_ubatch);\n",
        "udata.output.resize(n_ubatch);\n",
        "\n",
        "llama_ubatch ubatch = {\n",
        "    /*equal_seqs   =*/ true,\n",
        "    /*n_tokens     =*/ 0,\n",
        "    /*n_seq_tokens =*/ 0,\n",
        "    /*n_seqs       =*/ 0,\n",
        "    /*token        =*/ udata.token.data(),\n",
        "    /*embd         =*/ nullptr,\n",
        "    /*pos          =*/ udata.pos.data(),\n",
        "    /*n_seq_id     =*/ udata.n_seq_id.data(),\n",
        "    /*seq_id       =*/ udata.seq_id.data(),\n",
        "    /*output       =*/ udata.output.data(),\n",
        "};\n",
        "\n",
        "return ubatch;\n",
        "```"
      ],
      "metadata": {
        "id": "7Wy0UsQEFI1I"
      },
      "id": "7Wy0UsQEFI1I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`llama_sbatch::add_seq_to_ubatch`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L52)\n",
        "\n",
        "```c++\n",
        "add_seq_to_ubatch(ubatch, s, length);\n",
        "----------\n",
        "void llama_sbatch::add_seq_to_ubatch(llama_ubatch & ubatch, llama_sbatch_seq & seq, size_t length) {}\n",
        "```"
      ],
      "metadata": {
        "id": "UCUXaCpSHyFG"
      },
      "id": "UCUXaCpSHyFG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "// llama_sbatch:: const llama_batch * batch;\n",
        "\n",
        "// simple split\n",
        "ubatch.token = batch->token + seq.offset;\n",
        "ubatch.embd = nullptr;\n",
        "ubatch.pos = batch->pos + seq.offset;\n",
        "ubatch.n_seq_id = batch->n_seq_id + seq.offset;\n",
        "ubatch.seq_id = batch->seq_id + seq.offset;\n",
        "ubatch.output = batch->logits + seq.offset;\n",
        "for (size_t i = 0; i < length; ++i) if (ubatch.output[i] != 0) { out_ids.push_back(seq.offset + i); }\n",
        "// if (ubatch.n_tokens == 0 && ubatch.n_seqs == 0)\n",
        "ubatch.n_seq_tokens = 1;\n",
        "ubatch.n_tokens += length;\n",
        "ubatch.n_seqs += length; // virtual sequences for simple splits\n",
        "seq.offset += length;\n",
        "seq.length -= length;\n",
        "n_tokens -= length;\n",
        "```"
      ],
      "metadata": {
        "id": "DJPQeCNsIJTo"
      },
      "id": "DJPQeCNsIJTo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`llama_kv_cache_unified::prepare`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L373)\n",
        "\n",
        "```c++\n",
        "auto heads = prepare(ubatches);\n",
        "----------\n",
        "llama_kv_cache_unified::ubatch_heads llama_kv_cache_unified::prepare(const std::vector<llama_ubatch> & ubatches) {}\n",
        "```"
      ],
      "metadata": {
        "id": "PJtrQGoo8V-S"
      },
      "id": "PJtrQGoo8V-S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>llama_kv_cache_unified::ubatch_heads</summary>\n",
        "\n",
        "```c++\n",
        "using llama_kv_cache_unified::ubatch_heads = std::vector<uint32_t>;\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "llama_kv_cache_unified::ubatch_heads res;\n",
        "\n",
        "struct state {\n",
        "    uint32_t head_old; // old position of the head, before placing the ubatch\n",
        "    uint32_t head_new; // new position of the head, after placing the ubatch\n",
        "\n",
        "    llama_kv_cells_unified cells; // copy of the old cells, before placing the ubatch\n",
        "};\n",
        "\n",
        "// remember the old state of the cells so we can restore it in the end\n",
        "std::vector<state> states;\n",
        "\n",
        "bool success = true;\n",
        "//for (const auto & ubatch : ubatches)\n",
        "// only find a suitable slot for the ubatch. don't modify the cells yet\n",
        "--> llama_kv_cache_unified::find_slot ---> const int32_t head_new;\n",
        "// remeber the position that we found\n",
        "res.push_back(head_new);\n",
        "// store the old state of the cells in the recovery stack\n",
        "--> states.push_back({head, (uint32_t) head_new, cells.cp(head_new, ubatch.n_tokens)});\n",
        "// forfind_slot\n",
        "// now emplace the ubatch\n",
        "--> llama_kv_cache_unified::apply_ubatch;\n",
        "// iterate backwards and restore the cells to their original state\n",
        "for (auto it = states.rbegin(); it != states.rend(); ++it)\n",
        "    --> cells.set(it->head_new, it->cells);\n",
        "    head = it->head_old;\n",
        "return res;\n",
        "```"
      ],
      "metadata": {
        "id": "uL0224yoMgHW"
      },
      "id": "uL0224yoMgHW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`llama_kv_cache_unified::find_slot`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L510)\n",
        "\n",
        "```c++\n",
        "const int32_t head_new = find_slot(ubatch);\n",
        "----------\n",
        "int32_t llama_kv_cache_unified::find_slot(const llama_ubatch & ubatch) const {}\n",
        "```"
      ],
      "metadata": {
        "id": "iZH5uaEhRdpd"
      },
      "id": "iZH5uaEhRdpd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "const uint32_t n_tokens = ubatch.n_tokens;\n",
        "uint32_t head_cur = this->head;\n",
        "\n",
        "// if we have enough unused cells before the current head ->\n",
        "// better to start searching from the beginning of the cache, hoping to fill it\n",
        "if (head_cur > cells.get_used() + 2*n_tokens) head_cur = 0;\n",
        "while (true) {\n",
        "    if (head_cur + n_tokens > cells.size()) {\n",
        "        head_cur = 0;\n",
        "        continue;\n",
        "    }\n",
        "    bool found = true;\n",
        "    for (uint32_t i = 0; i < n_tokens; i++) {\n",
        "        // can we use this cell? either:\n",
        "        //  - the cell is empty\n",
        "        //  - the cell is occupied only by one sequence: <-why?:/\n",
        "        //    - (disabled) mask causally, if the sequence is the same as the one we are inserting\n",
        "        //    - mask SWA, using current max pos for that sequence in the cache, always insert in the cell with minimum pos\n",
        "        bool can_use = cells.is_empty(head_cur + i);\n",
        "        if (!can_use && cells.seq_count(head_cur + i) == 1) {\n",
        "            const llama_pos pos_cell = cells.pos_get(head_cur + i);\n",
        "            const llama_seq_id seq_id_cell = cells.seq_get(head_cur + i);\n",
        "            // (disabled) causal mask\n",
        "            // note: it's better to purge any \"future\" tokens beforehand\n",
        "            //if (cells.seq_has(head_cur + i, seq_id)) {\n",
        "            //    can_use = pos_cell >= pos;\n",
        "            //}\n",
        "            // SWA mask\n",
        "            if (!can_use && is_masked_swa(pos_cell, cells.seq_pos_max(seq_id_cell) + 1)) can_use = true;\n",
        "        }\n",
        "        if (!can_use) {\n",
        "            found = false;\n",
        "            head_cur += i + 1;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "    if (found) break;\n",
        "}\n",
        "return head_cur;\n",
        "```"
      ],
      "metadata": {
        "id": "GwxYfFD7yHOZ"
      },
      "id": "GwxYfFD7yHOZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`llama_kv_cache_unified::apply_ubatch`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L646)\n",
        "\n",
        "```c++\n",
        "apply_ubatch(head_new, ubatch);\n",
        "----------\n",
        "void llama_kv_cache_unified::apply_ubatch(uint32_t head_cur, const llama_ubatch & ubatch) {}\n",
        "```"
      ],
      "metadata": {
        "id": "IOFSsjIuRdv3"
      },
      "id": "IOFSsjIuRdv3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "// keep track of the max sequence position that we would overwrite with this ubatch for non-SWA cache, this would be always empty\n",
        "llama_seq_id seq_pos_max_rm[LLAMA_MAX_SEQ];\n",
        "for (int s = 0; s < LLAMA_MAX_SEQ; ++s) {\n",
        "    seq_pos_max_rm[s] = -1;\n",
        "}\n",
        "\n",
        "for (uint32_t s = 0; s < ubatch.n_seqs; ++s) {\n",
        "    // for (uint32_t j = 0; j < ubatch.n_seq_tokens; ++j) {\n",
        "        // const uint32_t idx = s*ubatch.n_seq_tokens + j;\n",
        "        // head_cur + idx\n",
        "    if (!cells.is_empty(head_cur + s)) {\n",
        "        assert(cells.seq_count(head_cur + s) == 1);\n",
        "        const llama_seq_id seq_id = cells.seq_get(head_cur + s);\n",
        "        const llama_pos    pos    = cells.pos_get(head_cur + s);\n",
        "        seq_pos_max_rm[seq_id] = std::max(seq_pos_max_rm[seq_id], pos);\n",
        "        cells.rm(head_cur + s);\n",
        "    }\n",
        "    --> cells.pos_set(head_cur + s, ubatch.pos[idx]);\n",
        "    // TODO: fix indexing [UBATCH_IDX]\n",
        "    for (int32_t i = 0; i < ubatch.n_seq_id[s]; i++) {\n",
        "        --> cells.seq_add(head_cur + s, ubatch.seq_id[s][i]);\n",
        "    }\n",
        "}\n",
        "// move the head at the end of the slot\n",
        "head = head_cur + ubatch.n_tokens;\n",
        "```"
      ],
      "metadata": {
        "id": "uWlt7VDg114O"
      },
      "id": "uWlt7VDg114O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`llama_kv_cache_unified_state::llama_kv_cache_unified_state`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L1747)\n",
        "\n",
        "```c++\n",
        "return std::make_unique<llama_kv_cache_unified_state>(\n",
        "        this, std::move(sbatch), std::move(heads), std::move(ubatches));\n",
        "----------\n",
        "llama_kv_cache_unified_state::llama_kv_cache_unified_state(\n",
        "        llama_kv_cache_unified * kv,\n",
        "        llama_sbatch sbatch,\n",
        "        llama_kv_cache_unified::ubatch_heads heads,\n",
        "        std::vector<llama_ubatch> ubatches) :\n",
        "        status(LLAMA_MEMORY_STATUS_SUCCESS), kv(kv), sbatch(std::move(sbatch)),\n",
        "        heads(std::move(heads)), ubatches(std::move(ubatches)) {\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "QydNDnD89G52"
      },
      "id": "QydNDnD89G52"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[`llama_context::process_ubatch`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L681)\n",
        "\n",
        "```c++\n",
        "const auto res = process_ubatch(ubatch, LLM_GRAPH_TYPE_DECODER, mstate.get(), status);\n",
        "----------\n",
        "llm_graph_result_ptr llama_context::process_ubatch(const llama_ubatch & ubatch, llm_graph_type gtype, llama_memory_state_i * mstate, ggml_status & ret) {}\n",
        "```"
      ],
      "metadata": {
        "id": "aWinbl5DJviV"
      },
      "id": "aWinbl5DJviV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "--> graph_init --> ggml_cgraph * gf;\n",
        "// auto res = graph_build(ctx_compute.get(), gf, ubatch, LLM_GRAPH_TYPE_DECODER, mstate);\n",
        "--> llama_context::graph_build --> llama_model::build_graph --> llm_build_llama::llm_build_llama --> std::unique_ptr<llm_graph_result> res;\n",
        "--> ggml_backend_sched_alloc_graph;\n",
        "--> ggml_backend_cpu_buffer_set_tensor;/llama_kv_cache_unified::set_input_kq_mask;\n",
        "--> llama_context::graph_compute --> ggml_backend_sched_graph_compute_async --> ggml_status ggml_backend_sched_compute_splits --> ggml_backend_graph_compute_async --> ggml_backend_cpu_graph_compute;\n",
        "ret = GGML_STATUS_SUCCESS;\n",
        "return res;\n",
        "```"
      ],
      "metadata": {
        "id": "8ncESwyLERiq"
      },
      "id": "8ncESwyLERiq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`ggml_backend_sched_alloc_graph`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1565)\n",
        "\n",
        "```c++\n",
        "ggml_backend_sched_alloc_graph(sched.get(), gf);\n",
        "----------\n",
        "bool ggml_backend_sched_alloc_graph(ggml_backend_sched_t sched, struct ggml_cgraph * graph) {}\n",
        "```"
      ],
      "metadata": {
        "id": "gcsgUWocJbi2"
      },
      "id": "gcsgUWocJbi2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "--> ggml_backend_sched_split_graph;\n",
        "--> ggml_backend_sched_alloc_splits;\n",
        "sched->is_alloc = true;\n",
        "return true;\n",
        "```"
      ],
      "metadata": {
        "id": "92LEIhp-A6UC"
      },
      "id": "92LEIhp-A6UC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`ggml_backend_sched_alloc_splits`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1321)\n",
        "\n",
        "```c++\n",
        "ggml_backend_sched_alloc_splits(sched);\n",
        "----------\n",
        "static bool ggml_backend_sched_alloc_splits(ggml_backend_sched_t sched) {}\n",
        "```"
      ],
      "metadata": {
        "id": "xglyOTGzDaV3"
      },
      "id": "xglyOTGzDaV3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "bool backend_ids_changed = false;\n",
        "for (int i = 0; i < sched->graph.n_nodes; i++) {\n",
        "    if (sched->node_backend_ids[i] != sched->prev_node_backend_ids[i] &&\n",
        "        sched->bufts[sched->node_backend_ids[i]] != sched->bufts[sched->prev_node_backend_ids[i]]) {\n",
        "        backend_ids_changed = true;\n",
        "        break;\n",
        "    }\n",
        "}\n",
        "if (!backend_ids_changed) {\n",
        "    for (int i = 0; i < sched->graph.n_leafs; i++) {\n",
        "        if (sched->leaf_backend_ids[i] != sched->prev_leaf_backend_ids[i] &&\n",
        "            sched->bufts[sched->leaf_backend_ids[i]] != sched->bufts[sched->prev_leaf_backend_ids[i]]) {\n",
        "            backend_ids_changed = true;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "// allocate graph\n",
        "--> if (backend_ids_changed || !ggml_gallocr_alloc_graph(sched->galloc, &sched->graph)) {\n",
        "    // the re-allocation may cause the split inputs to be moved to a different address\n",
        "    // synchronize without ggml_backend_sched_synchronize to avoid changing cur_copy\n",
        "    --> for (int i = 0; i < sched->n_backends; i++) ggml_backend_synchronize(sched->backends[i]);\n",
        "    --> ggml_gallocr_reserve_n(sched->galloc, &sched->graph, sched->node_backend_ids, sched->leaf_backend_ids);\n",
        "    --> if (!ggml_gallocr_alloc_graph(sched->galloc, &sched->graph)) {\n",
        "        GGML_LOG_ERROR(\"%s: failed to allocate graph\\n\", __func__);\n",
        "        return false;\n",
        "    }\n",
        "}\n",
        "return true;\n",
        "```"
      ],
      "metadata": {
        "id": "FX13m4rdGyFt"
      },
      "id": "FX13m4rdGyFt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_gallocr_alloc_graph`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L871)\n",
        "\n",
        "```c++\n",
        "ggml_gallocr_alloc_graph(sched->galloc, &sched->graph);\n",
        "----------\n",
        "bool ggml_gallocr_alloc_graph(ggml_gallocr_t galloc, struct ggml_cgraph * graph) {}\n",
        "```"
      ],
      "metadata": {
        "id": "yFqlchbRXXKH"
      },
      "id": "yFqlchbRXXKH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "--> if (ggml_gallocr_needs_realloc(galloc, graph)) {\n",
        "    if (galloc->n_buffers == 1) {\n",
        "        --> if (!ggml_gallocr_reserve(galloc, graph)) return false;\n",
        "            return ggml_gallocr_reserve_n(galloc, graph, NULL, NULL);\n",
        "    } else return false;\n",
        "}\n",
        "// reset buffers\n",
        "for (int i = 0; i < galloc->n_buffers; i++) {\n",
        "    if (galloc->buffers[i] != NULL) {\n",
        "        --> ggml_backend_buffer_reset(galloc->buffers[i]);\n",
        "    }\n",
        "}\n",
        "// allocate the graph tensors from the previous assignments leafs\n",
        "for (int i = 0; i < graph->n_leafs; i++) {\n",
        "    struct ggml_tensor * leaf = graph->leafs[i];\n",
        "    struct leaf_alloc * leaf_alloc = &galloc->leaf_allocs[i];\n",
        "    --> ggml_gallocr_init_tensor(galloc, leaf, &leaf_alloc->leaf);\n",
        "}\n",
        "// nodes\n",
        "for (int i = 0; i < graph->n_nodes; i++) {\n",
        "    struct ggml_tensor * node = graph->nodes[i];\n",
        "    struct node_alloc * node_alloc = &galloc->node_allocs[i];\n",
        "    for (int j = 0; j < GGML_MAX_SRC; j++) {\n",
        "        struct ggml_tensor * src = node->src[j];\n",
        "        if (src == NULL) continue;\n",
        "        --> ggml_gallocr_init_tensor(galloc, src, &node_alloc->src[j]);\n",
        "    }\n",
        "    --> ggml_gallocr_init_tensor(galloc, node, &node_alloc->dst);\n",
        "}\n",
        "\n",
        "return true;\n",
        "```"
      ],
      "metadata": {
        "id": "aXgj_ePsX6mH"
      },
      "id": "aXgj_ePsX6mH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_gallocr_needs_realloc`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L828)\n",
        "\n",
        "```c++\n",
        "ggml_gallocr_needs_realloc(galloc, graph);\n",
        "----------\n",
        "static bool ggml_gallocr_needs_realloc(ggml_gallocr_t galloc, struct ggml_cgraph * graph) {\n",
        "    if (galloc->n_nodes != graph->n_nodes) return true;\n",
        "    if (galloc->n_leafs != graph->n_leafs) return true;\n",
        "    for (int i = 0; i < graph->n_nodes; i++) {\n",
        "        struct ggml_tensor * node = graph->nodes[i];\n",
        "        struct node_alloc * node_alloc = &galloc->node_allocs[i];\n",
        "        --> if (!ggml_gallocr_node_needs_realloc(galloc, node, &node_alloc->dst)) return true;\n",
        "        for (int j = 0; j < GGML_MAX_SRC; j++) {\n",
        "            struct ggml_tensor * src = node->src[j];\n",
        "            if (src == NULL) continue;\n",
        "            --> if (!ggml_gallocr_node_needs_realloc(galloc, src, &node_alloc->src[j])) return true;\n",
        "        }\n",
        "    }\n",
        "    return false;\n",
        "}\n",
        "\n",
        "static bool ggml_gallocr_node_needs_realloc(ggml_gallocr_t galloc, struct ggml_tensor * node, struct tensor_alloc * talloc) {\n",
        "    size_t node_size = 0;\n",
        "    if (!node->data && !node->view_src) {\n",
        "        // If we previously had data but don't now then reallocate\n",
        "        if (talloc->buffer_id < 0) return false;\n",
        "        node_size = ggml_backend_buft_get_alloc_size(galloc->bufts[talloc->buffer_id], node);\n",
        "    }\n",
        "    return talloc->size_max >= node_size;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "lJoCmrBHWKrY"
      },
      "id": "lJoCmrBHWKrY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_gallocr_init_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-alloc.c#L787)\n",
        "\n",
        "```c++\n",
        "ggml_gallocr_init_tensor(galloc, leaf, &leaf_alloc->leaf);\n",
        "ggml_gallocr_init_tensor(galloc, src, &node_alloc->src[j]);\n",
        "ggml_gallocr_init_tensor(galloc, node, &node_alloc->dst);\n",
        "----------\n",
        "static void ggml_gallocr_init_tensor(ggml_gallocr_t galloc, struct ggml_tensor * tensor, struct tensor_alloc * tensor_alloc) {\n",
        "    int buffer_id = tensor_alloc->buffer_id;\n",
        "    if (tensor->view_src != NULL && tensor->buffer == NULL && tensor->view_src->buffer != NULL)\n",
        "        --> ggml_backend_view_init(tensor);\n",
        "    if (tensor->view_src == NULL && tensor->data == NULL)\n",
        "        void * base = ggml_backend_buffer_get_base(galloc->buffers[buffer_id]);\n",
        "        void * addr = (char *)base + tensor_alloc->offset;\n",
        "        --> ggml_backend_tensor_alloc(galloc->buffers[buffer_id], tensor, addr);\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "7Lqi1uMDpF4D"
      },
      "id": "7Lqi1uMDpF4D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`ggml_backend_cpu_buffer_set_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1890)\n",
        "\n",
        "```c++\n",
        "res->set_inputs(&ubatch);\n",
        "llm_graph_result::set_inputs -->\n",
        "    for (auto & input : inputs) input->set_input(ubatch);\n",
        "    llm_graph_input_embd::set_input;\n",
        "    llm_graph_input_pos::set_input;\n",
        "\n",
        "ggml_backend_tensor_set(tokens, ubatch->token, 0, n_tokens*ggml_element_size(tokens));\n",
        "ggml_backend_tensor_set(pos, ubatch->pos, 0, n_tokens*n_pos_per_embd*ggml_element_size(pos));\n",
        "----------\n",
        "void ggml_backend_tensor_set(struct ggml_tensor * tensor, const void * data, size_t offset, size_t size) {\n",
        "    ggml_backend_buffer_t buf = tensor->view_src ? tensor->view_src->buffer : tensor->buffer;\n",
        "    if (size == 0) {\n",
        "        return;\n",
        "    }\n",
        "    --> buf->iface.set_tensor(buf, tensor, data, offset, size);\n",
        "}\n",
        "\n",
        "static void ggml_backend_cpu_buffer_set_tensor(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor, const void * data, size_t offset, size_t size)\n",
        "    memcpy((char *)tensor->data + offset, data, size);\n",
        "```"
      ],
      "metadata": {
        "id": "4dtmaghYQRaC"
      },
      "id": "4dtmaghYQRaC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [`llama_kv_cache_unified::set_input_kq_mask`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-kv-cache-unified.cpp#L794)\n",
        "\n",
        "```c++\n",
        "res->set_inputs(&ubatch);\n",
        "llm_graph_result::set_inputs -->\n",
        "    for (auto & input : inputs) input->set_input(ubatch);\n",
        "----------\n",
        "void llm_graph_input_attn_kv_unified::set_input(const llama_ubatch * ubatch) {\n",
        "    --> kv_state->set_input_kq_mask(self_kq_mask, ubatch, cparams.causal_attn);\n",
        "}\n",
        "\n",
        "void llama_kv_cache_unified_state::set_input_kq_mask(ggml_tensor * dst, const llama_ubatch * ubatch, bool causal_attn) const {\n",
        "    --> kv->set_input_kq_mask(dst, ubatch, causal_attn);\n",
        "}\n",
        "\n",
        "void llama_kv_cache_unified::set_input_kq_mask(ggml_tensor * dst, const llama_ubatch * ubatch, bool causal_attn) const {}\n",
        "```"
      ],
      "metadata": {
        "id": "_S6sI2iMU6g5"
      },
      "id": "_S6sI2iMU6g5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "const uint32_t n_tokens     = ubatch->n_tokens;\n",
        "const uint32_t n_seq_tokens = ubatch->n_seq_tokens;\n",
        "const uint32_t n_seqs       = ubatch->n_seqs;\n",
        "\n",
        "float * data = (float *) dst->data;\n",
        "const int64_t n_kv = dst->ne[0];\n",
        "// Use only the previous KV cells of the correct sequence for each token of the ubatch.\n",
        "// It's assumed that if a token in the batch has multiple sequences, they are equivalent.\n",
        "// Example with a cache of 10 tokens, 2 tokens populated in cache and 3 tokens in batch:\n",
        "//   Causal mask:\n",
        "//      xxx-------\n",
        "//      xxxx------\n",
        "//      xxxxx-----\n",
        "//   Non-causal mask:\n",
        "//      xxxxx-----\n",
        "//      xxxxx-----\n",
        "//      xxxxx-----\n",
        "// To visualize the mask, see https://github.com/ggml-org/llama.cpp/pull/12615\n",
        "for (uint32_t s = 0; s < n_seqs; ++s) {\n",
        "    const llama_seq_id seq_id = ubatch->seq_id[s][0];\n",
        "    // for (uint32_t j = 0; j < n_seq_tokens; ++j)\n",
        "        // const uint32_t idx = s*n_seq_tokens + j;\n",
        "    const llama_pos p1 = ubatch->pos[s];\n",
        "    for (uint32_t i = 0; i < n_kv; ++i) {\n",
        "        float f = 0.0f;\n",
        "        bool masked = false;\n",
        "        if (cells.is_empty(i)) {\n",
        "            masked = true;\n",
        "        } else {\n",
        "            const llama_pos p0 = cells.pos_get(i);\n",
        "            // mask the token if not the same sequence\n",
        "            masked = masked || (!cells.seq_has(i, seq_id));\n",
        "            // mask future tokens\n",
        "            masked = masked || (causal_attn && p0 > p1);\n",
        "            // apply SWA if any\n",
        "            masked = masked || (is_masked_swa(p0, p1));\n",
        "            if (!masked && hparams.use_alibi) f = -std::abs(p0 - p1);\n",
        "        }\n",
        "        if (masked) f = -INFINITY;\n",
        "        data[n_kv*n_tokens + s*n_kv + i] = f;\n",
        "    }\n",
        "}\n",
        "\n",
        "// mask padded tokens\n",
        "if (data) {\n",
        "    for (uint32_t j = n_tokens; j < GGML_PAD(n_tokens, GGML_KQ_MASK_PAD); ++j) {\n",
        "        for (uint32_t i = 0; i < n_kv; ++i) {\n",
        "            data[n_kv*n_tokens + j*n_kv + i] = -INFINITY;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "DK8bnYH2WHlq"
      },
      "id": "DK8bnYH2WHlq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***[`ggml_backend_cpu_graph_compute`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.cpp#L153)\n",
        "\n",
        "- [`llama_context::graph_compute`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L1370)\n",
        "- [`ggml_backend_sched_compute_splits`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1360)\n",
        "\n",
        "```c++\n",
        "const auto status = graph_compute(gf, ubatch.n_tokens > 1);\n",
        "----------\n",
        "ggml_status llama_context::graph_compute(\n",
        "            ggml_cgraph * gf,\n",
        "                   bool   batched) {\n",
        "    int n_threads        = batched ? cparams.n_threads_batch : cparams.n_threads;\n",
        "    \n",
        "    ggml_threadpool_t tp = batched ? threadpool_batch        : threadpool;\n",
        "    auto * reg = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu));\n",
        "    auto * set_threadpool_fn = (decltype(ggml_backend_cpu_set_threadpool) *) ggml_backend_reg_get_proc_address(reg, \"ggml_backend_cpu_set_threadpool\");\n",
        "    set_threadpool_fn(backend_cpu, tp);\n",
        "        void ggml_backend_cpu_set_threadpool(ggml_backend_t backend_cpu, ggml_threadpool_t threadpool) {}\n",
        "    \n",
        "    // set the number of threads for all the backends\n",
        "    for (const auto & set_n_threads_fn : set_n_threads_fns)\n",
        "        set_n_threads_fn.second(set_n_threads_fn.first, n_threads);\n",
        "            void ggml_backend_cpu_set_n_threads(ggml_backend_t backend_cpu, int n_threads) {}\n",
        "    \n",
        "    --> auto status = ggml_backend_sched_graph_compute_async(sched.get(), gf);\n",
        "    return status;\n",
        "}\n",
        "\n",
        "enum ggml_status ggml_backend_sched_graph_compute_async(ggml_backend_sched_t sched, struct ggml_cgraph * graph) {\n",
        "    if (!sched->is_reset && !sched->is_alloc) ggml_backend_sched_reset(sched);\n",
        "    if (!sched->is_alloc)\n",
        "        if (!ggml_backend_sched_alloc_graph(sched, graph)) return GGML_STATUS_ALLOC_FAILED;\n",
        "    --> return ggml_backend_sched_compute_splits(sched);\n",
        "}\n",
        "\n",
        "static enum ggml_status ggml_backend_sched_compute_splits(ggml_backend_sched_t sched) {\n",
        "    struct ggml_backend_sched_split * splits = sched->splits;\n",
        "    // for (int i = 0; i < sched->n_splits; i++)\n",
        "    struct ggml_backend_sched_split * split = &splits[0];\n",
        "    int split_backend_id = split->backend_id;\n",
        "    ggml_backend_t split_backend = sched->backends[split_backend_id];\n",
        "    // copy the input tensors to the split backend\n",
        "    for (int j = 0; j < split->n_inputs; j++) {}\n",
        "\n",
        "    --> ggml_backend_graph_compute_async(split_backend, &split->graph);\n",
        "\n",
        "    // record the event of this copy\n",
        "    if (split->n_inputs > 0 && sched->events[split_backend_id][sched->cur_copy] != NULL) {}\n",
        "\n",
        "    sched->cur_copy = (sched->cur_copy + 1) % sched->n_copies;\n",
        "    return GGML_STATUS_SUCCESS;\n",
        "}\n",
        "\n",
        "enum ggml_status ggml_backend_graph_compute_async(ggml_backend_t backend, struct ggml_cgraph * cgraph) {\n",
        "    --> return backend->iface.graph_compute(backend, cgraph);\n",
        "}\n",
        "\n",
        "static enum ggml_status ggml_backend_cpu_graph_compute(ggml_backend_t backend, struct ggml_cgraph * cgraph) {}\n",
        "```"
      ],
      "metadata": {
        "id": "EwLiKExAOzNZ"
      },
      "id": "EwLiKExAOzNZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "struct ggml_backend_cpu_context * cpu_ctx = (struct ggml_backend_cpu_context *)backend->context;\n",
        "\n",
        "--> ggml_graph_plan --> struct ggml_cplan cplan;\n",
        "\n",
        "if (cpu_ctx->work_size < cplan.work_size) {\n",
        "    delete[] cpu_ctx->work_data;\n",
        "    cpu_ctx->work_data = new uint8_t[cplan.work_size];\n",
        "    cpu_ctx->work_size = cplan.work_size;\n",
        "}\n",
        "cplan.work_data = (uint8_t *)cpu_ctx->work_data;\n",
        "--> return ggml_graph_compute;\n",
        "```"
      ],
      "metadata": {
        "id": "jCA4R77KBi6q"
      },
      "id": "jCA4R77KBi6q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### [`ggml_graph_plan`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.c#L2674)\n",
        "\n",
        "<details>\n",
        "<summary>struct ggml_cplan</summary>\n",
        "\n",
        "```c++\n",
        "// the compute plan that needs to be prepared for ggml_graph_compute()\n",
        "// since https://github.com/ggml-org/ggml/issues/287\n",
        "struct ggml_cplan {\n",
        "    size_t    work_size; // size of work buffer, calculated by `ggml_graph_plan()`\n",
        "    uint8_t * work_data; // work buffer, to be allocated by caller before calling to `ggml_graph_compute()`\n",
        "\n",
        "    int n_threads;\n",
        "    struct ggml_threadpool * threadpool;\n",
        "\n",
        "    // abort ggml_graph_compute when true\n",
        "    ggml_abort_callback abort_callback;\n",
        "    void *              abort_callback_data;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "struct ggml_cplan cplan = ggml_graph_plan(cgraph, cpu_ctx->n_threads, cpu_ctx->threadpool);\n",
        "----------\n",
        "struct ggml_cplan ggml_graph_plan(\n",
        "          const struct ggml_cgraph * cgraph,\n",
        "                               int   n_threads,\n",
        "            struct ggml_threadpool * threadpool) {}\n",
        "```"
      ],
      "metadata": {
        "id": "HD0RdhZYCsRz"
      },
      "id": "HD0RdhZYCsRz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>struct ggml_type_traits_cpu</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_type_traits_cpu {\n",
        "    ggml_from_float_t        from_float;\n",
        "    ggml_vec_dot_t           vec_dot;\n",
        "    enum ggml_type           vec_dot_type;\n",
        "    int64_t                  nrows; // number of rows to process simultaneously\n",
        "};\n",
        "\n",
        "[GGML_TYPE_Q4_0] = {\n",
        "    .from_float               = quantize_row_q4_0,\n",
        "    .vec_dot                  = ggml_vec_dot_q4_0_q8_0,\n",
        "    .vec_dot_type             = GGML_TYPE_Q8_0,\n",
        "    .nrows                    = 1,\n",
        "},\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "size_t work_size = 0;\n",
        "struct ggml_cplan cplan;\n",
        "memset(&cplan, 0, sizeof(struct ggml_cplan));\n",
        "int max_tasks = 1;\n",
        "// thread scheduling for the different operations + work buffer size estimation\n",
        "for (int i = 0; i < cgraph->n_nodes; i++)\n",
        "    struct ggml_tensor * node = cgraph->nodes[i];\n",
        "    const int n_tasks = ggml_get_n_tasks(node, n_threads);\n",
        "    max_tasks = MAX(max_tasks, n_tasks);\n",
        "    size_t cur = 0;\n",
        "    // extra\n",
        "    // if (!ggml_cpu_extra_work_size(n_threads, node, &cur));\n",
        "    switch (node->op)\n",
        "        case GGML_OP_MUL_MAT:\n",
        "            {\n",
        "                const enum ggml_type vec_dot_type = type_traits_cpu[node->src[0]->type].vec_dot_type;\n",
        "\n",
        "                if (node->src[1]->type != vec_dot_type) {\n",
        "                    cur = ggml_row_size(vec_dot_type, ggml_nelements(node->src[1]));\n",
        "                }\n",
        "            } break;\n",
        "        case GGML_OP_SOFT_MAX:\n",
        "        case GGML_OP_ROPE:\n",
        "        case GGML_OP_ROPE_BACK:\n",
        "            {\n",
        "                cur = ggml_type_size(GGML_TYPE_F32) * node->ne[0] * n_tasks;\n",
        "            } break;\n",
        "        case GGML_OP_CPY:\n",
        "        case GGML_OP_DUP:\n",
        "            {\n",
        "                if (ggml_is_quantized(node->type) ||\n",
        "                    // F16 -> BF16 and BF16 -> F16 copies go through intermediate F32\n",
        "                    (node->src[0]->type == GGML_TYPE_F16  && node->src[1] && node->src[1]->type == GGML_TYPE_BF16) ||\n",
        "                    (node->src[0]->type == GGML_TYPE_BF16 && node->src[1] && node->src[1]->type == GGML_TYPE_F16)) {\n",
        "                    cur = ggml_type_size(GGML_TYPE_F32) * node->ne[0] * n_tasks;\n",
        "                }\n",
        "            } break;\n",
        "        case GGML_OP_ADD:\n",
        "        case GGML_OP_ADD1:\n",
        "            {\n",
        "                if (ggml_is_quantized(node->src[0]->type)) {\n",
        "                    cur = ggml_type_size(GGML_TYPE_F32) * node->src[0]->ne[0] * n_tasks;\n",
        "                }\n",
        "            } break;\n",
        "    work_size = MAX(work_size, cur);\n",
        "\n",
        "work_size += CACHE_LINE_SIZE*(n_threads);\n",
        "cplan.threadpool = threadpool;\n",
        "cplan.n_threads  = MIN(max_tasks, n_threads);\n",
        "cplan.work_size  = work_size;\n",
        "cplan.work_data  = NULL;\n",
        "\n",
        "return cplan;\n",
        "```"
      ],
      "metadata": {
        "id": "EtHDLo8gJEj4"
      },
      "id": "EtHDLo8gJEj4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ****[`ggml_graph_compute`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.c#L3118)\n",
        "\n",
        "<details>\n",
        "<summary>struct ggml_threadpool</summary>\n",
        "\n",
        "```c++\n",
        "// Threadpool def\n",
        "struct ggml_threadpool {\n",
        "    ggml_mutex_t mutex;       // mutex for cond.var\n",
        "    ggml_cond_t  cond;        // cond.var for waiting for new work\n",
        "\n",
        "    struct ggml_cgraph * cgraph;\n",
        "    struct ggml_cplan  * cplan;\n",
        "\n",
        "    // synchronization primitives\n",
        "    atomic_int n_graph;       // incremented when there is work to be done (i.e each graph)\n",
        "    atomic_int GGML_CACHE_ALIGN n_barrier;\n",
        "    atomic_int GGML_CACHE_ALIGN n_barrier_passed;\n",
        "    atomic_int GGML_CACHE_ALIGN current_chunk; // currently processing chunk during Mat_Mul, shared between all the threads.\n",
        "\n",
        "    // these are atomic as an annotation for thread-sanitizer\n",
        "    atomic_bool stop;         // Used for stopping the threadpool altogether\n",
        "    atomic_bool pause;        // Used for pausing the threadpool or individual threads\n",
        "    atomic_int abort;         // Used for aborting processing of a graph\n",
        "\n",
        "    struct ggml_compute_state * workers;   // per thread state\n",
        "    int          n_threads_max; // number of threads in the pool\n",
        "    atomic_int   n_threads_cur; // number of threads used in the current graph\n",
        "\n",
        "    int32_t      prio;        // Scheduling priority\n",
        "    uint32_t     poll;        // Polling level (0 - no polling)\n",
        "\n",
        "    enum ggml_status ec;\n",
        "};\n",
        "\n",
        "void ggml_threadpool_params_init(struct ggml_threadpool_params * p, int n_threads) {\n",
        "    p->n_threads  = n_threads;\n",
        "    p->prio       = 0;     // default priority (usually means normal or inherited)\n",
        "    p->poll       = 50;    // hybrid-polling enabled\n",
        "    p->strict_cpu = false; // no strict placement (all threads share same cpumask)\n",
        "    p->paused     = false; // threads are ready to go\n",
        "    memset(p->cpumask, 0, GGML_MAX_N_THREADS); // all-zero means use the default affinity (usually inherited)\n",
        "}\n",
        "\n",
        "struct ggml_threadpool_params ggml_threadpool_params_default(int n_threads) {\n",
        "    struct ggml_threadpool_params p;\n",
        "    ggml_threadpool_params_init(&p, n_threads);\n",
        "    return p;\n",
        "}\n",
        "\n",
        "static struct ggml_threadpool * ggml_threadpool_new_impl(\n",
        "    struct ggml_threadpool_params * tpp,\n",
        "               struct ggml_cgraph * cgraph,\n",
        "                struct ggml_cplan * cplan) {\n",
        "\n",
        "    struct ggml_threadpool * threadpool =\n",
        "        ggml_aligned_malloc(sizeof(struct ggml_threadpool));\n",
        "    {\n",
        "        threadpool->cgraph           = cgraph;\n",
        "        threadpool->cplan            = cplan;\n",
        "        threadpool->n_graph          = 0;\n",
        "        threadpool->n_barrier        = 0;\n",
        "        threadpool->n_barrier_passed = 0;\n",
        "        threadpool->current_chunk    = 0;\n",
        "        threadpool->stop             = false;\n",
        "        threadpool->pause            = tpp->paused;\n",
        "        threadpool->abort            = -1;\n",
        "        threadpool->workers          = NULL;\n",
        "        threadpool->n_threads_max    = tpp->n_threads;\n",
        "        threadpool->n_threads_cur    = tpp->n_threads;\n",
        "        threadpool->poll             = tpp->poll;\n",
        "        threadpool->prio             = tpp->prio;\n",
        "        threadpool->ec               = GGML_STATUS_SUCCESS;\n",
        "    }\n",
        "\n",
        "    // Allocate and init workers state\n",
        "    const size_t workers_size = sizeof(struct ggml_compute_state) * tpp->n_threads;\n",
        "    struct ggml_compute_state * workers = ggml_aligned_malloc(workers_size);\n",
        "\n",
        "    memset(workers, 0, workers_size);\n",
        "    for (int j = 0; j < tpp->n_threads; j++) {\n",
        "        workers[j].threadpool = threadpool;\n",
        "        workers[j].ith        = j;\n",
        "    }\n",
        "\n",
        "    threadpool->workers = workers;\n",
        "\n",
        "    return threadpool;\n",
        "}\n",
        "\n",
        "void ggml_threadpool_free(struct ggml_threadpool* threadpool) {\n",
        "    if (!threadpool) return;\n",
        "    const int n_threads = threadpool->n_threads_max;\n",
        "    const size_t workers_size = sizeof(struct ggml_compute_state) * n_threads;\n",
        "    ggml_aligned_free(threadpool->workers, workers_size);\n",
        "    ggml_aligned_free(threadpool, sizeof(struct ggml_threadpool));\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>struct ggml_threadpool_params</summary>\n",
        "\n",
        "```c++\n",
        "// threadpool params\n",
        "// Use ggml_threadpool_params_default() or ggml_threadpool_params_init() to populate the defaults\n",
        "struct ggml_threadpool_params {\n",
        "    bool                cpumask[GGML_MAX_N_THREADS]; // mask of cpu cores (all-zeros means use default affinity settings)\n",
        "    int                 n_threads;                   // number of threads\n",
        "    enum ggml_sched_priority prio;                   // thread priority\n",
        "    uint32_t            poll;                        // polling level (0 - no polling, 100 - aggressive polling)\n",
        "    bool                strict_cpu;                  // strict cpu placement\n",
        "    bool                paused;                      // start in paused state\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "ggml_graph_compute(cgraph, &cplan);\n",
        "----------\n",
        "enum ggml_status ggml_graph_compute(struct ggml_cgraph * cgraph, struct ggml_cplan * cplan) {}\n",
        "```"
      ],
      "metadata": {
        "id": "ZeasLYKSC9_s"
      },
      "id": "ZeasLYKSC9_s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "ggml_cpu_init();\n",
        "int n_threads = cplan->n_threads;\n",
        "struct ggml_threadpool * threadpool = cplan->threadpool;\n",
        "bool disposable_threadpool = false;\n",
        "if (threadpool == NULL) {\n",
        "    //GGML_PRINT_DEBUG(\"Threadpool is not specified. Will create a disposable threadpool : n_threads %d\\n\", n_threads);\n",
        "    disposable_threadpool = true;\n",
        "\n",
        "    struct ggml_threadpool_params ttp = ggml_threadpool_params_default(n_threads);\n",
        "    threadpool = ggml_threadpool_new_impl(&ttp, cgraph, cplan);\n",
        "}\n",
        "\n",
        "if (n_threads > 1) {\n",
        "    #pragma omp parallel num_threads(n_threads)\n",
        "    {\n",
        "        #pragma omp single\n",
        "        {\n",
        "            // update the number of threads from the actual number of threads that we got from OpenMP\n",
        "            n_threads = omp_get_num_threads();\n",
        "            atomic_store_explicit(&threadpool->n_threads_cur, n_threads, memory_order_relaxed);\n",
        "        }\n",
        "        --> ggml_graph_compute_thread(&threadpool->workers[omp_get_thread_num()]);\n",
        "    }\n",
        "} else {\n",
        "    atomic_store_explicit(&threadpool->n_threads_cur, 1, memory_order_relaxed);\n",
        "    --> ggml_graph_compute_thread(&threadpool->workers[0]);\n",
        "}\n",
        "enum ggml_status ret = threadpool->ec;\n",
        "\n",
        "if (disposable_threadpool) {\n",
        "    ggml_threadpool_free(threadpool);\n",
        "}\n",
        "\n",
        "return ret;\n",
        "```"
      ],
      "metadata": {
        "id": "NIeRnANuUDao"
      },
      "id": "NIeRnANuUDao"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### [`ggml_graph_compute_thread`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/ggml-cpu.c#L2862)\n",
        "\n",
        "<details>\n",
        "<summary>struct ggml_compute_state</summary>\n",
        "\n",
        "```c++\n",
        "// Per-thread state\n",
        "struct ggml_compute_state {\n",
        "    struct ggml_threadpool * threadpool;\n",
        "    int ith;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>struct ggml_compute_params</summary>\n",
        "\n",
        "```c++\n",
        "struct ggml_compute_params {\n",
        "    // ith = thread index, nth = number of threads\n",
        "    int ith, nth;\n",
        "\n",
        "    // work buffer for all threads\n",
        "    size_t wsize;\n",
        "    void * wdata;\n",
        "\n",
        "    struct ggml_threadpool * threadpool;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "static thread_ret_t ggml_graph_compute_thread(void * data) {\n",
        "    struct ggml_compute_state * state = (struct ggml_compute_state *) data;\n",
        "    struct ggml_threadpool    * tp    = state->threadpool;\n",
        "\n",
        "    const struct ggml_cgraph * cgraph = tp->cgraph;\n",
        "    const struct ggml_cplan  * cplan  = tp->cplan;\n",
        "\n",
        "    struct ggml_compute_params params = {\n",
        "        /*.ith       =*/ state->ith,\n",
        "        /*.nth       =*/ atomic_load_explicit(&tp->n_threads_cur, memory_order_relaxed),\n",
        "        /*.wsize     =*/ cplan->work_size,\n",
        "        /*.wdata     =*/ cplan->work_data,\n",
        "        /*.threadpool=*/ tp,\n",
        "    };\n",
        "\n",
        "    for (int node_n = 0; node_n < cgraph->n_nodes && atomic_load_explicit(&tp->abort, memory_order_relaxed) != node_n; node_n++) {\n",
        "        struct ggml_tensor * node = cgraph->nodes[node_n];\n",
        "        --> ggml_compute_forward(&params, node);\n",
        "        if (node_n + 1 < cgraph->n_nodes) {\n",
        "            --> ggml_barrier(state->threadpool);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    --> ggml_barrier(state->threadpool);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "static void ggml_compute_forward(struct ggml_compute_params * params, struct ggml_tensor * tensor) {\n",
        "    if (tensor->op == GGML_OP_NONE || ggml_is_empty(tensor)) return;\n",
        "\n",
        "    // extra_buffer op? extra\n",
        "    if (ggml_cpu_extra_compute_forward(params, tensor)) return;\n",
        "    switch (tensor->op) {}\n",
        "}\n",
        "\n",
        "void ggml_barrier(struct ggml_threadpool * tp) {\n",
        "    int n_threads = atomic_load_explicit(&tp->n_threads_cur, memory_order_relaxed);\n",
        "    if (n_threads == 1) return;\n",
        "\n",
        "    #pragma omp barrier\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "VmDmN7blcbEA"
      },
      "id": "VmDmN7blcbEA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_compute_forward_get_rows_q`](ggml/src/ggml-cpu/ops.cpp#L4236)\n",
        "\n",
        "```c++\n",
        "ggml_compute_forward_get_rows_q(params, dst);\n",
        "----------\n",
        "static void ggml_compute_forward_get_rows_q(\n",
        "        const ggml_compute_params * params,\n",
        "              ggml_tensor * dst) {\n",
        "    const ggml_tensor * src0 = dst->src[0];\n",
        "    const ggml_tensor * src1 = dst->src[1];\n",
        "\n",
        "    GGML_TENSOR_BINARY_OP_LOCALS\n",
        "\n",
        "    const int64_t nc = ne00;\n",
        "    const int64_t nr = ggml_nelements(src1);\n",
        "\n",
        "    const ggml_type type = src0->type;\n",
        "    ggml_to_float_t const dequantize_row_q = ggml_get_type_traits(type)->to_float;\n",
        "\n",
        "    const int ith = params->ith;\n",
        "    const int nth = params->nth;\n",
        "\n",
        "    // rows per thread\n",
        "    const int dr = (nr + nth - 1)/nth;\n",
        "\n",
        "    // row range for this thread\n",
        "    const int ir0 = dr*ith;\n",
        "    const int ir1 = MIN(ir0 + dr, nr);\n",
        "\n",
        "    for (int64_t i = ir0; i < ir1; ++i) {\n",
        "        const int64_t i12 = i/(ne11*ne10);\n",
        "        const int64_t i11 = (i - i12*ne11*ne10)/ne10;\n",
        "        const int64_t i10 = (i - i12*ne11*ne10 - i11*ne10);\n",
        "        const int64_t i01 = *(int32_t *) ((char *) src1->data + i10*nb10 + i11*nb11 + i12*nb12);\n",
        "        dequantize_row_q(\n",
        "                (const void *) ((char *) src0->data + i01*nb01 + i11*nb02 + i12*nb03),\n",
        "                     (float *) ((char *)  dst->data + i10*nb1  + i11*nb2  + i12*nb3), nc);\n",
        "    }\n",
        "}\n",
        "\n",
        "void dequantize_row_q8_0(const block_q8_0 * GGML_RESTRICT x, float * GGML_RESTRICT y, int64_t k) {\n",
        "    static const int qk = QK8_0;\n",
        "    const int nb = k / qk;\n",
        "    for (int i = 0; i < nb; i++) {\n",
        "        const float d = GGML_FP16_TO_FP32(x[i].d);\n",
        "        for (int j = 0; j < qk; ++j) {\n",
        "            y[i*qk + j] = x[i].qs[j]*d;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "wLP5FfwA7VDU"
      },
      "id": "wLP5FfwA7VDU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_compute_forward_rms_norm_f32`](ggml/src/ggml-cpu/ops.cpp#L3270)\n",
        "\n",
        "```c++\n",
        "ggml_compute_forward_rms_norm(params, tensor);\n",
        "----------\n",
        "static void ggml_compute_forward_rms_norm_f32(\n",
        "        const ggml_compute_params * params,\n",
        "        ggml_tensor * dst) {\n",
        "    const ggml_tensor * src0 = dst->src[0];\n",
        "    const int ith = params->ith;\n",
        "    const int nth = params->nth;\n",
        "    GGML_TENSOR_UNARY_OP_LOCALS\n",
        "    float eps;\n",
        "    memcpy(&eps, dst->op_params, sizeof(float));\n",
        "    // TODO: optimize\n",
        "    for (int64_t i03 = 0; i03 < ne03; i03++) {\n",
        "        for (int64_t i02 = 0; i02 < ne02; i02++) {\n",
        "            for (int64_t i01 = ith; i01 < ne01; i01 += nth) {\n",
        "                const float * x = (float *) ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03);\n",
        "                ggml_float sum = 0.0;\n",
        "                for (int64_t i00 = 0; i00 < ne00; i00++) sum += (ggml_float)(x[i00] * x[i00]);\n",
        "                const float mean = sum/ne00;\n",
        "                float * y = (float *) ((char *) dst->data + i01*nb1 + i02*nb2 + i03*nb3);\n",
        "                memcpy(y, x, ne00 * sizeof(float));\n",
        "                // for (int i00 = 0; i00 < ne00; i00++) {\n",
        "                //     y[i00] = x[i00];\n",
        "                // }\n",
        "                const float scale = 1.0f/sqrtf(mean + eps);\n",
        "                ggml_vec_scale_f32(ne00, y, scale);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "FWeTlzsrcgbK"
      },
      "id": "FWeTlzsrcgbK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`apply_binary_op`](ggml/src/ggml-cpu/binary-ops.cpp#L50)\n",
        "\n",
        "```c++\n",
        "ggml_compute_forward_mul(params, tensor);\n",
        "----------\n",
        "void ggml_compute_forward_mul(const ggml_compute_params * params, ggml_tensor * dst) {\n",
        "    binary_op<op_mul>(params, dst);\n",
        "}\n",
        "static void binary_op(const ggml_compute_params * params, ggml_tensor * dst)\n",
        "    const ggml_tensor * src0 = dst->src[0];\n",
        "    const ggml_tensor * src1 = dst->src[1];\n",
        "    /*  */ if (src0->type == GGML_TYPE_F32  && src1->type == GGML_TYPE_F32  && dst->type == GGML_TYPE_F32) // all f32\n",
        "        apply_binary_op<op, float, float, float>(params, dst);\n",
        "\n",
        "template <float (*op)(float, float), typename src0_t, typename src1_t, typename dst_t>\n",
        "static void apply_binary_op(const ggml_compute_params * params, ggml_tensor * dst) {\n",
        "    const ggml_tensor * src0 = dst->src[0];\n",
        "    const ggml_tensor * src1 = dst->src[1];\n",
        "\n",
        "    GGML_ASSERT(ggml_can_repeat(src1, src0) && ggml_are_same_shape(src0, dst));\n",
        "\n",
        "    GGML_TENSOR_BINARY_OP_LOCALS\n",
        "\n",
        "    GGML_ASSERT( nb0 == sizeof(dst_t));\n",
        "    GGML_ASSERT(nb00 == sizeof(src0_t));\n",
        "\n",
        "    const auto [ir0, ir1] = get_thread_range(params, src0);\n",
        "    const bool is_src1_contiguous = (nb10 == sizeof(src1_t)); // why:-?\n",
        "\n",
        "    if (!is_src1_contiguous) { // broadcast not implemented yet for non-contiguous\n",
        "        GGML_ASSERT(ggml_are_same_shape(src0, src1));\n",
        "    }\n",
        "\n",
        "    for (int64_t ir = ir0; ir < ir1; ++ir) {\n",
        "        const int64_t i03 = ir/(ne02*ne01);\n",
        "        const int64_t i02 = (ir - i03*ne02*ne01)/ne01;\n",
        "        const int64_t i01 = (ir - i03*ne02*ne01 - i02*ne01);\n",
        "\n",
        "        const int64_t i13 = i03 % ne13;\n",
        "        const int64_t i12 = i02 % ne12;\n",
        "        const int64_t i11 = i01 % ne11;\n",
        "\n",
        "        dst_t        * dst_ptr  = (dst_t  *)       ((char *)       dst->data  + i03*nb3  + i02*nb2  + i01*nb1 );\n",
        "        const src0_t * src0_ptr = (const src0_t *) ((const char *) src0->data + i03*nb03 + i02*nb02 + i01*nb01);\n",
        "        const src1_t * src1_ptr = (const src1_t *) ((const char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11);\n",
        "\n",
        "        if (is_src1_contiguous) {\n",
        "            // src1 is broadcastable across src0 and dst in i1, i2, i3\n",
        "            const int64_t nr0 = ne00 / ne10;\n",
        "\n",
        "            for (int64_t r = 0; r < nr0; ++r) {\n",
        "                --> vec_binary_op_contiguous<op>(ne10, dst_ptr + r*ne10, src0_ptr + r*ne10, src1_ptr);\n",
        "            }\n",
        "        } else {\n",
        "            vec_binary_op_non_contiguous<op>(ne0, ne10, nb10, dst_ptr, src0_ptr, src1_ptr);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "template <float (*op)(float, float), typename src0_t, typename src1_t, typename dst_t>\n",
        "static inline void vec_binary_op_contiguous(const int64_t n, dst_t * z, const src0_t * x, const src1_t * y) {\n",
        "    constexpr auto src0_to_f32 = type_conversion_table<src0_t>::to_f32;\n",
        "    constexpr auto src1_to_f32 = type_conversion_table<src1_t>::to_f32;\n",
        "    constexpr auto f32_to_dst  = type_conversion_table<dst_t >::from_f32;\n",
        "\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        z[i] = f32_to_dst(op(src0_to_f32(x[i]), src1_to_f32(y[i])));\n",
        "    }\n",
        "}\n",
        "\n",
        "static inline float f32_to_f32(float x) {\n",
        "    return x;\n",
        "}\n",
        "\n",
        "template <float (*op)(float, float), typename src0_t, typename src1_t, typename dst_t>\n",
        "static inline void vec_binary_op_non_contiguous(const int64_t n, const int64_t ne10, const int64_t nb10, dst_t * z, const src0_t * x, const src1_t * y) {\n",
        "    constexpr auto src0_to_f32 = type_conversion_table<src0_t>::to_f32;\n",
        "    constexpr auto src1_to_f32 = type_conversion_table<src1_t>::to_f32;\n",
        "    constexpr auto f32_to_dst  = type_conversion_table<dst_t >::from_f32;\n",
        "\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        int i10 = i % ne10;\n",
        "        const src1_t * y_ptr = (const src1_t *)((const char *)y + i10*nb10);\n",
        "        z[i] = f32_to_dst(op(src0_to_f32(x[i]), src1_to_f32(*y_ptr)));\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "ajj30peBdIsN"
      },
      "id": "ajj30peBdIsN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_compute_forward_mul_mat`](ggml/src/ggml-cpu/ggml-cpu.c#L1269)\n",
        "\n",
        "\n",
        "```c++\n",
        "static void ggml_compute_forward_mul_mat(\n",
        "        const struct ggml_compute_params * params,\n",
        "              struct ggml_tensor * dst) {\n",
        "\n",
        "    const struct ggml_tensor * src0 = dst->src[0];\n",
        "    const struct ggml_tensor * src1 = dst->src[1];\n",
        "\n",
        "    GGML_TENSOR_BINARY_OP_LOCALS\n",
        "\n",
        "    const int ith = params->ith;\n",
        "    const int nth = params->nth;\n",
        "\n",
        "    enum ggml_type           const vec_dot_type         = type_traits_cpu[src0->type].vec_dot_type;\n",
        "    ggml_from_float_t        const from_float           = type_traits_cpu[vec_dot_type].from_float;\n",
        "    int64_t                  const vec_dot_num_rows     = type_traits_cpu[src0->type].nrows;\n",
        "\n",
        "    // nb01 >= nb00 - src0 is not transposed\n",
        "    //   compute by src0 rows\n",
        "\n",
        "    // TODO: extract to \"extra_op\"\n",
        "\n",
        "    // if (src1->type != vec_dot_type) {\n",
        "    char * wdata = params->wdata;\n",
        "    const size_t nbw0 = ggml_type_size(vec_dot_type);\n",
        "    const size_t nbw1 = ggml_row_size(vec_dot_type, ne10);\n",
        "    const size_t nbw2 = nbw1*ne11;\n",
        "    const size_t nbw3 = nbw2*ne12;\n",
        "\n",
        "    for (int64_t i11 = 0; i11 < ne11; ++i11) {\n",
        "        size_t bs = ggml_blck_size(vec_dot_type);\n",
        "        int64_t ne10_block_start = (ith * ne10/bs) / nth;\n",
        "        int64_t ne10_block_end   = ((ith + 1) * ne10/bs) / nth;\n",
        "        --> from_float(\n",
        "          (float *)((char *) src1->data + i11*nb11 + ne10_block_start*bs*nb10),\n",
        "          (void *) (wdata + i11*nbw1 + ne10_block_start*nbw0),\n",
        "          (ne10_block_end - ne10_block_start) * bs);\n",
        "    }\n",
        "\n",
        "    if (ith == 0) {\n",
        "        // Every thread starts at ith, so the first unprocessed chunk is nth.  This save a bit of coordination right at the start.\n",
        "        // \n",
        "        // atomic_fetch_add_explicit\n",
        "        atomic_store_explicit(&params->threadpool->current_chunk, nth, memory_order_relaxed);\n",
        "    }\n",
        "\n",
        "    ggml_barrier(params->threadpool);\n",
        "\n",
        "    // This is the size of the first dimension of the result, so we can iterate that way. (see the ASSERT above, these are the same numbers)\n",
        "    const int64_t nr0 = ne0;\n",
        "\n",
        "    // This is the size of the rest of the dimensions of the result\n",
        "    const int64_t nr1 = ne1 * ne2 * ne3;\n",
        "\n",
        "    // Now select a reasonable chunk size.\n",
        "    int chunk_size = 16;\n",
        "\n",
        "    // We need to step up the size if it's small\n",
        "    if (nr0 == 1 || nr1 == 1) {\n",
        "        chunk_size = 64;\n",
        "    }\n",
        "\n",
        "    // distribute the work across the inner or outer loop based on which one is larger\n",
        "    // The number of chunks in the 0/1 dim.\n",
        "    // CEIL(nr0/chunk_size)\n",
        "    int64_t nchunk0 = (nr0 + chunk_size - 1) / chunk_size;\n",
        "    int64_t nchunk1 = (nr1 + chunk_size - 1) / chunk_size;\n",
        "\n",
        "    // If the chunking is poor for the number of threads on this setup, scrap the whole plan.  Re-chunk it by thread.\n",
        "    //   Also, chunking by thread was measured to have perform better on NUMA systems.  See https://github.com/ggml-org/llama.cpp/pull/6915\n",
        "    //   In theory, chunking should be just as useful on NUMA and non NUMA systems, but testing disagreed with that.\n",
        "    if (nchunk0 * nchunk1 < nth * 4 || ggml_is_numa()) {\n",
        "        // distribute the thread work across the inner or outer loop based on which one is larger\n",
        "        nchunk0 = nr0 > nr1 ? nth : 1; // parallelize by src0 rows\n",
        "        nchunk1 = nr0 > nr1 ? 1 : nth; // parallelize by src1 rows\n",
        "    }\n",
        "\n",
        "    // The number of elements in each chunk\n",
        "    const int64_t dr0 = (nr0 + nchunk0 - 1) / nchunk0;\n",
        "    const int64_t dr1 = (nr1 + nchunk1 - 1) / nchunk1;\n",
        "\n",
        "    // The first chunk comes from our thread_id, the rest will get auto-assigned.\n",
        "    int current_chunk = ith;\n",
        "\n",
        "    while (current_chunk < nchunk0 * nchunk1) {\n",
        "        const int64_t ith0 = current_chunk % nchunk0;\n",
        "        const int64_t ith1 = current_chunk / nchunk0;\n",
        "\n",
        "        const int64_t ir0_start = dr0 * ith0;\n",
        "        const int64_t ir0_end = MIN(ir0_start + dr0, nr0);\n",
        "\n",
        "        const int64_t ir1_start = dr1 * ith1;\n",
        "        const int64_t ir1_end = MIN(ir1_start + dr1, nr1);\n",
        "\n",
        "        // dot kernels can handle 1 row and col at a time, but mmla kernels can process 2 rows and cols\n",
        "        int64_t num_rows_per_vec_dot = vec_dot_num_rows;\n",
        "\n",
        "        // these checks are needed to avoid crossing dim1 boundaries\n",
        "        // can be optimized, but the logic would become more complicated, so keeping it like this for simplicity\n",
        "        if ((nr0 % 2 != 0) || (ne11 % 2 != 0) || ((ir0_end - ir0_start) % 2 != 0) || ((ir1_end - ir1_start) % 2 != 0)) {\n",
        "            num_rows_per_vec_dot = 1;\n",
        "        }\n",
        "        --> ggml_compute_forward_mul_mat_one_chunk(params, dst, src0->type, num_rows_per_vec_dot, ir0_start, ir0_end, ir1_start, ir1_end);\n",
        "\n",
        "        if (nth >= nchunk0 * nchunk1) {\n",
        "            break;\n",
        "        }\n",
        "\n",
        "        current_chunk = atomic_fetch_add_explicit(&params->threadpool->current_chunk, 1, memory_order_relaxed);\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "nmZj1wEu6KVQ"
      },
      "id": "nmZj1wEu6KVQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```c++\n",
        "static void ggml_compute_forward_mul_mat_one_chunk(\n",
        "    const struct ggml_compute_params * params,\n",
        "    struct ggml_tensor * dst,\n",
        "    const enum ggml_type type,\n",
        "    const int64_t num_rows_per_vec_dot,\n",
        "    const int64_t ir0_start,\n",
        "    const int64_t ir0_end,\n",
        "    const int64_t ir1_start,\n",
        "    const int64_t ir1_end) {\n",
        "\n",
        "    const struct ggml_tensor * src0 = dst->src[0];\n",
        "    const struct ggml_tensor * src1 = dst->src[1];\n",
        "\n",
        "    GGML_TENSOR_BINARY_OP_LOCALS\n",
        "\n",
        "    const bool src1_cont = ggml_is_contiguous(src1);\n",
        "\n",
        "    ggml_vec_dot_t const vec_dot      = type_traits_cpu[type].vec_dot;\n",
        "    enum ggml_type const vec_dot_type = type_traits_cpu[type].vec_dot_type;\n",
        "\n",
        "    // broadcast factors case\n",
        "    // threads with no work simply yield (not sure if it helps)\n",
        "    if (ir0_start >= ir0_end || ir1_start >= ir1_end) {\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    const void * wdata = params->wdata;\n",
        "    const size_t row_size = ggml_row_size(vec_dot_type, ne10);\n",
        "\n",
        "    assert(ne12 % ne02 == 0);\n",
        "    assert(ne13 % ne03 == 0);\n",
        "\n",
        "    // block-tiling attempt\n",
        "    const int64_t blck_0 = 16;\n",
        "    const int64_t blck_1 = 16;\n",
        "\n",
        "    // const size_t src1_col_stride = src1_cont || src1->type != vec_dot_type ? row_size : nb11; src1cont\n",
        "    const size_t src1_col_stride = row_size;\n",
        "\n",
        "    // attempt to reduce false-sharing (does not seem to make a difference)\n",
        "    // 16 * 2, accounting for mmla kernels\n",
        "    // float tmp[32];\n",
        "    for (int64_t iir1 = ir1_start; iir1 < ir1_end; iir1 += blck_1)\n",
        "    for (int64_t iir0 = ir0_start; iir0 < ir0_end; iir0 += blck_0)\n",
        "    // \n",
        "    for (int64_t ir1 = iir1; ir1 < iir1 + blck_1 && ir1 < ir1_end; ++ir1)\n",
        "    for (int64_t ir0 = iir0; ir0 < iir0 + blck_0 && ir0 < ir0_end; ++ir0)\n",
        "        --> vec_dot(\n",
        "              n=ne00,\n",
        "              s=(float*)((char*)dst->data + ir0 + ir1 * nb1), //dst_col\n",
        "              bs=0, // UNUSED\n",
        "              vx=(const char*)src0->data + ir0 * nb01, //src0_row\n",
        "              bx=0, // UNUSED\n",
        "              vy=(const char*)wdata + ir1 * row_size, // src1_col\n",
        "              by=0, // UNUSED\n",
        "              nrc=1 // UNUSED\n",
        "          );\n",
        "    }\n",
        "```"
      ],
      "metadata": {
        "id": "ty17RxOAFe5N"
      },
      "id": "ty17RxOAFe5N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_vec_dot_q4_0_q8_0`](ggml/src/ggml-cpu/arch/x86/quants.c#L531)\n",
        "\n",
        "|                 | `_mm256_maddubs_epi16`               | `_mm256_madd_epi16`               |\n",
        "|---------------------|--------------------------------------|-----------------------------------|\n",
        "| ****    |  8    8             |  16    16        |\n",
        "| ****    | 16 16          | 32 8        |\n",
        "| ****        | 8   16                          | 16   32                      |\n",
        "| ****        |  8                     |  16                 |\n",
        "| ****        |              |           |\n",
        "\n",
        "<details>\n",
        "<summary>original implementation</summary>\n",
        "\n",
        "```c++\n",
        "float sumf = 0;\n",
        "for (ib = 0; ib < nb; ++ib) {\n",
        "    int sumi0 = 0;\n",
        "    int sumi1 = 0;\n",
        "\n",
        "    for (int j = 0; j < qk/2; ++j) {\n",
        "        const int v0 = (x[ib].qs[j] & 0x0F) - 8;\n",
        "        const int v1 = (x[ib].qs[j] >>   4) - 8;\n",
        "\n",
        "        sumi0 += (v0 * y[ib].qs[j]);\n",
        "        sumi1 += (v1 * y[ib].qs[j + qk/2]);\n",
        "    }\n",
        "\n",
        "    int sumi = sumi0 + sumi1;\n",
        "    sumf += sumi*GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d);\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "```c++\n",
        "void ggml_vec_dot_q4_0_q8_0(int n, float * GGML_RESTRICT s, size_t, const void * GGML_RESTRICT vx, size_t, const void * GGML_RESTRICT vy, size_t, int) {\n",
        "    const int qk = QK8_0;\n",
        "    const int nb = n / qk;\n",
        "    const block_q4_0 * GGML_RESTRICT x = vx;\n",
        "    const block_q8_0 * GGML_RESTRICT y = vy;\n",
        "    int ib = 0;\n",
        "    float sumf = 0;\n",
        "    // Initialize accumulator with zeros\n",
        "    __m256 acc = _mm256_setzero_ps();\n",
        "\n",
        "    // Main loop\n",
        "    for (; ib < nb; ++ib) {\n",
        "        /* Compute combined scale for the block */\n",
        "        const __m256 d = _mm256_set1_ps( GGML_FP16_TO_FP32(x[ib].d) * GGML_FP16_TO_FP32(y[ib].d) );\n",
        "\n",
        "        --> __m256i qx = bytes_from_nibbles_32(x[ib].qs);\n",
        "\n",
        "        // Now we have a vector with bytes in [ 0 .. 15 ] interval. Offset them into [ -8 .. +7 ] interval.\n",
        "        const __m256i off = _mm256_set1_epi8( 8 );\n",
        "        qx = _mm256_sub_epi8( qx, off );\n",
        "\n",
        "        __m256i qy = _mm256_loadu_si256((const __m256i *)y[ib].qs);\n",
        "\n",
        "        --> const __m256 q = mul_sum_i8_pairs_float(qx, qy);\n",
        "\n",
        "        /* Multiply q with scale and accumulate */\n",
        "        acc = _mm256_fmadd_ps( d, q, acc );\n",
        "    }\n",
        "\n",
        "    --> sumf = hsum_float_8(acc);\n",
        "    *s = sumf;\n",
        "}\n",
        "\n",
        "// Unpack 32 4-bit fields into 32 bytes\n",
        "// The output vector contains 32 bytes, each one in [ 0 .. 15 ] interval\n",
        "static inline __m256i bytes_from_nibbles_32(const uint8_t * rsi)\n",
        "{\n",
        "    const __m128i tmp = _mm_loadu_si128((const __m128i *)rsi);\n",
        "    const __m256i bytes = MM256_SET_M128I(_mm_srli_epi16(tmp, 4), tmp);\n",
        "    const __m256i lowMask = _mm256_set1_epi8( 0xF );\n",
        "    return _mm256_and_si256(lowMask, bytes);\n",
        "}\n",
        "\n",
        "// 4->8->16->32\n",
        "\n",
        "// multiply int8_t, add results pairwise twice and return as float vector\n",
        "static inline __m256 mul_sum_i8_pairs_float(const __m256i x, const __m256i y) {\n",
        "#if __AVXVNNIINT8__\n",
        "    const __m256i zero = _mm256_setzero_si256();\n",
        "    const __m256i summed_pairs = _mm256_dpbssd_epi32(zero, x, y);\n",
        "    return _mm256_cvtepi32_ps(summed_pairs);\n",
        "#else\n",
        "    // Get absolute values of x vectors\n",
        "    const __m256i ax = _mm256_sign_epi8(x, x);\n",
        "    // Sign the values of the y vectors\n",
        "    const __m256i sy = _mm256_sign_epi8(y, x);\n",
        "    return mul_sum_us8_pairs_float(ax, sy);\n",
        "#endif\n",
        "}\n",
        "\n",
        "static inline __m256 mul_sum_us8_pairs_float(const __m256i ax, const __m256i sy) {\n",
        "#if defined(__AVXVNNI__)\n",
        "    const __m256i zero = _mm256_setzero_si256();\n",
        "    // const __m256i summed_pairs = _mm256_dpbusd_epi32(zero, ax, sy);\n",
        "    const __m256i summed_pairs = _mm256_dpbusd_avx_epi32(zero, ax, sy);\n",
        "    return _mm256_cvtepi32_ps(summed_pairs);\n",
        "#else\n",
        "    // Perform multiplication and create 16-bit values\n",
        "    const __m256i dot = _mm256_maddubs_epi16(ax, sy);\n",
        "    const __m256i ones = _mm256_set1_epi16(1);\n",
        "    const __m256i summed_pairs = _mm256_madd_epi16(ones, dot);\n",
        "    return _mm256_cvtepi32_ps(summed_pairs);\n",
        "#endif\n",
        "}\n",
        "\n",
        "// horizontally add 8 floats\n",
        "static inline float hsum_float_8(const __m256 x) {\n",
        "    __m128 res = _mm256_extractf128_ps(x, 1);\n",
        "    res = _mm_add_ps(res, _mm256_castps256_ps128(x));\n",
        "    res = _mm_add_ps(res, _mm_movehl_ps(res, res));\n",
        "    res = _mm_add_ss(res, _mm_movehdup_ps(res));\n",
        "    return _mm_cvtss_f32(res);\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "kT_ll-E_0rWg"
      },
      "id": "kT_ll-E_0rWg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`llamafile_sgemm`](ggml/src/ggml-cpu/llamafile/sgemm.cpp#L3275)\n",
        "\n",
        "2\n",
        "\n",
        "```c++\n",
        "llamafile_sgemm(params,\n",
        "                m=ne01, n=ne11, k=ne00/ggml_blck_size(src0->type),\n",
        "                A=(const char *)src0->data,\n",
        "                lda=nb01/ggml_type_size(src0->type),\n",
        "                B=(const char *)src1->data,\n",
        "                ldb=nb11/ggml_type_size(src1->type),\n",
        "                C=(char *)dst->data,\n",
        "                ldc=nb1/ggml_type_size(dst->type),\n",
        "                Atype=src0->type,\n",
        "                Btype=src1->type,\n",
        "                Ctype=dst->type);\n",
        "\n",
        "const void* wdata = params->wdata;\n",
        "const size_t row_size = ggml_row_size(vec_dot_type, ne10);\n",
        "llamafile_sgemm(params,\n",
        "                ne01, ne11, ne00/ggml_blck_size(src0->type),\n",
        "                (const char *)src0->data,\n",
        "                nb01/ggml_type_size(src0->type),\n",
        "                (const char *)wdata,\n",
        "                row_size/ggml_type_size(vec_dot_type),\n",
        "                (char *)dst->data,\n",
        "                nb1/ggml_type_size(dst->type),\n",
        "                src0->type,\n",
        "                vec_dot_type,\n",
        "                dst->type);\n",
        "----------\n",
        "/**\n",
        " * Performs optimized matrix multiplication on CPU.\n",
        " *\n",
        " * This subroutine may compute C = A * B with column major ordering.\n",
        " * Despite its name, this isn't a generalized implementation. Work is\n",
        " * only performed when a handwritten kernel is written and available.\n",
        " * Otherwise the caller should fall back to a general matmul routine.\n",
        " *\n",
        " * For example, for single-threaded single-precision GEMM you can say\n",
        " *\n",
        " *     llamafile_sgemm(m, n, k, A, lda, B, ldb, C, ldc,\n",
        " *                     0, 1,\n",
        " *                     GGML_TYPE_F32, GGML_TYPE_F32, GGML_TYPE_F32);\n",
        " *\n",
        " * @param m is rows in `A` and `C`\n",
        " * @param n is cols in `B` and `C`\n",
        " * @param k is cols in `A` and rows in `B`\n",
        " * @param A is first input matrix (always transposed)\n",
        " * @param lda is row stride of `A`\n",
        " * @param B is second input matrix (never transposed)\n",
        " * @param ldb is row stride of `B`\n",
        " * @param C is input/output array of output matrices\n",
        " * @param ldc is row stride of `C`\n",
        " * @param ith is thread id (must be less than `nth`)\n",
        " * @param nth is number of threads (must be greater than zero)\n",
        " * @param Atype is GGML data type of `A`\n",
        " * @param Btype is GGML data type of `B`\n",
        " * @param Ctype is GGML data type of `C`\n",
        " * @return true if this function was able to service the matmul request\n",
        " */\n",
        "bool llamafile_sgemm(const struct ggml_compute_params * params, int64_t m, int64_t n, int64_t k,\n",
        "                     const void *A, int64_t lda, const void *B, int64_t ldb, void *C,\n",
        "                     int64_t ldc, int Atype, int Btype, int Ctype) {}\n",
        "```"
      ],
      "metadata": {
        "id": "kJxjlKTh8oyg"
      },
      "id": "kJxjlKTh8oyg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```c++\n",
        "void quantize_row_q8_0(const float * GGML_RESTRICT x, void * GGML_RESTRICT vy, int64_t k) {\n",
        "    assert(QK8_0 == 32);\n",
        "    assert(k % QK8_0 == 0);\n",
        "    const int nb = k / QK8_0;\n",
        "    block_q8_0 * GGML_RESTRICT y = vy;\n",
        "\n",
        "    for (int i = 0; i < nb; i++) {\n",
        "        // Load elements into 4 AVX vectors\n",
        "        __m256 v0 = _mm256_loadu_ps( x );\n",
        "        __m256 v1 = _mm256_loadu_ps( x + 8 );\n",
        "        __m256 v2 = _mm256_loadu_ps( x + 16 );\n",
        "        __m256 v3 = _mm256_loadu_ps( x + 24 );\n",
        "        x += 32;\n",
        "        // Compute max(abs(e)) for the block\n",
        "        const __m256 signBit = _mm256_set1_ps( -0.0f );\n",
        "        __m256 maxAbs = _mm256_andnot_ps( signBit, v0 );\n",
        "        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v1 ) );\n",
        "        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v2 ) );\n",
        "        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v3 ) );\n",
        "        __m128 max4 = _mm_max_ps( _mm256_extractf128_ps( maxAbs, 1 ), _mm256_castps256_ps128( maxAbs ) );\n",
        "        max4 = _mm_max_ps( max4, _mm_movehl_ps( max4, max4 ) );\n",
        "        max4 = _mm_max_ss( max4, _mm_movehdup_ps( max4 ) );\n",
        "        const float maxScalar = _mm_cvtss_f32( max4 );\n",
        "        // Quantize these floats\n",
        "        const float d = maxScalar / 127.f;\n",
        "        y[i].d = GGML_FP32_TO_FP16(d);\n",
        "        const float id = ( maxScalar != 0.0f ) ? 127.f / maxScalar : 0.0f;\n",
        "        const __m256 mul = _mm256_set1_ps( id );\n",
        "        // Apply the multiplier\n",
        "        v0 = _mm256_mul_ps( v0, mul );\n",
        "        v1 = _mm256_mul_ps( v1, mul );\n",
        "        v2 = _mm256_mul_ps( v2, mul );\n",
        "        v3 = _mm256_mul_ps( v3, mul );\n",
        "        // Round to nearest integer\n",
        "        v0 = _mm256_round_ps( v0, _MM_ROUND_NEAREST );\n",
        "        v1 = _mm256_round_ps( v1, _MM_ROUND_NEAREST );\n",
        "        v2 = _mm256_round_ps( v2, _MM_ROUND_NEAREST );\n",
        "        v3 = _mm256_round_ps( v3, _MM_ROUND_NEAREST );\n",
        "        // Convert floats to integers\n",
        "        __m256i i0 = _mm256_cvtps_epi32( v0 );\n",
        "        __m256i i1 = _mm256_cvtps_epi32( v1 );\n",
        "        __m256i i2 = _mm256_cvtps_epi32( v2 );\n",
        "        __m256i i3 = _mm256_cvtps_epi32( v3 );\n",
        "        // Convert int32 to int16\n",
        "        i0 = _mm256_packs_epi32( i0, i1 );\t// 0, 1, 2, 3,  8, 9, 10, 11,  4, 5, 6, 7, 12, 13, 14, 15\n",
        "        i2 = _mm256_packs_epi32( i2, i3 );\t// 16, 17, 18, 19,  24, 25, 26, 27,  20, 21, 22, 23, 28, 29, 30, 31\n",
        "                                            // Convert int16 to int8\n",
        "        i0 = _mm256_packs_epi16( i0, i2 );\t// 0, 1, 2, 3,  8, 9, 10, 11,  16, 17, 18, 19,  24, 25, 26, 27,  4, 5, 6, 7, 12, 13, 14, 15, 20, 21, 22, 23, 28, 29, 30, 31\n",
        "        // We got our precious signed bytes, but the order is now wrong\n",
        "        // These AVX2 pack instructions process 16-byte pieces independently\n",
        "        // The following instruction is fixing the order\n",
        "        const __m256i perm = _mm256_setr_epi32( 0, 4, 1, 5, 2, 6, 3, 7 );\n",
        "        i0 = _mm256_permutevar8x32_epi32( i0, perm );\n",
        "        _mm256_storeu_si256((__m256i *)y[i].qs, i0);\n",
        "    }\n",
        "}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "5ogCMyjuGytL"
      },
      "id": "5ogCMyjuGytL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [`ggml_backend_tensor_get_async`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L245)\n",
        "\n",
        "```c++\n",
        "ggml_backend_tensor_get_async(backend_res, t_logits, logits_out, 0, n_outputs*n_vocab*sizeof(float));\n",
        "----------\n",
        "void ggml_backend_tensor_get_async(ggml_backend_t backend, const struct ggml_tensor * tensor, void * data, size_t offset, size_t size) {}\n",
        "```"
      ],
      "metadata": {
        "id": "ICX2FPjOQNu5"
      },
      "id": "ICX2FPjOQNu5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "if (backend->iface.get_tensor_async == NULL) {\n",
        "    --> ggml_backend_tensor_get(tensor, data, offset, size);\n",
        "} else {\n",
        "    backend->iface.get_tensor_async(backend, tensor, data, offset, size);\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "XMHmbrkPVKWZ"
      },
      "id": "XMHmbrkPVKWZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`ggml_backend_cpu_buffer_get_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1896)\n",
        "\n",
        "```c++\n",
        "void ggml_backend_tensor_get(const struct ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n",
        "    ggml_backend_buffer_t buf = tensor->view_src ? tensor->view_src->buffer : tensor->buffer;\n",
        "    --> buf->iface.get_tensor(buf, tensor, data, offset, size);\n",
        "}\n",
        "----------\n",
        "static void ggml_backend_cpu_buffer_get_tensor(ggml_backend_buffer_t buffer, const struct ggml_tensor * tensor, void * data, size_t offset, size_t size)\n",
        "    memcpy(data, (const char *)tensor->data + offset, size);\n",
        "```"
      ],
      "metadata": {
        "id": "2jnCvqzyVrEb"
      },
      "id": "2jnCvqzyVrEb"
    },
    {
      "cell_type": "markdown",
      "id": "956178f8-f679-474a-a2e8-dc00bf58cdac",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "956178f8-f679-474a-a2e8-dc00bf58cdac"
      },
      "source": [
        "# [`perplexity`](https://github.com/ggml-org/llama.cpp/blob/master/tools/perplexity/perplexity.cpp#L441)\n",
        "\n",
        "```c++\n",
        "results = perplexity(ctx, params, n_ctx);\n",
        "----------\n",
        "static results_perplexity perplexity(llama_context * ctx, const common_params & params, const int32_t n_ctx) {}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "// Download: https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip\n",
        "// Run `./llama-perplexity -m models/7B/ggml-model-q4_0.bin -f wiki.test.raw`\n",
        "// Output: `perplexity: 13.5106 [114/114]`\n",
        "// BOS tokens will be added for each chunk before eval\n",
        "\n",
        "const llama_model * model = llama_get_model(ctx);\n",
        "const llama_vocab * vocab = llama_model_get_vocab(model);\n",
        "const bool add_bos = llama_vocab_get_add_bos(vocab);\n",
        "--> common_tokenize --> llama_tokenize --> vocab->tokenize --> llama_vocab::tokenize --> llama_vocab::impl::tokenize --> std::vector<llama_token> tokens;\n",
        "\n",
        "std::vector<float> logit_history;\n",
        "logit_history.resize(tokens.size());\n",
        "\n",
        "std::vector<float> prob_history;\n",
        "prob_history.resize(tokens.size());\n",
        "\n",
        "const int n_chunk_max = tokens.size() / n_ctx;\n",
        "const int n_chunk = params.n_chunks < 0 ? n_chunk_max : std::min(params.n_chunks, n_chunk_max);\n",
        "const int n_batch = params.n_batch;\n",
        "const int n_vocab = llama_vocab_n_tokens(vocab);\n",
        "int count = 0;\n",
        "double nll = 0.0;\n",
        "double nll2 = 0.0;\n",
        "const int num_batches = (n_ctx + n_batch - 1) / n_batch;\n",
        "const int n_seq = std::max(1, n_batch / n_ctx);\n",
        "--> llama_batch_init --> llama_batch batch;\n",
        "std::vector<std::thread> workers(std::thread::hardware_concurrency() - 1);\n",
        "\n",
        "// We get the logits for all the tokens in the context window (params.n_ctx)\n",
        "// from llama_eval above.  Now, based on https://huggingface.co/docs/transformers/perplexity,\n",
        "// calculate the perplexity over the last half of the window (so the model always has\n",
        "// some context to predict the token).\n",
        "//\n",
        "// We rely on the fact that attention in the forward pass only looks at previous\n",
        "// tokens here, so the logits returned for each token are an accurate representation\n",
        "// of what the model would have predicted at that point.\n",
        "//\n",
        "// Example, we have a context window of 512, we will compute perplexity for each of the\n",
        "// last 256 tokens.  Then, we split the input up into context window size chunks to\n",
        "// process the entire prompt.\n",
        "const int first = n_ctx/2;\n",
        "for (int i = 0; i < n_chunk; i += n_seq)\n",
        "    const int start =     i * n_ctx;\n",
        "    const int end   = start + n_ctx;\n",
        "    const int n_seq_batch = std::min(n_seq, n_chunk - i);\n",
        "    // clear the KV cache\n",
        "    llama_memory_clear(llama_get_memory(ctx), true);\n",
        "    // for (int j = 0; j < num_batches; ++j)\n",
        "    const int batch_start = start + j * n_batch;\n",
        "    const int batch_size  = std::min(end - batch_start, n_batch);\n",
        "    int n_outputs = 0;\n",
        "    batch.n_tokens = 0;\n",
        "    for (int seq = 0; seq < n_seq_batch; seq++)\n",
        "        int seq_start = batch_start + seq*n_ctx;\n",
        "        // save original token and restore it after eval\n",
        "        const auto token_org = tokens[seq_start];\n",
        "        if (add_bos && j == 0) tokens[seq_start] = llama_vocab_bos(vocab);\n",
        "        for (int k = 0; k < batch_size; ++k) {\n",
        "            const int idx = seq*n_ctx + k;\n",
        "            batch.token   [idx]    = tokens[seq_start + k];\n",
        "            batch.pos     [idx]    = j*n_batch + k;\n",
        "            batch.n_seq_id[idx]    = 1;\n",
        "            batch.seq_id  [idx][0] = seq;\n",
        "            batch.logits  [idx]    = batch.pos[idx] >= first ? 1 : 0;\n",
        "\n",
        "            n_outputs += batch.logits[idx] != 0;\n",
        "        }\n",
        "        batch.n_tokens += batch_size;\n",
        "        // restore the original token in case it was set to BOS\n",
        "        tokens[seq_start] = token_org;\n",
        "\n",
        "    --> llama_decode(ctx, batch);\n",
        "    if (i == 0) llama_synchronize(ctx);\n",
        "\n",
        "    for (int seq = 0; seq < n_seq_batch; seq++)\n",
        "        const float * all_logits = llama_get_logits_ith(ctx, seq*n_ctx + first);\n",
        "            ctx->synchronize();\n",
        "            return ctx:: logits + output_ids[seq*n_ctx + first]*model.vocab.n_tokens();\n",
        "        llama_token * tokens_data = tokens.data() + start + seq*n_ctx + first;\n",
        "        --> process_logits(n_vocab, all_logits,\n",
        "                tokens_data, n_ctx - 1 - first,\n",
        "                workers, nll, nll2,\n",
        "                logit_history.data() + start + seq*n_ctx + first,\n",
        "                prob_history.data()  + start + seq*n_ctx + first);\n",
        "        count += n_ctx - first - 1;\n",
        "\n",
        "nll2 /= count;\n",
        "nll /= count;\n",
        "const double ppl = exp(nll);\n",
        "nll2 -= nll * nll;\n",
        "nll2 = sqrt(nll2/(count-1));\n",
        "\n",
        "--> llama_batch_free;\n",
        "return {tokens, ppl, logit_history, prob_history};\n",
        "```"
      ],
      "metadata": {
        "id": "HlvSDPRTaPLh"
      },
      "id": "HlvSDPRTaPLh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [`llama_vocab::impl::tokenize`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-vocab.cpp#L2389)\n",
        "\n",
        "```c++\n",
        "std::vector<llama_token> tokens = common_tokenize(ctx, params.prompt, true);\n",
        "----------\n",
        "```"
      ],
      "metadata": {
        "id": "N5cLIa6uaeTV"
      },
      "id": "N5cLIa6uaeTV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "// tokenizes a string into a vector of tokens\n",
        "// should work similar to Python's `tokenizer.encode`\n",
        "std::vector<llama_token> common_tokenize(\n",
        "  const struct llama_context * ctx,\n",
        "           const std::string & text,\n",
        "                        bool   add_special,\n",
        "                        bool   parse_special = false) {\n",
        "    const llama_model * model = llama_get_model(ctx);\n",
        "    const llama_vocab * vocab = llama_model_get_vocab(model);\n",
        "    --> return common_tokenize(vocab, text, add_special, parse_special);\n",
        "}\n",
        "\n",
        "std::vector<llama_token> common_tokenize(\n",
        "    const struct llama_vocab * vocab,\n",
        "           const std::string & text,\n",
        "                        bool   add_special,\n",
        "                        bool   parse_special) {\n",
        "    // upper limit for the number of tokens\n",
        "    int n_tokens = text.length() + 2 * add_special;\n",
        "    std::vector<llama_token> result(n_tokens);\n",
        "    --> n_tokens = llama_tokenize(vocab, text.data(), text.length(), result.data(), result.size(), add_special, parse_special);\n",
        "    result.resize(n_tokens);\n",
        "    return result;\n",
        "}\n",
        "\n",
        "int32_t llama_tokenize(\n",
        "    const struct llama_vocab * vocab,\n",
        "                  const char * text,\n",
        "                     int32_t   text_len,\n",
        "                 llama_token * tokens,\n",
        "                     int32_t   n_tokens_max,\n",
        "                        bool   add_special,\n",
        "                        bool   parse_special) {\n",
        "    --> return vocab->tokenize(text, text_len, tokens, n_tokens_max, add_special, parse_special);\n",
        "}\n",
        "\n",
        "int32_t llama_vocab::tokenize(\n",
        "                  const char * text,\n",
        "                     int32_t   text_len,\n",
        "                 llama_token * tokens,\n",
        "                     int32_t   n_tokens_max,\n",
        "                        bool   add_special,\n",
        "                        bool   parse_special) const {\n",
        "    --> auto res = tokenize(std::string(text, text_len), add_special, parse_special);\n",
        "    for (size_t i = 0; i < res.size(); i++) {\n",
        "        tokens[i] = res[i];\n",
        "    }\n",
        "    return res.size();\n",
        "}\n",
        "\n",
        "std::vector<llama_token> llama_vocab::tokenize(\n",
        "        const std::string & raw_text,\n",
        "        bool add_special,\n",
        "        bool parse_special) const {\n",
        "    --> return pimpl->tokenize(raw_text, add_special, parse_special);\n",
        "}\n",
        "\n",
        "std::vector<llama_token> llama_vocab::impl::tokenize(\n",
        "        const std::string & raw_text,\n",
        "        bool add_special,\n",
        "        bool parse_special) const {}\n",
        "```"
      ],
      "metadata": {
        "id": "qXuZOOwqjNTy"
      },
      "id": "qXuZOOwqjNTy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "std::vector<llama_token> output;\n",
        "std::forward_list<fragment_buffer_variant> fragment_buffer;\n",
        "\n",
        "fragment_buffer.emplace_front(raw_text, 0, raw_text.length());\n",
        "tokenizer_st_partition(fragment_buffer, parse_special);\n",
        "\n",
        "llm_tokenizer_bpe_session session(vocab, *static_cast<const llm_tokenizer_bpe *>(tokenizer.get()));\n",
        "// it calls some other methods that are not exist in llm_tokenizer,\n",
        "// here just cast it to bpe tokenizer object\n",
        "if (add_special) {\n",
        "    session.append_bos(output);\n",
        "}\n",
        "for (const auto & fragment : fragment_buffer) {\n",
        "    std::string text = fragment.raw_text.substr(fragment.offset, fragment.length);\n",
        "    --> llm_tokenizer_bpe_session::tokenize;\n",
        "}\n",
        "\n",
        "if (add_special) {\n",
        "    session.append_eos(output);\n",
        "    session.check_double_bos_eos(output);\n",
        "}\n",
        "\n",
        "return output;\n",
        "```"
      ],
      "metadata": {
        "id": "obgPbFpXeXQd"
      },
      "id": "obgPbFpXeXQd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`llm_tokenizer_bpe_session::tokenize`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-vocab.cpp#L482)\n",
        "\n",
        "<details>\n",
        "<summary>struct llm_tokenizer_bpe_session</summary>\n",
        "\n",
        "```c++\n",
        "struct llm_tokenizer_bpe_session {\n",
        "    llm_tokenizer_bpe_session(const llama_vocab & vocab, const llm_tokenizer_bpe & tokenizer) : vocab(vocab), tokenizer(tokenizer) {}\n",
        "\n",
        "    static void append(const llama_token token_id, std::vector<llama_token> & output)  {\n",
        "    bool append_bos(std::vector<llama_token> & output) const\n",
        "    bool append_eos(std::vector<llama_token> & output) const\n",
        "    void check_double_bos_eos(const std::vector<llama_token> & output) const\n",
        "    void tokenize(const std::string & text, std::vector<llama_token> & output)\n",
        "\n",
        "private:\n",
        "    void add_new_bigram(int left, int right)\n",
        "    const llama_vocab & vocab;\n",
        "    const llm_tokenizer_bpe & tokenizer;\n",
        "\n",
        "    std::vector<llm_symbol> symbols;\n",
        "    std::vector<llm_symbol> symbols_final;\n",
        "    llm_bigram_bpe::queue work_queue;\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "```c++\n",
        "session.tokenize(text, output);\n",
        "----------\n",
        "void llm_tokenizer_bpe_session::tokenize(const std::string & text, std::vector<llama_token> & output) {}\n",
        "```"
      ],
      "metadata": {
        "id": "BMJIwOtyiNV0"
      },
      "id": "BMJIwOtyiNV0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [`llama_batch_init`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L339)\n",
        "\n",
        "```c++\n",
        "llama_batch batch = llama_batch_init(std::min(n_batch, n_ctx*n_seq), 0, 1);\n",
        "----------\n",
        "struct llama_batch llama_batch_init(int32_t n_tokens_alloc, int32_t embd, int32_t n_seq_max) {}\n",
        "```"
      ],
      "metadata": {
        "id": "_-FyHhnun2uG"
      },
      "id": "_-FyHhnun2uG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "llama_batch batch = {\n",
        "    /*n_tokens       =*/ 0,\n",
        "    /*tokens         =*/ nullptr,\n",
        "    /*embd           =*/ nullptr,\n",
        "    /*pos            =*/ nullptr,\n",
        "    /*n_seq_id       =*/ nullptr,\n",
        "    /*seq_id         =*/ nullptr,\n",
        "    /*logits         =*/ nullptr,\n",
        "};\n",
        "\n",
        "if (embd) {\n",
        "    batch.embd = (float *) malloc(sizeof(float) * n_tokens_alloc * embd);\n",
        "} else {\n",
        "    batch.token = (llama_token *) malloc(sizeof(llama_token) * n_tokens_alloc);\n",
        "}\n",
        "\n",
        "batch.pos      = (llama_pos *)     malloc(sizeof(llama_pos)      * n_tokens_alloc);\n",
        "batch.n_seq_id = (int32_t *)       malloc(sizeof(int32_t)        * n_tokens_alloc);\n",
        "batch.seq_id   = (llama_seq_id **) malloc(sizeof(llama_seq_id *) * (n_tokens_alloc + 1));\n",
        "for (int i = 0; i < n_tokens_alloc; ++i) {\n",
        "    batch.seq_id[i] = (llama_seq_id *) malloc(sizeof(llama_seq_id) * n_seq_max);\n",
        "}\n",
        "batch.seq_id[n_tokens_alloc] = nullptr;\n",
        "\n",
        "batch.logits   = (int8_t *)        malloc(sizeof(int8_t)         * n_tokens_alloc);\n",
        "\n",
        "return batch;\n",
        "```"
      ],
      "metadata": {
        "id": "9_wfg-xcoEHj"
      },
      "id": "9_wfg-xcoEHj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [`process_logits`](https://github.com/ggml-org/llama.cpp/blob/master/tools/perplexity/perplexity.cpp#L107)\n",
        "\n",
        "```c++\n",
        "process_logits(n_vocab, all_logits,\n",
        "        tokens_data, n_ctx - 1 - first,\n",
        "        workers, nll, nll2,\n",
        "        logit_history.data() + start + seq*n_ctx + first,\n",
        "        prob_history.data()  + start + seq*n_ctx + first);\n",
        "----------\n",
        "static void process_logits(\n",
        "    int n_vocab, const float * logits, const int * tokens, int n_token, std::vector<std::thread> & workers,\n",
        "    double & nll, double & nll2, float * logit_history, float * prob_history) {}\n",
        "```"
      ],
      "metadata": {
        "id": "KlIl9uGu0oc3"
      },
      "id": "KlIl9uGu0oc3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "std::mutex mutex;\n",
        "int counter = 0;\n",
        "auto compute = [&mutex, &counter, &nll, &nll2, logit_history, prob_history, n_vocab, logits, tokens, n_token] () {\n",
        "    double local_nll  = 0;\n",
        "    double local_nll2 = 0;\n",
        "    while (true) {\n",
        "        std::unique_lock<std::mutex> lock(mutex);\n",
        "        int i = counter++;\n",
        "        if (i >= n_token) {\n",
        "            nll += local_nll; nll2 += local_nll2;\n",
        "            break;\n",
        "        }\n",
        "        lock.unlock();\n",
        "        --> log_softmax --> const results_log_softmax results;\n",
        "        const double v = -results.log_softmax;\n",
        "        local_nll += v;\n",
        "        local_nll2 += v*v;\n",
        "\n",
        "        logit_history[i] = results.logit;\n",
        "        prob_history[i]  = results.prob;\n",
        "    }\n",
        "};\n",
        "for (auto & w : workers) {\n",
        "    w = std::thread(compute);\n",
        "}\n",
        "compute();\n",
        "for (auto & w : workers) {\n",
        "    w.join();\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "sQIGDu411eES"
      },
      "id": "sQIGDu411eES"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`log_softmax`](https://github.com/ggml-org/llama.cpp/blob/master/tools/perplexity/perplexity.cpp#L58)\n",
        "\n",
        "<details>\n",
        "<summary>struct results_log_softmax</summary>\n",
        "\n",
        "```c++\n",
        "struct results_log_softmax {\n",
        "    double log_softmax;\n",
        "    float  logit;\n",
        "    float  prob;\n",
        "};\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "```c++\n",
        "const results_log_softmax results = log_softmax(n_vocab, logits + size_t(i)*n_vocab, tokens[i+1]);\n",
        "----------\n",
        "static results_log_softmax log_softmax(int n_vocab, const float * logits, int tok) {\n",
        "    float max_logit = logits[0];\n",
        "    for (int i = 1; i < n_vocab; ++i) {\n",
        "        max_logit = std::max(max_logit, logits[i]);\n",
        "    }\n",
        "    double sum_exp = 0.0;\n",
        "    for (int i = 0; i < n_vocab; ++i) {\n",
        "        sum_exp += expf(logits[i] - max_logit);\n",
        "    }\n",
        "    return {logits[tok] - max_logit - log(sum_exp), logits[tok], expf(logits[tok] - max_logit) / (float) sum_exp};\n",
        "}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "QkDpQiXv13Nq"
      },
      "id": "QkDpQiXv13Nq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [`llama_batch_free`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-batch.cpp#L369)\n",
        "\n",
        "```c++\n",
        "llama_batch_free(batch);\n",
        "----------\n",
        "void llama_batch_free(struct llama_batch batch) {}\n",
        "```"
      ],
      "metadata": {
        "id": "nxv91gZNzyqS"
      },
      "id": "nxv91gZNzyqS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c++\n",
        "if (batch.token)    free(batch.token);\n",
        "if (batch.embd)     free(batch.embd);\n",
        "if (batch.pos)      free(batch.pos);\n",
        "if (batch.n_seq_id) free(batch.n_seq_id);\n",
        "if (batch.seq_id) {\n",
        "    for (int i = 0; batch.seq_id[i] != nullptr; ++i) {\n",
        "        free(batch.seq_id[i]);\n",
        "    }\n",
        "    free(batch.seq_id);\n",
        "}\n",
        "if (batch.logits)   free(batch.logits);\n",
        "```"
      ],
      "metadata": {
        "id": "It9cQT67z5SF"
      },
      "id": "It9cQT67z5SF"
    },
    {
      "cell_type": "markdown",
      "id": "598a1c09-8369-40ef-943f-36d6e8a90f15",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "598a1c09-8369-40ef-943f-36d6e8a90f15"
      },
      "source": [
        "# [`ggml_quantize_free`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L6457)\n",
        "\n",
        "```c++\n",
        "llama_backend_free();\n",
        "----------\n",
        "void llama_backend_free(void) {\n",
        "    --> ggml_quantize_free();\n",
        "}\n",
        "\n",
        "void ggml_quantize_free(void) {\n",
        "    ggml_critical_section_start();\n",
        "\n",
        "    iq2xs_free_impl(GGML_TYPE_IQ2_XXS);\n",
        "    iq2xs_free_impl(GGML_TYPE_IQ2_XS);\n",
        "    iq2xs_free_impl(GGML_TYPE_IQ1_S);\n",
        "    iq3xs_free_impl(256);\n",
        "\n",
        "    ggml_critical_section_end();\n",
        "}\n",
        "\n",
        "void iq2xs_free_impl(enum ggml_type type)\n",
        "    const int gindex = iq2_data_index(type);\n",
        "    if (iq2_data[gindex].grid) {\n",
        "        free(iq2_data[gindex].grid);       iq2_data[gindex].grid = NULL;\n",
        "        free(iq2_data[gindex].map);        iq2_data[gindex].map  = NULL;\n",
        "        free(iq2_data[gindex].neighbours); iq2_data[gindex].neighbours = NULL;\n",
        "    }\n",
        "\n",
        "void iq3xs_free_impl(int grid_size)\n",
        "    const int gindex = iq3_data_index(grid_size);\n",
        "    if (iq3_data[gindex].grid) {\n",
        "        free(iq3_data[gindex].grid);       iq3_data[gindex].grid = NULL;\n",
        "        free(iq3_data[gindex].map);        iq3_data[gindex].map  = NULL;\n",
        "        free(iq3_data[gindex].neighbours); iq3_data[gindex].neighbours = NULL;\n",
        "    }\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "23b35917-d3da-45b2-996e-cf9b9e26cfb9",
        "9ad88911-3e43-461f-bcb1-8599dccb1fe6",
        "7d4cdb26-bd29-4915-96b6-9573765ce679"
      ],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}