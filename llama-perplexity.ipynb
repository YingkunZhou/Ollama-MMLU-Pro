{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10009b7d-823c-46ae-8355-4e580c686bf3",
   "metadata": {},
   "source": [
    "```c++\n",
    "common_params params;\n",
    "--> common_params_parse;\n",
    "common_init();\n",
    "llama_backend_init();\n",
    "llama_numa_init(params.numa);\n",
    "// load the model and apply lora adapter, if any\n",
    "--> common_init_from_params;\n",
    "llama_model * model = llama_init.model.get();\n",
    "llama_context * ctx = llama_init.context.get();\n",
    "struct results_perplexity results;\n",
    "--> perplexity;\n",
    "llama_perf_context_print(ctx);\n",
    "--> llama_backend_free;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019f8f4-e92b-49a1-97c0-7da9b098ca40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# [`common_params_parse`](https://github.com/ggml-org/llama.cpp/blob/master/common/arg.cpp#L1183)\n",
    "\n",
    "```c++\n",
    "common_params_parse(argc, argv, params, LLAMA_EXAMPLE_PERPLEXITY);\n",
    "\n",
    "bool common_params_parse(int argc, char ** argv, common_params & params, llama_example ex, void(*print_usage)(int, char **)) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48b2ed-fc26-4326-b2a8-aaecfa12d61a",
   "metadata": {},
   "source": [
    "# [`common_init_from_params`](https://github.com/ggml-org/llama.cpp/blob/master/common/common.cpp#L888)\n",
    "\n",
    "```c++\n",
    "common_init_result llama_init = common_init_from_params(params);\n",
    "\n",
    "struct common_init_result common_init_from_params(common_params & params) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cbab3d-c6dc-4363-b124-d687c8808a0a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct common_init_result</summary>\n",
    "\n",
    "```c++\n",
    "// note: defines object's lifetime\n",
    "struct common_init_result {\n",
    "    llama_model_ptr   model;\n",
    "    llama_context_ptr context;\n",
    "\n",
    "    std::vector<llama_adapter_lora_ptr> lora;\n",
    "};\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "common_init_result iparams;\n",
    "\n",
    "auto mparams = common_model_params_to_llama(params);\n",
    "--> llama_model_load_from_file --> llama_model_load_from_file_impl --> llama_model_load --> llama_model * model;\n",
    "auto cparams = common_context_params_to_llama(params);\n",
    "--> llama_init_from_model --> llama_context::llama_context --> llama_context * lctx;\n",
    "llama_set_warmup(lctx, warmup=true); --> cparams.warmup = warmup;\n",
    "std::vector<llama_token> tmp;\n",
    "llama_token bos = llama_vocab_bos(vocab);\n",
    "llama_token eos = llama_vocab_eos(vocab);\n",
    "tmp.push_back(bos);\n",
    "tmp.push_back(eos);\n",
    "--> llama_decode;\n",
    "llama_memory_clear(llama_get_memory(lctx), true);\n",
    "llama_synchronize(lctx);\n",
    "llama_perf_context_reset(lctx);\n",
    "llama_set_warmup(lctx, false);\n",
    "\n",
    "iparams.model.reset(model);\n",
    "iparams.context.reset(lctx);\n",
    "return iparams;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1fd37-e078-4d43-bbe4-ef923887305b",
   "metadata": {},
   "source": [
    "## [`llama_model_load`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama.cpp#L87)\n",
    "\n",
    "```c++\n",
    "llama_model * model = llama_model_load_from_file(params.model.path.c_str(), mparams);\n",
    "\n",
    "struct llama_model * llama_model_load_from_file(\n",
    "    const char * path_model,\n",
    "    struct llama_model_params params) {\n",
    "    std::vector<std::string> splits = {};\n",
    "    --> return llama_model_load_from_file_impl(path_model, splits, params);\n",
    "}\n",
    "\n",
    "static struct llama_model * llama_model_load_from_file_impl(\n",
    "        const std::string & path_model,\n",
    "        std::vector<std::string> & splits,\n",
    "        struct llama_model_params params) {\n",
    "    llama_model * model = new llama_model(params);\n",
    "    --> const int status = llama_model_load(path_model, splits, *model, params);\n",
    "    return model;\n",
    "}\n",
    "\n",
    "// Returns 0 on success, -1 on error, and -2 on cancellation via llama_progress_callback\n",
    "static int llama_model_load(const std::string & fname, std::vector<std::string> & splits, llama_model & model, llama_model_params & params) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d2a10-42f8-4e7d-b13b-70e2ada15840",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct llama_model</summary>\n",
    "\n",
    "```c++\n",
    "struct llama_model {\n",
    "    llm_type type = LLM_TYPE_UNKNOWN;\n",
    "    llm_arch arch = LLM_ARCH_UNKNOWN;\n",
    "\n",
    "    std::string name = \"n/a\";\n",
    "\n",
    "    llama_hparams hparams = {};\n",
    "    llama_vocab   vocab;\n",
    "\n",
    "    struct ggml_tensor * tok_embd   = nullptr;\n",
    "    struct ggml_tensor * type_embd  = nullptr;\n",
    "    struct ggml_tensor * pos_embd   = nullptr;\n",
    "    struct ggml_tensor * tok_norm   = nullptr;\n",
    "    struct ggml_tensor * tok_norm_b = nullptr;\n",
    "\n",
    "    struct ggml_tensor * output_norm     = nullptr;\n",
    "    struct ggml_tensor * output_norm_b   = nullptr;\n",
    "    struct ggml_tensor * output          = nullptr;\n",
    "    struct ggml_tensor * output_b        = nullptr;\n",
    "    struct ggml_tensor * output_norm_enc = nullptr;\n",
    "\n",
    "    std::vector<llama_layer> layers;\n",
    "\n",
    "    llama_model_params params;\n",
    "\n",
    "    // gguf metadata\n",
    "    std::unordered_map<std::string, std::string> gguf_kv;\n",
    "\n",
    "    // list of devices used in this model\n",
    "    std::vector<ggml_backend_dev_t> devices;\n",
    "\n",
    "    // for quantize-stats only\n",
    "    std::vector<std::pair<std::string, struct ggml_tensor *>> tensors_by_name;\n",
    "\n",
    "    // note: can mutate `cparams`\n",
    "    // TODO: move this to new llm_arch_model_i interface\n",
    "    llama_memory_i * create_memory(const llama_memory_params & params, llama_cparams & cparams) const;\n",
    "\n",
    "    // TODO: move this to new llm_arch_model_i interface\n",
    "    llm_graph_result_ptr build_graph(\n",
    "            const llm_graph_params & params,\n",
    "                       ggml_cgraph * gf,\n",
    "                    llm_graph_type   type) const;\n",
    "\n",
    "private:\n",
    "    struct impl;\n",
    "    std::unique_ptr<impl> pimpl;\n",
    "};\n",
    "\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "```c++\n",
    "--> llama_model_loader::llama_model_loader --> llama_model_loader ml;\n",
    "ml.print_info();\n",
    "model.load_arch(ml);\n",
    "--> llama_model::load_hparams;\n",
    "--> llama_model::load_vocab --> llama_vocab::load --> llama_vocab::impl::load;\n",
    "model.load_stats(ml);\n",
    "model.print_info();\n",
    "--> llama_model::load_tensors;\n",
    "return 0;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b35917-d3da-45b2-996e-cf9b9e26cfb9",
   "metadata": {},
   "source": [
    "### [`llama_model_loader::llama_model_loader`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#468)\n",
    "\n",
    "```c++\n",
    "llama_model_loader ml(fname, splits, params.use_mmap, params.check_tensors, params.kv_overrides, params.tensor_buft_overrides);\n",
    "\n",
    "llama_model_loader::llama_model_loader(\n",
    "        const std::string & fname,\n",
    "        std::vector<std::string> & splits,\n",
    "        bool use_mmap,\n",
    "        bool check_tensors,\n",
    "        const llama_model_kv_override * param_overrides_p,\n",
    "        const llama_model_tensor_buft_override * param_tensor_buft_overrides_p) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf2a36-2b0c-4ce8-a29a-11121839b801",
   "metadata": {},
   "source": [
    "```c++\n",
    "// Load the main GGUF\n",
    "struct ggml_context * ctx = NULL;\n",
    "struct gguf_init_params params = {\n",
    "    /*.no_alloc = */ true,\n",
    "    /*.ctx      = */ &ctx,\n",
    "};\n",
    "\n",
    "llama_model_loader:: gguf_context_ptr meta;\n",
    "--> gguf_init_from_file --> gguf_init_from_file_impl --> meta;\n",
    "\n",
    "files.emplace_back(new llama_file(fname.c_str(), \"rb\"));\n",
    "contexts.emplace_back(ctx);\n",
    "\n",
    "for (ggml_tensor * cur = ggml_get_first_tensor(ctx); cur; cur = ggml_get_next_tensor(ctx, cur)) {\n",
    "    std::string tensor_name = std::string(cur->name);\n",
    "    n_elements += ggml_nelements(cur);\n",
    "    n_bytes    += ggml_nbytes(cur);\n",
    "    weights_map.emplace(tensor_name, llama_tensor_weight(files.back().get(), 0, meta.get(), cur));\n",
    "}\n",
    "\n",
    "n_kv      = gguf_get_n_kv(meta.get());\n",
    "n_tensors = weights_map.size();\n",
    "\n",
    "fver = (enum llama_fver) gguf_get_version(meta.get());\n",
    "\n",
    "this->use_mmap = use_mmap;\n",
    "this->check_tensors = check_tensors;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad88911-3e43-461f-bcb1-8599dccb1fe6",
   "metadata": {},
   "source": [
    "#### [`gguf_init_from_file_impl`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/gguf.cpp#319)\n",
    "\n",
    "```c++\n",
    "meta.reset(gguf_init_from_file(fname.c_str(), params));\n",
    "\n",
    "struct gguf_context * gguf_init_from_file(const char * fname, struct gguf_init_params params) {\n",
    "    FILE * file = ggml_fopen(fname, \"rb\");\n",
    "    --> struct gguf_context * result = gguf_init_from_file_impl(file, params);\n",
    "    fclose(file);\n",
    "    return result;\n",
    "}\n",
    "\n",
    "struct gguf_context * gguf_init_from_file_impl(FILE * file, struct gguf_init_params params) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed93c7-14ac-4787-82b3-d6f8c7ff1fc2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct gguf_context</summary>\n",
    "\n",
    "```c++\n",
    "struct gguf_context {\n",
    "    uint32_t version = GGUF_VERSION;\n",
    "\n",
    "    std::vector<struct gguf_kv> kv;\n",
    "    std::vector<struct gguf_tensor_info> info;\n",
    "\n",
    "    size_t alignment = GGUF_DEFAULT_ALIGNMENT;\n",
    "    size_t offset    = 0; // offset of `data` from beginning of file\n",
    "    size_t size      = 0; // size of `data` in bytes\n",
    "\n",
    "    void * data = nullptr;\n",
    "};\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "```c++\n",
    "const struct gguf_reader gr(file);\n",
    "struct gguf_context * ctx = new gguf_context;\n",
    "// file magic\n",
    "gr.read(magic, 4);\n",
    "// header\n",
    "gr.read(ctx->version);\n",
    "gr.read(n_tensors);\n",
    "gr.read(n_kv);\n",
    "// KV pairs\n",
    "for (int64_t i = 0; ok && i < n_kv; ++i)\n",
    "    gr.read(key);\n",
    "    gr.read(type);\n",
    "    is_array = true; gr.read(type); gr.read(n);\n",
    "    gguf_read_emplace_helper<xxx>    (gr, ctx->kv, key, is_array, n);\n",
    "const int alignment_idx = gguf_find_key(ctx, GGUF_KEY_GENERAL_ALIGNMENT);\n",
    "ctx->alignment = alignment_idx == -1 ? GGUF_DEFAULT_ALIGNMENT : gguf_get_val_u32(ctx, alignment_idx);\n",
    "\n",
    "// read the tensor info\n",
    "for (int64_t i = 0; ok && i < n_tensors; ++i)\n",
    "    struct gguf_tensor_info info;\n",
    "    std::string name; gr.read(name); ggml_set_name(&info.t, name.c_str()); // tensor name\n",
    "    uint32_t n_dims = -1; gr.read(n_dims); gr.read(info.t.ne); // tensor shape\n",
    "    gr.read(info.t.type); // tensor type\n",
    "    // calculate byte offsets given the tensor shape and type\n",
    "    const size_t  type_size = ggml_type_size(info.t.type);\n",
    "    const int64_t blck_size = ggml_blck_size(info.t.type);\n",
    "    info.t.nb;\n",
    "    gr.read(info.offset); // tensor data offset within buffer\n",
    "    ctx->info.push_back(info);\n",
    "\n",
    "// store the current file offset - this is where the data section starts\n",
    "ctx->offset = ftell(file);\n",
    "\n",
    "// compute the total size of the data section, taking into account the alignment\n",
    "ctx->size = 0;\n",
    "for (size_t i = 0; i < ctx->info.size(); ++i)\n",
    "    const gguf_tensor_info & ti = ctx->info[i];\n",
    "    ctx->size += GGML_PAD(ggml_nbytes(&ti.t), ctx->alignment);\n",
    "\n",
    "\n",
    "// load the tensor data only if requested\n",
    "// compute the exact size needed for the new ggml_context\n",
    "const size_t mem_size = n_tensors * ggml_tensor_overhead();\n",
    "\n",
    "struct ggml_init_params pdata = {\n",
    "    /*mem_size   =*/ mem_size,\n",
    "    /*mem_buffer =*/ nullptr,\n",
    "    /*no_alloc   =*/ params.no_alloc,\n",
    "};\n",
    "--> ggml_init --> *params.ctx;\n",
    "struct ggml_context * ctx_data = *params.ctx;\n",
    "\n",
    "// create the tensors\n",
    "for (size_t i = 0; i < ctx->info.size(); ++i) {\n",
    "    const struct gguf_tensor_info & info = ctx->info[i];\n",
    "    --> ggml_new_tensor --> ggml_new_tensor_impl --> struct ggml_tensor * cur;\n",
    "    ggml_set_name(cur, info.t.name);\n",
    "}\n",
    "return ctx;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186d1c0b-b609-4be9-8132-b8a912a71e7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### [`ggml_init`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1420)\n",
    "\n",
    "```c++\n",
    "*params.ctx = ggml_init(pdata);\n",
    "\n",
    "struct ggml_context * ggml_init(struct ggml_init_params params) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddffe5c-6aea-4666-b676-87a6dd835057",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_context</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_context {\n",
    "    size_t mem_size;\n",
    "    void * mem_buffer;\n",
    "    bool   mem_buffer_owned;\n",
    "    bool   no_alloc;\n",
    "\n",
    "    int    n_objects;\n",
    "\n",
    "    struct ggml_object * objects_begin;\n",
    "    struct ggml_object * objects_end;\n",
    "};\n",
    "\n",
    "size_t ggml_tensor_overhead(void) {\n",
    "    return GGML_OBJECT_SIZE + GGML_TENSOR_SIZE;\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "struct ggml_context * ctx = GGML_MALLOC(sizeof(struct ggml_context));\n",
    "\n",
    "// allow to call ggml_init with 0 size\n",
    "if (params.mem_size == 0) {\n",
    "    params.mem_size = GGML_MEM_ALIGN;\n",
    "}\n",
    "\n",
    "const size_t mem_size = params.mem_buffer ? params.mem_size : GGML_PAD(params.mem_size, GGML_MEM_ALIGN);\n",
    "\n",
    "*ctx = (struct ggml_context) {\n",
    "    /*.mem_size           =*/ mem_size,\n",
    "    /*.mem_buffer         =*/ params.mem_buffer ? params.mem_buffer : ggml_aligned_malloc(mem_size),\n",
    "    /*.mem_buffer_owned   =*/ params.mem_buffer ? false : true,\n",
    "    /*.no_alloc           =*/ params.no_alloc,\n",
    "    /*.n_objects          =*/ 0,\n",
    "    /*.objects_begin      =*/ NULL,\n",
    "    /*.objects_end        =*/ NULL,\n",
    "};\n",
    "\n",
    "return ctx;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6458a4-f1d0-4a4f-8ef3-e8ba3d78e344",
   "metadata": {},
   "source": [
    "##### [`ggml_new_tensor_impl`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1647)\n",
    "\n",
    "```c++\n",
    "struct ggml_tensor * cur = ggml_new_tensor(ctx_data, info.t.type, GGML_MAX_DIMS, info.t.ne);\n",
    "\n",
    "struct ggml_tensor * ggml_new_tensor(\n",
    "        struct ggml_context * ctx,\n",
    "        enum   ggml_type      type,\n",
    "        int                   n_dims,\n",
    "        const int64_t       * ne) {\n",
    "    --> return ggml_new_tensor_impl(ctx, type, n_dims, ne, NULL, 0);\n",
    "}\n",
    "\n",
    "static struct ggml_tensor * ggml_new_tensor_impl(\n",
    "        struct ggml_context * ctx,\n",
    "        enum   ggml_type      type,\n",
    "        int                   n_dims,\n",
    "        const int64_t       * ne,\n",
    "        struct ggml_tensor  * view_src,\n",
    "        size_t                view_offs) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d12c4-9416-4598-8809-05b5ee817c3b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_tensor</summary>\n",
    "\n",
    "```c++\n",
    "// n-dimensional tensor\n",
    "struct ggml_tensor {\n",
    "    enum ggml_type type;\n",
    "\n",
    "    struct ggml_backend_buffer * buffer;\n",
    "\n",
    "    int64_t ne[GGML_MAX_DIMS]; // number of elements\n",
    "    size_t  nb[GGML_MAX_DIMS]; // stride in bytes:\n",
    "                               // nb[0] = ggml_type_size(type)\n",
    "                               // nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding\n",
    "                               // nb[i] = nb[i-1] * ne[i-1]\n",
    "\n",
    "    // compute data\n",
    "    enum ggml_op op;\n",
    "\n",
    "    // op params - allocated as int32_t for alignment\n",
    "    int32_t op_params[GGML_MAX_OP_PARAMS / sizeof(int32_t)];\n",
    "\n",
    "    int32_t flags;\n",
    "\n",
    "    struct ggml_tensor * src[GGML_MAX_SRC];\n",
    "\n",
    "    // source tensor and offset for views\n",
    "    struct ggml_tensor * view_src;\n",
    "    size_t               view_offs;\n",
    "\n",
    "    void * data;\n",
    "\n",
    "    char name[GGML_MAX_NAME];\n",
    "\n",
    "    void * extra; // extra things e.g. for ggml-cuda.cu\n",
    "\n",
    "    char padding[8];\n",
    "};\n",
    "\n",
    "static const size_t GGML_TENSOR_SIZE = sizeof(struct ggml_tensor);\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "--> ggml_new_object --> obj_new;\n",
    "struct ggml_tensor * const result = (struct ggml_tensor *)((char *)ctx->mem_buffer + obj_new->offs);\n",
    "*result = (struct ggml_tensor) {\n",
    "    /*.type         =*/ type,\n",
    "    /*.buffer       =*/ NULL,\n",
    "    /*.ne           =*/ { 1, 1, 1, 1 },\n",
    "    /*.nb           =*/ { 0, 0, 0, 0 },\n",
    "    /*.op           =*/ GGML_OP_NONE,\n",
    "    /*.op_params    =*/ { 0 },\n",
    "    /*.flags        =*/ 0,\n",
    "    /*.src          =*/ { NULL },\n",
    "    /*.view_src     =*/ view_src,\n",
    "    /*.view_offs    =*/ view_offs,\n",
    "    /*.data         =*/ view_src != NULL ? view_src->data + view_offs : NULL,\n",
    "    /*.name         =*/ { 0 },\n",
    "    /*.extra        =*/ NULL,\n",
    "    /*.padding      =*/ { 0 },\n",
    "};\n",
    "\n",
    "result->ne;\n",
    "result->nb;\n",
    "ctx->n_objects++;\n",
    "return result;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f77f5-8623-49cc-8aa9-d6cc788469af",
   "metadata": {},
   "source": [
    "[`ggml_new_object`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1525)\n",
    "\n",
    "```c++\n",
    "struct ggml_object * const obj_new = ggml_new_object(ctx, GGML_OBJECT_TYPE_TENSOR, GGML_TENSOR_SIZE);\n",
    "\n",
    "static struct ggml_object * ggml_new_object(struct ggml_context * ctx, enum ggml_object_type type, size_t size) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a747dcb-8a63-4695-8d73-68a3009ab9d1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_object</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_object {\n",
    "    size_t offs;\n",
    "    size_t size;\n",
    "\n",
    "    struct ggml_object * next;\n",
    "\n",
    "    enum ggml_object_type type;\n",
    "\n",
    "    char padding[4];\n",
    "};\n",
    "\n",
    "static const size_t GGML_OBJECT_SIZE = sizeof(struct ggml_object);\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "```c++\n",
    "// always insert objects at the end of the context's memory pool\n",
    "struct ggml_object * obj_cur = ctx->objects_end;\n",
    "\n",
    "const size_t cur_offs = obj_cur == NULL ? 0 : obj_cur->offs;\n",
    "const size_t cur_size = obj_cur == NULL ? 0 : obj_cur->size;\n",
    "const size_t cur_end  = cur_offs + cur_size;\n",
    "\n",
    "// align to GGML_MEM_ALIGN\n",
    "size_t size_needed = GGML_PAD(size, GGML_MEM_ALIGN);\n",
    "\n",
    "char * const mem_buffer = ctx->mem_buffer;\n",
    "struct ggml_object * const obj_new = (struct ggml_object *)(mem_buffer + cur_end);\n",
    "\n",
    "*obj_new = (struct ggml_object) {\n",
    "    .offs = cur_end + GGML_OBJECT_SIZE,\n",
    "    .size = size_needed,\n",
    "    .next = NULL,\n",
    "    .type = type,\n",
    "};\n",
    "\n",
    "GGML_ASSERT_ALIGNED(mem_buffer + obj_new->offs);\n",
    "\n",
    "if (obj_cur != NULL) {\n",
    "    obj_cur->next = obj_new;\n",
    "} else {\n",
    "    // this is the first object in this context\n",
    "    ctx->objects_begin = obj_new;\n",
    "}\n",
    "\n",
    "ctx->objects_end = obj_new;\n",
    "return obj_new;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4cdb26-bd29-4915-96b6-9573765ce679",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [`llama_model::load_hparams`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#423)\n",
    "\n",
    "```c++\n",
    "model.load_hparams(ml);\n",
    "\n",
    "void llama_model::load_hparams(llama_model_loader & ml) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0018b980-85ef-4e0f-a684-e88a10f1ae01",
   "metadata": {},
   "source": [
    "```c++\n",
    "const gguf_context * ctx = ml.meta.get();\n",
    "// get metadata as string\n",
    "// gguf metadata\n",
    "llama_model:: std::unordered_map<std::string, std::string> gguf_kv;\n",
    "gguf_kv.emplace(name, value);\n",
    "hparams.xxx = xxx; // via ml.get_key\n",
    "pimpl->n_bytes = ml.n_bytes;\n",
    "pimpl->desc_str = arch_name() + \" \" + type_name() + \" \" + ml.ftype_name();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1729941-5b75-45f6-9598-a0800fb9e442",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [`llama_vocab::impl::load`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-vocab.cpp#1372)\n",
    "\n",
    "```c++\n",
    "model.load_vocab(ml);\n",
    "\n",
    "void llama_model::load_vocab(llama_model_loader & ml) {\n",
    "    const auto kv = LLM_KV(arch);\n",
    "    --> vocab.load(ml, kv);\n",
    "}\n",
    "\n",
    "void llama_vocab::load(llama_model_loader & ml, const LLM_KV & kv) {\n",
    "    --> pimpl->load(ml, kv);\n",
    "}\n",
    "\n",
    "void llama_vocab::impl::load(llama_model_loader & ml, const LLM_KV & kv) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f22f50-c727-4d33-8bac-95a2825460a2",
   "metadata": {},
   "source": [
    "```c++\n",
    "struct gguf_context * ctx = ml.meta.get();\n",
    "// determine vocab type\n",
    "ml.get_key(LLM_KV_TOKENIZER_MODEL, tokenizer_model);\n",
    "ml.get_key(LLM_KV_TOKENIZER_PRE,   tokenizer_pre, false);\n",
    "ml.get_key(LLM_KV_TOKENIZER_TOKEN_TYPE_COUNT, n_token_types, false);\n",
    "// for now, only BPE models have pre-tokenizers\n",
    "llama_vocab::impl::\n",
    "    std::unordered_map<std::string, llama_token> token_to_id;\n",
    "    std::vector<token_data> id_to_token;\n",
    "--> init_tokenizer(type); --> tokenizer = std::make_unique<llm_tokenizer_bpe>(vocab);\n",
    "// determine the newline token: LLaMA \"<0x0A>\" == 10 == '\\n', Falcon 193 == '\\n\n",
    "// special tokens\n",
    "    std::set<llama_token> special_eog_ids; // set of all tokens that cause \"end of generation\"\n",
    "// build special tokens cache\n",
    "    std::vector<llama_token> cache_special_tokens;\n",
    "// build token to piece cache\n",
    "    std::vector<std::string> cache_token_to_piece; // llama_token_to_piece(special = true);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c62852-51d9-4b64-9244-73cb6e2081d8",
   "metadata": {},
   "source": [
    "### [`llama_model::load_tensors`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#1467)\n",
    "\n",
    "```c++\n",
    "model.load_tensors(ml);\n",
    "\n",
    "bool llama_model::load_tensors(llama_model_loader & ml) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3073523b-3048-4b55-bfb4-01af5ddc4195",
   "metadata": {},
   "source": [
    "```c++\n",
    "// build a list of buffer types for the CPU and GPU devices\n",
    "pimpl->cpu_buft_list = make_cpu_buft_list(devices);\n",
    "// calculate the split points\n",
    "std::vector<float> splits(n_devices());\n",
    "std::copy(tensor_split, tensor_split + n_devices(), splits.begin());\n",
    "// sum and normalize the splits to get the split points\n",
    "ggml_backend_dev_t cpu_dev = ggml_backend_dev_by_type(GGML_BACKEND_DEVICE_TYPE_CPU);\n",
    "auto get_layer_buft_list = [&](int il) -> llama_model::impl::layer_dev {}\n",
    "// assign the input layer, there is very little benefit to offloading the input layer, so always keep it on the CPU\n",
    "pimpl->dev_input = { cpu_dev, &pimpl->cpu_buft_list };\n",
    "// assign the repeating layers to the devices according to the splits\n",
    "pimpl->dev_layer.resize(n_layer);\n",
    "for (int il = 0; il < n_layer; ++il) pimpl->dev_layer[il] = get_layer_buft_list(il);\n",
    "// assign the output layer\n",
    "pimpl->dev_output = get_layer_buft_list(n_layer);\n",
    "// one ggml context per buffer type\n",
    "int max_n_tensors = ml.n_tensors;\n",
    "max_n_tensors += 1;         // duplicated output tensor\n",
    "max_n_tensors += n_layer*2; // duplicated rope freq tensors\n",
    "const size_t ctx_size = ggml_tensor_overhead()*max_n_tensors;\n",
    "std::map<ggml_backend_buffer_type_t, ggml_context *> ctx_map;\n",
    "auto ctx_for_buft = [&](ggml_backend_buffer_type_t buft) -> ggml_context * {\n",
    "    auto it = ctx_map.find(buft);\n",
    "    if (it == ctx_map.end()) {\n",
    "        ggml_init_params params = {\n",
    "            /*.mem_size   =*/ ctx_size,\n",
    "            /*.mem_buffer =*/ NULL,\n",
    "            /*.no_alloc   =*/ true,\n",
    "        };\n",
    "        ggml_context * ctx = ggml_init(params);\n",
    "        ctx_map[buft] = ctx;\n",
    "        pimpl->ctxs.emplace_back(ctx);\n",
    "        return ctx;\n",
    "    }\n",
    "    return it->second;\n",
    "};\n",
    "// create tensors for the weights\n",
    "auto create_tensor = [&](const LLM_TN_IMPL & tn, const std::initializer_list<int64_t> & ne, int flags) -> ggml_tensor * {\n",
    "    ggml_tensor * t_meta = ml.get_tensor_meta(tn.str().c_str());\n",
    "    llm_tensor_info info = llm_tensor_info_for(tn_tensor);\n",
    "    // select the buffer type for this tensor\n",
    "    switch (info.layer) {}\n",
    "    --> select_weight_buft --> weight_buft_supported --> buft;\n",
    "    ggml_context * ctx = ctx_for_buft(buft);\n",
    "    --> return llama_model_loader::create_tensor;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd8350-9ed4-4b32-9a89-c73cd160f294",
   "metadata": {},
   "source": [
    "```c++\n",
    "layers.resize(n_layer);\n",
    "// TODO: move to a separate function\n",
    "const auto tn = LLM_TN(arch);\n",
    "\n",
    "tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, 0);\n",
    "\n",
    "// output\n",
    "output_norm = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd}, 0);\n",
    "output      = create_tensor(tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, TENSOR_NOT_REQUIRED);\n",
    "\n",
    "// if output is NULL, init from the input tok embed\n",
    "if (output == NULL)\n",
    "    output = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, TENSOR_DUPLICATED);\n",
    "\n",
    "for (int i = 0; i < n_layer; ++i)\n",
    "    auto & layer = layers[i];\n",
    "\n",
    "    layer.attn_norm = create_tensor(tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), {n_embd}, 0);\n",
    "\n",
    "    layer.wq = create_tensor(tn(LLM_TENSOR_ATTN_Q,   \"weight\", i), {n_embd, n_embd_head_k * n_head}, 0);\n",
    "    layer.wk = create_tensor(tn(LLM_TENSOR_ATTN_K,   \"weight\", i), {n_embd, n_embd_k_gqa}, 0);\n",
    "    layer.wv = create_tensor(tn(LLM_TENSOR_ATTN_V,   \"weight\", i), {n_embd, n_embd_v_gqa}, 0);\n",
    "    layer.wo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd_head_k * n_head, n_embd}, 0);\n",
    "\n",
    "    // optional bias tensors\n",
    "    layer.bq = create_tensor(tn(LLM_TENSOR_ATTN_Q,   \"bias\", i), {n_embd},     TENSOR_NOT_REQUIRED);\n",
    "    layer.bk = create_tensor(tn(LLM_TENSOR_ATTN_K,   \"bias\", i), {n_embd_gqa}, TENSOR_NOT_REQUIRED);\n",
    "    layer.bv = create_tensor(tn(LLM_TENSOR_ATTN_V,   \"bias\", i), {n_embd_gqa}, TENSOR_NOT_REQUIRED);\n",
    "    layer.bo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"bias\", i), {n_embd},     TENSOR_NOT_REQUIRED);\n",
    "\n",
    "    layer.ffn_norm = create_tensor(tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd}, 0);\n",
    "\n",
    "    layer.ffn_gate = create_tensor(tn(LLM_TENSOR_FFN_GATE, \"weight\", i), {n_embd,   n_ff}, 0);\n",
    "    layer.ffn_down = create_tensor(tn(LLM_TENSOR_FFN_DOWN, \"weight\", i), {  n_ff, n_embd}, 0);\n",
    "    layer.ffn_up   = create_tensor(tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff}, 0);\n",
    "\n",
    "    // optional MLP bias\n",
    "    layer.ffn_gate_b = create_tensor(tn(LLM_TENSOR_FFN_GATE, \"bias\", i), {n_ff}, TENSOR_NOT_REQUIRED);\n",
    "    layer.ffn_down_b = create_tensor(tn(LLM_TENSOR_FFN_DOWN, \"bias\", i), {n_embd}, TENSOR_NOT_REQUIRED);\n",
    "    layer.ffn_up_b   = create_tensor(tn(LLM_TENSOR_FFN_UP,   \"bias\", i), {n_ff}, TENSOR_NOT_REQUIRED);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038a45e-ce5b-4c44-b79b-eb423abf15b6",
   "metadata": {},
   "source": [
    "```c++\n",
    "ml.done_getting_tensors();\n",
    "--> llama_model_loader::init_mappings --> ml.mappings;\n",
    "pimpl->mappings.reserve(ml.mappings.size());\n",
    "for (auto & mapping : ml.mappings)\n",
    "    pimpl->mappings.emplace_back(std::move(mapping));\n",
    "\n",
    "// create the backend buffers\n",
    "using llama_buf_map = std::unordered_map<uint32_t, ggml_backend_buffer_t>;\n",
    "std::vector<std::pair<ggml_context *, llama_buf_map>> ctx_bufs;\n",
    "ctx_bufs.reserve(ctx_map.size());\n",
    "\n",
    "// Ensure we have enough capacity for the maximum backend buffer we will potentially create\n",
    "pimpl->bufs.reserve(ctx_map.size());\n",
    "\n",
    "for (auto & it : ctx_map)\n",
    "    ggml_context * ctx = it.second;\n",
    "    llama_buf_map buf_map;\n",
    "    buf_map.reserve(1); // ml.files.size()\n",
    "    //if (ml.use_mmap && use_mmap_buffer && buffer_from_host_ptr_supported && is_default_buft)\n",
    "    //for (uint32_t idx = 0; idx < ml.files.size(); idx++)\n",
    "    // only the mmap region containing the tensors in the model is mapped to the backend buffer\n",
    "    // this is important for metal with apple silicon: if the entire model could be mapped to a metal buffer, then we could just use metal for all layers\n",
    "    // this allows using partial offloading when the model size exceeds the metal buffer size, but not the RAM size\n",
    "    void * addr = nullptr;\n",
    "    size_t first, last; // NOLINT\n",
    "    --> llama_model_loader::get_mapping_range;\n",
    "    --> ggml_backend_dev_buffer_from_host_ptr --> ggml_backend_cpu_buffer_from_ptr --> ggml_backend_buffer_t buf;\n",
    "    // indicate that this buffer contains weights, this is used by ggml_backend_sched to improve op scheduling: ops that use a weight are preferably scheduled to the backend that contains the weight\n",
    "ggml_backend_buffer_set_usage(buf, GGML_BACKEND_BUFFER_USAGE_WEIGHTS);\n",
    "    pimpl->bufs.emplace_back(buf);\n",
    "    buf_map.emplace(0, buf);\n",
    "\n",
    "    ctx_bufs.emplace_back(ctx, buf_map);\n",
    "\n",
    "// populate tensors_by_name\n",
    "for (auto & ctx : pimpl->ctxs) for (auto * cur = ggml_get_first_tensor(ctx.get()); cur != NULL; cur = ggml_get_next_tensor(ctx.get(), cur)) tensors_by_name.emplace_back(ggml_get_name(cur), cur);\n",
    "\n",
    "// load tensor data\n",
    "for (auto & it : ctx_bufs)\n",
    "    ggml_context * ctx = it.first;\n",
    "    auto & bufs = it.second;\n",
    "    --> llama_model_loader::load_all_data;\n",
    "\n",
    "return true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22f4be-1f51-437c-ba6e-32f1caed638b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [`weight_buft_supported`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model.cpp#L138)\n",
    "\n",
    "```c++\n",
    "ggml_backend_buffer_type_t buft = select_weight_buft(hparams, t_meta, op, *buft_list);\n",
    "\n",
    "// find the first buffer type in the list that can use the tensor\n",
    "static ggml_backend_buffer_type_t select_weight_buft(const llama_hparams & hparams, ggml_tensor * tensor, ggml_op op, const buft_list_t & buft_list) {\n",
    "    for (const auto & cur : buft_list)\n",
    "        --> if (weight_buft_supported(hparams, tensor, op, cur_buft = cur.second, cur_dev = cur.first)) {\n",
    "            return cur_buft;\n",
    "}\n",
    "\n",
    "// checks if the weight tensor can be used with the specified buffer type and device\n",
    "static bool weight_buft_supported(const llama_hparams & hparams, ggml_tensor * w, ggml_op op, ggml_backend_buffer_type_t buft, ggml_backend_dev_t dev) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbb3d1-24e1-47e1-8308-ec9de9ebe01f",
   "metadata": {},
   "source": [
    "```c++\n",
    "ggml_init_params params = {\n",
    "    /*.mem_size   =*/ ggml_tensor_overhead()*8,\n",
    "    /*.mem_buffer =*/ NULL,\n",
    "    /*.no_alloc   =*/ true,\n",
    "};\n",
    "ggml_context_ptr ctx_ptr { ggml_init(params) };\n",
    "\n",
    "ggml_context * ctx = ctx_ptr.get();\n",
    "ggml_tensor * op_tensor = nullptr;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a5303-6f78-4266-a722-206ea33d8d92",
   "metadata": {},
   "source": [
    "```c++\n",
    "switch (op) {\n",
    "    case GGML_OP_GET_ROWS:\n",
    "        {\n",
    "            ggml_tensor * b = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 512);\n",
    "            op_tensor = ggml_get_rows(ctx, w, b);\n",
    "        } break;\n",
    "    case GGML_OP_MUL_MAT:\n",
    "        {\n",
    "            ggml_tensor * b = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, w->ne[0], 512, w->ne[2], w->ne[3]);\n",
    "            op_tensor = ggml_mul_mat(ctx, w, b);\n",
    "        } break;\n",
    "    case GGML_OP_ADD:\n",
    "        {\n",
    "            ggml_tensor * a = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, w->ne[0], w->ne[1], w->ne[2], w->ne[3]);\n",
    "            op_tensor = ggml_add(ctx, a, w);\n",
    "        } break;\n",
    "    case GGML_OP_MUL:\n",
    "        {\n",
    "            ggml_tensor * a = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, w->ne[0], w->ne[1], w->ne[2], w->ne[3]);\n",
    "            op_tensor = ggml_mul(ctx, a, w);\n",
    "        } break;\n",
    "    case GGML_OP_DIV:\n",
    "        {\n",
    "            ggml_tensor * a = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, w->ne[0]);\n",
    "            op_tensor = ggml_div(ctx, a, w);\n",
    "        } break;\n",
    "    case GGML_OP_ROPE:\n",
    "        {\n",
    "            int n_embd_head = hparams.n_embd_head_v;\n",
    "            int n_head = hparams.n_head();\n",
    "            ggml_tensor * a = ggml_new_tensor_3d(ctx, GGML_TYPE_F32, n_embd_head, n_head, 512);\n",
    "            ggml_tensor * b = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 512);\n",
    "            op_tensor = ggml_rope_ext(\n",
    "                ctx, a, b, w,\n",
    "                0, 0, 0, 0, 0,\n",
    "                0, 0, 0, 0\n",
    "            );\n",
    "        } break;\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>ggml_new_tensor</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_tensor * ggml_new_tensor_1d()\n",
    "    return ggml_new_tensor(ctx, type, 1, &ne0);\n",
    "\n",
    "struct ggml_tensor * ggml_new_tensor_3d()\n",
    "    const int64_t ne[3] = { ne0, ne1, ne2 };\n",
    "    return ggml_new_tensor(ctx, type, 3, ne);\n",
    "\n",
    "struct ggml_tensor * ggml_new_tensor_4d()\n",
    "    const int64_t ne[4] = { ne0, ne1, ne2, ne3 };\n",
    "    return ggml_new_tensor(ctx, type, 4, ne);\n",
    "\n",
    "struct ggml_tensor * ggml_get_rows()\n",
    "    struct ggml_tensor * result = ggml_new_tensor_4d(ctx, type, a->ne[0], b->ne[0], b->ne[1], b->ne[2]);\n",
    "    result->op     = GGML_OP_GET_ROWS;\n",
    "    result->src[0] = a;\n",
    "    result->src[1] = b;\n",
    "    return result;\n",
    "\n",
    "static struct ggml_tensor * ggml_rope_impl()\n",
    "    int sections[4] = {0, 0, 0, 0};\n",
    "\n",
    "    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n",
    "\n",
    "    int32_t params[15] = { /*n_past*/ 0, n_dims, mode, /*n_ctx*/ 0, n_ctx_orig };\n",
    "    memcpy(params +  5, &freq_base,    sizeof(float));\n",
    "    memcpy(params +  6, &freq_scale,   sizeof(float));\n",
    "    memcpy(params +  7, &ext_factor,   sizeof(float));\n",
    "    memcpy(params +  8, &attn_factor,  sizeof(float));\n",
    "    memcpy(params +  9, &beta_fast,    sizeof(float));\n",
    "    memcpy(params + 10, &beta_slow,    sizeof(float));\n",
    "    memcpy(params + 11, &sections,     sizeof(int)*4);\n",
    "    ggml_set_op_params(result, params, sizeof(params));\n",
    "\n",
    "    result->op     = GGML_OP_ROPE;\n",
    "    result->src[0] = a;\n",
    "    result->src[1] = b;\n",
    "    result->src[2] = c;\n",
    "\n",
    "    return result;\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c9982-bda8-41f4-8741-18241262b8b0",
   "metadata": {},
   "source": [
    "```c++\n",
    "w->buffer = ggml_backend_buft_alloc_buffer(buft, 0);\n",
    "bool op_supported = ggml_backend_dev_supports_op(dev, op_tensor);\n",
    "ggml_backend_buffer_free(w->buffer);\n",
    "return op_supported;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c727af-2ca8-44e5-828f-09c3d2ec4abc",
   "metadata": {},
   "source": [
    "#### [`llama_model_loader::create_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L789)\n",
    "\n",
    "```c++\n",
    "return ml.create_tensor(ctx, tn, ne, flags);\n",
    "\n",
    "struct ggml_tensor * llama_model_loader::create_tensor(struct ggml_context * ctx, const std::string & name, const std::initializer_list<int64_t> & ne, int flags) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77bfe6-68d8-442e-85b9-7b9913d6e15f",
   "metadata": {},
   "source": [
    "```c++\n",
    "--> check_tensor_dims --> cur;\n",
    "bool duplicated = flags & TENSOR_DUPLICATED;\n",
    "--> ggml_dup_tensor --> tensor;\n",
    "ggml_set_name(tensor, ggml_get_name(cur));\n",
    "if (duplicated) {\n",
    "    size_data += ggml_nbytes(cur);\n",
    "} else {\n",
    "    n_created++;\n",
    "}\n",
    "return tensor;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c992f-40c0-4e93-85fa-ab915b5ee0dc",
   "metadata": {},
   "source": [
    "[`llama_model_loader::check_tensor_dims`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L759)\n",
    "\n",
    "```c++\n",
    "const struct ggml_tensor * cur = check_tensor_dims(name, ne, !(flags & TENSOR_NOT_REQUIRED));\n",
    "\n",
    "const struct ggml_tensor * llama_model_loader::check_tensor_dims(const std::string & name, const std::vector<int64_t> & ne, bool required) const {\n",
    "    const struct ggml_tensor * cur = get_tensor_meta(name.c_str());\n",
    "    return cur;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b327e9-86e6-4308-9eb6-aa39a0812b6b",
   "metadata": {},
   "source": [
    "[`ggml_dup_tensor`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml.c#L1698)\n",
    "\n",
    "```c++\n",
    "struct ggml_tensor * tensor = ggml_dup_tensor(ctx, cur);\n",
    "\n",
    "struct ggml_tensor * ggml_dup_tensor(struct ggml_context * ctx, const struct ggml_tensor * src) {\n",
    "    return ggml_new_tensor(ctx, src->type, GGML_MAX_DIMS, src->ne);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54ddae-ac49-42fd-badb-fe9410f808d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [`llama_model_loader::init_mappings`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L845)\n",
    "\n",
    "```c++\n",
    "ml.init_mappings(true, nullptr);\n",
    "\n",
    "void llama_model_loader::init_mappings(bool prefetch, llama_mlocks * mlock_mmaps) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5c1a2-b9aa-4269-ac36-93b676d6909e",
   "metadata": {},
   "source": [
    "```c++\n",
    "--> llama_mmap::llama_mmap --> llama_mmap::impl::impl --> mapping;\n",
    "llama_model_loader:: std::vector<std::pair<size_t, size_t>> mmaps_used;\n",
    "mmaps_used.emplace_back(mapping->size(), 0);\n",
    "\n",
    "using llama_mmaps  = std::vector<std::unique_ptr<llama_mmap>>;\n",
    "llama_model_loader:: llama_mmaps mappings;\n",
    "mappings.emplace_back(std::move(mapping));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5504b-b640-44c4-bce7-ec63d58875f9",
   "metadata": {},
   "source": [
    "```c++\n",
    "std::unique_ptr<llama_mmap> mapping = std::make_unique<llama_mmap>(file.get(), prefetch ? -1 : 0, is_numa);\n",
    "\n",
    "\n",
    "llama_mmap::llama_mmap(struct llama_file * file, size_t prefetch, bool numa) : pimpl(std::make_unique<impl>(file, prefetch, numa)) {}\n",
    "struct llama_mmap::impl {\n",
    "    std::vector<std::pair<size_t, size_t>> mapped_fragments;\n",
    "    impl(struct llama_file * file, size_t prefetch, bool numa) {\n",
    "        size = file->size();\n",
    "        int fd = file->file_id();\n",
    "        int flags = MAP_SHARED;\n",
    "        if (prefetch) { flags |= MAP_POPULATE; }\n",
    "        --> addr = mmap(NULL, file->size(), PROT_READ, flags, fd, 0);\n",
    "        if (prefetch > 0) {\n",
    "            if (posix_madvise(addr, std::min(file->size(), prefetch), POSIX_MADV_WILLNEED)) {\n",
    "                LLAMA_LOG_WARN(\"warning: posix_madvise(.., POSIX_MADV_WILLNEED) failed: %s\\n\",\n",
    "                        strerror(errno));\n",
    "            }\n",
    "        }\n",
    "        mapped_fragments.emplace_back(0, file->size());\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea6ed7-7a50-44f2-8e68-37447c81f32a",
   "metadata": {},
   "source": [
    "#### [`llama_model_loader::get_mapping_range`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L878)\n",
    "\n",
    "```c++\n",
    "ml.get_mapping_range(&first, &last, &addr, idx=0, ctx);\n",
    "\n",
    "void llama_model_loader::get_mapping_range(size_t * first, size_t * last, void ** addr, int idx, ggml_context * ctx) const {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b56a65-03b8-4a4d-9574-caa9e8d79942",
   "metadata": {},
   "source": [
    "```c++\n",
    "const auto & mapping = mappings.at(idx);\n",
    "\n",
    "*first = mapping->size();\n",
    "*last  = 0;\n",
    "*addr = mapping->addr();\n",
    "for (ggml_tensor * tensor = ggml_get_first_tensor(ctx); tensor; tensor = ggml_get_next_tensor(ctx, tensor)) {\n",
    "    const auto * weight = get_weight(ggml_get_name(tensor));\n",
    "    if (!weight || weight->idx != idx) {\n",
    "        continue;\n",
    "    }\n",
    "    *first = std::min(*first, weight->offs);\n",
    "    *last  = std::max(*last,  weight->offs + ggml_nbytes(tensor));\n",
    "}\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>struct llama_model_loader::llama_tensor_weight</summary>\n",
    "\n",
    "```c++\n",
    "// Holds information on a model weight\n",
    "struct llama_tensor_weight {\n",
    "    uint16_t  idx; // source file index\n",
    "    size_t   offs; // tensor data offset in the original file\n",
    "    ggml_tensor * tensor;\n",
    "}\n",
    "\n",
    "const llama_model_loader::llama_tensor_weight * llama_model_loader::get_weight(const char * name) const {\n",
    "    auto pos = weights_map.find(name);\n",
    "    if (pos != weights_map.end()) {\n",
    "        return &pos->second;\n",
    "    }\n",
    "\n",
    "    return nullptr;\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed2404-8fdc-48b2-8c24-173fd6321bda",
   "metadata": {},
   "source": [
    "#### [`ggml_backend_cpu_buffer_from_ptr`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L2013)\n",
    "\n",
    "```c++\n",
    "ggml_backend_buffer_t buf = ggml_backend_dev_buffer_from_host_ptr(dev, (char *) addr + first, last - first, ggml_get_max_tensor_size(ctx));\n",
    "\n",
    "ggml_backend_buffer_t ggml_backend_dev_buffer_from_host_ptr(ggml_backend_dev_t device, void * ptr, size_t size, size_t max_tensor_size) {\n",
    "    --> return device->iface.buffer_from_host_ptr(device, ptr, size, max_tensor_size);\n",
    "}\n",
    "\n",
    "static ggml_backend_buffer_t ggml_backend_cpu_device_buffer_from_host_ptr(ggml_backend_dev_t dev, void * ptr, size_t size, size_t max_tensor_size) {\n",
    "    --> return ggml_backend_cpu_buffer_from_ptr(ptr, size);\n",
    "}\n",
    "\n",
    "ggml_backend_buffer_t ggml_backend_cpu_buffer_from_ptr(void * ptr, size_t size) {\n",
    "    --> return ggml_backend_buffer_init(ggml_backend_cpu_buffer_from_ptr_type(), ggml_backend_cpu_buffer_from_ptr_i, ptr, size);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ba812-14e6-4642-a5bf-6eb4509254b2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_backend_buffer</summary>\n",
    "\n",
    "```c++\n",
    "struct ggml_backend_buffer {\n",
    "    struct ggml_backend_buffer_i  iface;\n",
    "    ggml_backend_buffer_type_t    buft;\n",
    "    void * context;\n",
    "    size_t size;\n",
    "    enum ggml_backend_buffer_usage usage;\n",
    "};\n",
    "\n",
    "typedef struct ggml_backend_buffer * ggml_backend_buffer_t;\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "// backend buffer\n",
    "\n",
    "ggml_backend_buffer_t ggml_backend_buffer_init(\n",
    "               ggml_backend_buffer_type_t buft,\n",
    "        struct ggml_backend_buffer_i      iface,\n",
    "               void *                     context,\n",
    "               size_t                     size) {\n",
    "    ggml_backend_buffer_t buffer = new ggml_backend_buffer {\n",
    "        /* .interface = */ iface,\n",
    "        /* .buft      = */ buft,\n",
    "        /* .context   = */ context,\n",
    "        /* .size      = */ size,\n",
    "        /* .usage     = */ GGML_BACKEND_BUFFER_USAGE_ANY\n",
    "    };\n",
    "\n",
    "    return buffer;\n",
    "}\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>struct ggml_backend_buffer_type</summary>\n",
    "    \n",
    "```c++\n",
    "struct ggml_backend_buffer_type {\n",
    "    struct ggml_backend_buffer_type_i  iface;\n",
    "    ggml_backend_dev_t device;\n",
    "    void * context;\n",
    "};\n",
    "\n",
    "typedef struct ggml_backend_buffer_type * ggml_backend_buffer_type_t;\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```c++\n",
    "\n",
    "static ggml_backend_buffer_type_t ggml_backend_cpu_buffer_from_ptr_type(void) {\n",
    "    static struct ggml_backend_buffer_type ggml_backend_cpu_buffer_type = {\n",
    "        /* .iface   = */ {\n",
    "            /* .get_name         = */ ggml_backend_cpu_buffer_from_ptr_type_get_name,\n",
    "            /* .alloc_buffer     = */ ggml_backend_cpu_buffer_type_alloc_buffer,\n",
    "            /* .get_alignment    = */ ggml_backend_cpu_buffer_type_get_alignment,\n",
    "            /* .get_max_size     = */ NULL, // defaults to SIZE_MAX\n",
    "            /* .get_alloc_size   = */ NULL, // defaults to ggml_nbytes\n",
    "            /* .is_host          = */ ggml_backend_cpu_buffer_type_is_host,\n",
    "        },\n",
    "        /* .device  = */ NULL, // FIXME ggml_backend_reg_dev_get(ggml_backend_cpu_reg(), 0),\n",
    "        /* .context = */ NULL,\n",
    "    };\n",
    "\n",
    "    return &ggml_backend_cpu_buffer_type;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded1b402-3b72-4096-9f7a-961391ff4079",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [`llama_model_loader::load_all_data`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-model-loader.cpp#L918)\n",
    "\n",
    "```c++\n",
    "ml.load_all_data(ctx, bufs, use_mlock ? &pimpl->mlock_mmaps : NULL, params.progress_callback, params.progress_callback_user_data);\n",
    "\n",
    "bool llama_model_loader::load_all_data(\n",
    "        struct ggml_context * ctx,\n",
    "        llama_buf_map & bufs,\n",
    "        llama_mlocks * lmlocks,\n",
    "        llama_progress_callback progress_callback,\n",
    "        void * progress_callback_user_data) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3dd224-27e8-467f-aad1-7901c3893ce9",
   "metadata": {},
   "source": [
    "```c++\n",
    "for (struct ggml_tensor * cur = ggml_get_first_tensor(ctx); cur != NULL; cur = ggml_get_next_tensor(ctx, cur))\n",
    "    const auto * weight = get_weight(ggml_get_name(cur));\n",
    "    size_t n_size = ggml_nbytes(cur);\n",
    "    const auto & mapping = mappings.at(weight->idx);\n",
    "    ggml_backend_buffer_t buf_mmap = bufs.at(weight->idx);\n",
    "    uint8_t * data = (uint8_t *) mapping->addr() + weight->offs;\n",
    "    --> ggml_backend_tensor_alloc(buf_mmap, cur, data);\n",
    "    auto & mmap_used = mmaps_used[weight->idx];\n",
    "    mmap_used.first  = std::min(mmap_used.first,  weight->offs);\n",
    "    mmap_used.second = std::max(mmap_used.second, weight->offs + n_size);\n",
    "    size_done += n_size;\n",
    "// check if this is the last call and do final cleanup\n",
    "for (uint32_t idx = 0; idx < mappings.size(); idx++)\n",
    "    const auto & mmap_used = mmaps_used.at(idx);\n",
    "    auto & mapping = mappings.at(idx);\n",
    "    --> mapping->unmap_fragment(0, mmap_used.first);\n",
    "    --> if (mmap_used.second != 0) mapping->unmap_fragment(mmap_used.second, mapping->size());\n",
    "return true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40142326-2ae9-489a-addc-70ddb31e281b",
   "metadata": {},
   "source": [
    "```c++\n",
    "enum ggml_status ggml_backend_tensor_alloc(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor, void * addr)\n",
    "    tensor->buffer = buffer;\n",
    "    tensor->data = addr;\n",
    "    return ggml_backend_buffer_init_tensor(buffer, tensor);\n",
    "\n",
    "void llama_mmap::unmap_fragment(size_t first, size_t last) { pimpl->unmap_fragment(first, last); }\n",
    "\n",
    "struct llama_mmap::impl {\n",
    "    void unmap_fragment(size_t first, size_t last) {\n",
    "        int page_size = sysconf(_SC_PAGESIZE);\n",
    "        align_range(&first, &last, page_size);\n",
    "        size_t len = last - first;\n",
    "        void * next_page_start = (uint8_t *) addr + first;\n",
    "        munmap(next_page_start, len);\n",
    "        for (const auto & frag : mapped_fragments) new_mapped_fragments.emplace_back(last, frag.second);\n",
    "        mapped_fragments = std::move(new_mapped_fragments);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635e210-5a12-4060-b2ef-1aae86fe9b14",
   "metadata": {},
   "source": [
    "## [`llama_context::llama_context`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L18)\n",
    "\n",
    "```c++\n",
    "llama_context * lctx = llama_init_from_model(model, cparams);\n",
    "\n",
    "llama_context * llama_init_from_model(\n",
    "                 llama_model * model,\n",
    "        llama_context_params   params) {\n",
    "    --> auto * ctx = new llama_context(*model, params);\n",
    "    return ctx;\n",
    "}\n",
    "\n",
    "llama_context::llama_context(\n",
    "        const llama_model & model,\n",
    "              llama_context_params params) :\n",
    "    model(model) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42f04f-0568-431f-940f-bf7e71945255",
   "metadata": {},
   "source": [
    "```c++\n",
    "// GPU backends\n",
    "// add ACCEL backends (such as BLAS)\n",
    "// add CPU backend\n",
    "llama_context:: ggml_backend_t backend_cpu = nullptr;\n",
    "--> ggml_backend_init_by_type --> ggml_backend_dev_init --> ggml_backend_cpu_device_init_backend --> ggml_backend_cpu_init --> backend_cpu;\n",
    "backends.emplace_back(backend_cpu);\n",
    "// create a list of the set_n_threads functions in the backends\n",
    "llama_context:: std::vector<std::pair<ggml_backend_t, ggml_backend_set_n_threads_t>>\n",
    "set_n_threads_fns.emplace_back(backend.get(), ggml_backend_set_n_threads_fn);\n",
    "// graph outputs buffer\n",
    "// resized during inference when a batch uses more outputs\n",
    "--> llama_context::output_reserve;\n",
    "\n",
    "// init the memory module\n",
    "llama_memory_params params_mem = {\n",
    "    /*.type_k   =*/ params.type_k,\n",
    "    /*.type_v   =*/ params.type_v,\n",
    "    /*.swa_full =*/ params.swa_full,\n",
    "};\n",
    "llama_context:: std::unique_ptr<llama_memory_i> memory;\n",
    "--> llama_model::create_memory --> llama_context:: memory;\n",
    "\n",
    "// init backends\n",
    "backend_buft.clear();\n",
    "backend_ptrs.clear();\n",
    "//for (auto & backend : backends)\n",
    "auto * buft = ggml_backend_get_default_buffer_type(backend.get());\n",
    "auto backend_type = ggml_backend_dev_type(ggml_backend_get_device(backend.get()));\n",
    "backend_buft.push_back(buft);\n",
    "backend_ptrs.push_back(backend.get());\n",
    "\n",
    "const size_t max_nodes = this->graph_max_nodes();\n",
    "// memory buffers used to evaluate the model\n",
    "llama_context:: std::vector<uint8_t> buf_compute_meta;\n",
    "// buffer used to store the computation graph and the tensor meta data\n",
    "--> ggml_graph_nbytes --> buf_compute_meta;\n",
    "llama_context:: ggml_backend_sched_ptr sched;\n",
    "--> ggml_backend_sched_new --> sched;\n",
    "\n",
    "// reserve worst-case graph\n",
    "const uint32_t n_seqs = cparams.n_seq_max;\n",
    "const uint32_t n_tokens = std::min(cparams.n_ctx, cparams.n_ubatch);\n",
    "// simulate full KV cache\n",
    "--> llama_kv_cache_unified_state::llama_kv_cache_unified_state --> mstate;\n",
    "\n",
    "// reserve pp graph first so that buffers are only allocated once\n",
    "--> llama_context::graph_reserve --> gf;\n",
    "// reserve with tg graph to get the number of splits and nodes\n",
    "// reserve again with pp graph to avoid ggml-alloc reallocations during inference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844a666-be63-4d77-bfde-6ee25055d585",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [`ggml_backend_cpu_init`]()\n",
    "\n",
    "```c++\n",
    "backend_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);\n",
    "\n",
    "ggml_backend_t ggml_backend_init_by_type(enum ggml_backend_dev_type type, const char * params) {\n",
    "    ggml_backend_dev_t dev = ggml_backend_dev_by_type(type);\n",
    "    if (!dev) {\n",
    "        return nullptr;\n",
    "    }\n",
    "    --> return ggml_backend_dev_init(dev, params);\n",
    "}\n",
    "\n",
    "\n",
    "ggml_backend_dev_t ggml_backend_dev_by_type(enum ggml_backend_dev_type type)\n",
    "    for (size_t i = 0; i < ggml_backend_dev_count(); i++) {\n",
    "        ggml_backend_dev_t dev = ggml_backend_dev_get(i);\n",
    "        if (ggml_backend_dev_type(dev) == type) {\n",
    "            return dev;\n",
    "        }\n",
    "    }\n",
    "\n",
    "ggml_backend_t ggml_backend_dev_init(ggml_backend_dev_t device, const char * params)\n",
    "    --> return device->iface.init_backend(device, params);\n",
    "\n",
    "static ggml_backend_t ggml_backend_cpu_device_init_backend(ggml_backend_dev_t dev, const char * params)\n",
    "    --> return ggml_backend_cpu_init();\n",
    "\n",
    "ggml_backend_t ggml_backend_cpu_init(void) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198c8f7-2d5f-4d39-928d-122e8bb7b8f2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>struct ggml_backend</summary>\n",
    "\n",
    "```c++\n",
    "\n",
    "struct ggml_backend {\n",
    "    ggml_guid_t guid;\n",
    "    struct ggml_backend_i iface;\n",
    "    ggml_backend_dev_t device;\n",
    "    void * context;\n",
    "};\n",
    "\n",
    "typedef struct ggml_backend * ggml_backend_t;\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "```c++\n",
    "// initialize CPU backend now to avoid slowing the first graph computation\n",
    "ggml_cpu_init();\n",
    "\n",
    "struct ggml_backend_cpu_context * ctx = new ggml_backend_cpu_context;\n",
    "\n",
    "ctx->n_threads           = GGML_DEFAULT_N_THREADS;\n",
    "ctx->threadpool          = NULL;\n",
    "ctx->work_data           = NULL;\n",
    "ctx->work_size           = 0;\n",
    "ctx->abort_callback      = NULL;\n",
    "ctx->abort_callback_data = NULL;\n",
    "\n",
    "ggml_backend_t cpu_backend = new ggml_backend {\n",
    "    /* .guid      = */ ggml_backend_cpu_guid(),\n",
    "    /* .interface = */ ggml_backend_cpu_i,\n",
    "    /* .device    = */ ggml_backend_reg_dev_get(ggml_backend_cpu_reg(), 0),\n",
    "    /* .context   = */ ctx,\n",
    "};\n",
    "\n",
    "return cpu_backend;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f065a-2f96-4902-8503-06b7e5d3ed02",
   "metadata": {},
   "source": [
    "### [`llama_context::output_reserve`](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-context.cpp#L1239)\n",
    "\n",
    "```c++\n",
    "output_reserve(params.n_seq_max);\n",
    "\n",
    "int32_t llama_context::output_reserve(int32_t n_outputs) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbf15b-b5f0-4e74-9322-0f72dcf2c072",
   "metadata": {},
   "source": [
    "```c++\n",
    "// map batch token positions to ids of the logits and embd buffers\n",
    "llama_context:: std::vector<int32_t> output_ids;\n",
    "output_ids.resize(n_batch);\n",
    "\n",
    "const int64_t n_outputs_max = std::max<int64_t>(n_outputs, n_seq_max());\n",
    "logits_size = has_logits ? n_vocab*n_outputs_max : 0;\n",
    "const size_t new_size  = (logits_size + embd_size) * sizeof(float);\n",
    "auto * buft = ggml_backend_cpu_buffer_type();\n",
    "// host buffer for the model output (logits and embeddings)\n",
    "llama_context:: ggml_backend_buffer_ptr buf_output;\n",
    "--> ggml_backend_buft_alloc_buffer --> ggml_backend_cpu_buffer_type_alloc_buffer --> buf_output;\n",
    "\n",
    "--> ggml_backend_buffer_get_base --> ggml_backend_cpu_buffer_get_base --> output_base;\n",
    "logits = has_logits ? output_base : nullptr;\n",
    "// set all ids as invalid (negative)\n",
    "std::fill(output_ids.begin(), output_ids.end(), -1);\n",
    "this->n_outputs = 0;\n",
    "this->n_outputs_max = n_outputs_max;\n",
    "return n_outputs_max;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477f94d-3156-4dc7-ac70-7d921e4a3f2d",
   "metadata": {},
   "source": [
    "[`ggml_backend_cpu_buffer_type_alloc_buffer`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1950)\n",
    "\n",
    "```c++\n",
    "ggml_backend_buffer_ptr llama_context:: buf_output.reset(ggml_backend_buft_alloc_buffer(buft, new_size));\n",
    "\n",
    "ggml_backend_buffer_t ggml_backend_buft_alloc_buffer(ggml_backend_buffer_type_t buft, size_t size) {\n",
    "    if (size == 0) {\n",
    "        // return a dummy buffer for zero-sized allocations\n",
    "        return ggml_backend_buffer_init(buft, {}, NULL, 0);\n",
    "    }\n",
    "    --> return buft->iface.alloc_buffer(buft, size);\n",
    "}\n",
    "\n",
    "static ggml_backend_buffer_t ggml_backend_cpu_buffer_type_alloc_buffer(ggml_backend_buffer_type_t buft, size_t size) {\n",
    "    void * data = ggml_aligned_malloc(size);\n",
    "    return ggml_backend_buffer_init(buft, ggml_backend_cpu_buffer_i, data, size);\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348dedc-0e61-4c11-91bf-a825ae29e133",
   "metadata": {},
   "source": [
    "[`ggml_backend_cpu_buffer_get_base`](https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-backend.cpp#L1869)\n",
    "\n",
    "```c++\n",
    "float * output_base = (float *) ggml_backend_buffer_get_base(buf_output.get());\n",
    "\n",
    "void * ggml_backend_buffer_get_base(ggml_backend_buffer_t buffer) {\n",
    "    // get_base is optional if the buffer is zero-sized\n",
    "    if (buffer->size == 0) {\n",
    "        return NULL;\n",
    "    }\n",
    "    return buffer->iface.get_base(buffer);\n",
    "}\n",
    "\n",
    "static void * ggml_backend_cpu_buffer_get_base(ggml_backend_buffer_t buffer) {\n",
    "    uintptr_t data = (uintptr_t)buffer->context;\n",
    "\n",
    "    // align the buffer\n",
    "    if (data % TENSOR_ALIGNMENT != 0) {\n",
    "        data = GGML_PAD(data, TENSOR_ALIGNMENT);\n",
    "    }\n",
    "\n",
    "    return (void *)data;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c21825-1ade-484e-a922-f1f009a36f1b",
   "metadata": {},
   "source": [
    "### [`llama_model::create_memory`](src/llama-model.cpp#L13206)\n",
    "\n",
    "\n",
    "```c++\n",
    "memory.reset(model.create_memory(params_mem, cparams));\n",
    "\n",
    "llama_memory_i * llama_model::create_memory(const llama_memory_params & params, llama_cparams & cparams) const {}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7da1ea-1d80-4200-8a17-00d86ace2407",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [`ggml_graph_nbytes`](ggml/src/ggml.c#L5961)\n",
    "\n",
    "```c++\n",
    "buf_compute_meta.resize(ggml_tensor_overhead()*max_nodes + ggml_graph_overhead_custom(max_nodes, false));\n",
    "\n",
    "size_t ggml_graph_overhead_custom(size_t size, bool grads) {\n",
    "    return GGML_OBJECT_SIZE + GGML_PAD(ggml_graph_nbytes(size, grads), GGML_MEM_ALIGN);\n",
    "}\n",
    "\n",
    "static size_t ggml_graph_nbytes(size_t size, bool grads) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4298b9-4dbc-4a12-9752-2297cb732f4c",
   "metadata": {},
   "source": [
    "```c++\n",
    "size_t hash_size = ggml_hash_size(size * 2);\n",
    "void * p = 0;\n",
    "incr_ptr_aligned(&p, sizeof(struct ggml_cgraph), 1);\n",
    "incr_ptr_aligned(&p, size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)); // nodes\n",
    "incr_ptr_aligned(&p, size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)); // leafs\n",
    "incr_ptr_aligned(&p, hash_size * sizeof(struct ggml_tensor *), sizeof(struct ggml_tensor *)); // hash keys\n",
    "incr_ptr_aligned(&p, ggml_bitset_size(hash_size) * sizeof(ggml_bitset_t), sizeof(ggml_bitset_t));\n",
    "\n",
    "size_t nbytes = (size_t) p;\n",
    "return nbytes;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e48e9-d085-43ff-bae4-8a10ce230c0f",
   "metadata": {},
   "source": [
    "### [`ggml_backend_sched_new`](ggml/src/ggml-backend.cpp#L1455)\n",
    "\n",
    "```c++\n",
    "sched.reset(ggml_backend_sched_new(backend_ptrs.data(), backend_buft.data(), backend_ptrs.size(), max_nodes, pipeline_parallel, cparams.op_offload));\n",
    "\n",
    "ggml_backend_sched_t ggml_backend_sched_new(\n",
    "        ggml_backend_t * backends,\n",
    "        ggml_backend_buffer_type_t * bufts,\n",
    "        int n_backends,\n",
    "        size_t graph_size,\n",
    "        bool parallel,\n",
    "        bool op_offload) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c4e74-0310-4b08-98f4-ac271b0818d3",
   "metadata": {},
   "source": [
    "### [`llama_kv_cache_unified_state::llama_kv_cache_unified_state`](src/llama-kv-cache-unified.cpp#L1668)\n",
    "\n",
    "```c++\n",
    "const auto mstate = memory->init_full();\n",
    "\n",
    "llama_memory_state_ptr llama_kv_cache_unified::init_full() {\n",
    "    return std::make_unique<llama_kv_cache_unified_state>(this);\n",
    "}\n",
    "\n",
    "class llama_kv_cache_unified_state : public llama_memory_state_i {\n",
    "public:\n",
    "    // used to create a full-cache state\n",
    "    llama_kv_cache_unified_state(\n",
    "            llama_kv_cache_unified * kv);\n",
    "}\n",
    "\n",
    "llama_kv_cache_unified_state::llama_kv_cache_unified_state(\n",
    "        llama_kv_cache_unified * kv) : status(LLAMA_MEMORY_STATUS_SUCCESS), kv(kv) {\n",
    "    n_kv = kv->get_size();\n",
    "    head = 0;\n",
    "}           \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc1a12-1a27-4be5-9673-58f3d51d7c5c",
   "metadata": {},
   "source": [
    "### [`llama_context::graph_reserve`](src/llama-context.cpp#L1331)\n",
    "\n",
    "```c++\n",
    "auto * gf = graph_reserve(n_tokens, n_seqs, n_tokens, mstate.get());\n",
    "auto * gf = graph_reserve(1, 1, 1, mstate.get());\n",
    "\n",
    "ggml_cgraph * llama_context::graph_reserve(uint32_t n_tokens, uint32_t n_seqs, uint32_t n_outputs, const llama_memory_state_i * mstate) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7034405-372b-4f82-803f-87dca0e81d41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## `llama_decode`\n",
    "\n",
    "```c++\n",
    "llama_decode(lctx, llama_batch_get_one(tmp.data(), std::min(tmp.size(), (size_t) params.n_batch)));\n",
    "\n",
    "int32_t llama_decode(\n",
    "        llama_context * ctx,\n",
    "        llama_batch   batch) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956178f8-f679-474a-a2e8-dc00bf58cdac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# `perplexity`\n",
    "\n",
    "```c++\n",
    "results = perplexity(ctx, params, n_ctx);\n",
    "\n",
    "static results_perplexity perplexity(llama_context * ctx, const common_params & params, const int32_t n_ctx) {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a1c09-8369-40ef-943f-36d6e8a90f15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# `llama_backend_free`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
