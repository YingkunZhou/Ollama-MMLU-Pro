mainly refered benchmark-eval repositories:
0. [evalscope.readthedocs.io](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html)
1. [modelscope/evalscope](https://github.com/modelscope/evalscope/blob/main/evalscope/benchmarks)
2. [openai/simple-evals](https://github.com/openai/simple-evals/blob/main/common.py)
3. [EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks)
4. [openai/gpt-oss](https://github.com/openai/gpt-oss/blob/main/gpt_oss/evals)
5. [chigkim/Ollama-MMLU-Pro](https://github.com/chigkim/Ollama-MMLU-Pro)
6. [Qwen3 Standardize Output Format](https://huggingface.co/Qwen/Qwen3-32B)

## HumanEval (Coding)

## LiveCodeBench (Coding, Reasoning)

## Winogrande (MCQ/Prefill)

## ARC-Challenge (MCQ)

## MMLU-Redux (MCQ)

## MMLU-Pro (MCQ)

## GPQA-Diamond (MCQ Reasoning)

## GSM8K (Math, Reasoning?)

## AIME-2025 (Math, Reasoning)
