mainly refered benchmark-eval repositories:
0. [evalscope.readthedocs.io](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html)
1. [modelscope/evalscope](https://github.com/modelscope/evalscope/blob/main/evalscope/benchmarks)
2. [openai/simple-evals](https://github.com/openai/simple-evals/blob/main/common.py)
3. [EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks)
4. [openai/gpt-oss](https://github.com/openai/gpt-oss/blob/main/gpt_oss/evals)
5. [chigkim/Ollama-MMLU-Pro](https://github.com/chigkim/Ollama-MMLU-Pro)
6. [Qwen3 Standardize Output Format](https://huggingface.co/Qwen/Qwen3-32B)

## ~~Winogrande (MCQ/Prefill)~~

## ARC-Challenge (MCQ)

## MMLU-Redux (MCQ)

## MMLU-Pro (MCQ)

Reasoning

- law
- engineering
- history?
- computer science

## GPQA-Diamond (MCQ Reasoning)

## GSM8K (Math)

- Reasoning?

## AIME-2025 (Math Reasoning)

- non-Reasoning?

## HumanEval (Coding)

## LiveCodeBench (Coding, Reasoning)

# non-Reasoning

1. ARC-Challenge (1172)
2. MMLU-Redux (5700)
3. MMLU-Pro (12032)
4. GSM8K (1319)
5. HumanEval (164)

# Reasoning

1. MMLU-Pro -- law (1101)
2. MMLU-Pro -- engineering (969)
3. GPQA-Diamond (198)
4. AIME-2025 (30)
5. LiveCodeBench (175)
