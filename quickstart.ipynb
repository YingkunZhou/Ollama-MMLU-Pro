{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for find out more details if needed\n",
    "%set_env LLAMA_LOG_VERBOSITY=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LLAMA_SET_ROWS to reduce rebuild graph time, in order to improve the performance\n",
    "# FIXME: running perplexity has something wrong!\n",
    "%set_env LLAMA_SET_ROWS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env GGML_CUDA_DISABLE_GRAPHS=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Quick build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build for cpu debug\n",
    "%cd llama.cpp\n",
    "!cmake -B cpu_debug -DGGML_CUDA=OFF -DGGML_RPC=OFF -DGGML_BLAS=OFF \\\n",
    "    -DGGML_SCHED_MAX_COPIES=1 -DLLAMA_CURL=OFF -DCMAKE_BUILD_TYPE=Debug\n",
    "!cmake --build cpu_debug --config Debug -j $(nproc)\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build for gpu debug\n",
    "%cd llama.cpp\n",
    "!cmake -B cuda_debug -DGGML_CUDA=ON -DGGML_RPC=OFF -DGGML_BLAS=OFF -DCMAKE_CUDA_ARCHITECTURES=\"80;86;89\" \\\n",
    "    -DGGML_SCHED_MAX_COPIES=1 -DLLAMA_CURL=OFF -DCMAKE_BUILD_TYPE=Debug\n",
    "!cmake --build cuda_debug --config Debug -j $(nproc)\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build for cpu execution\n",
    "%cd llama.cpp\n",
    "!cmake -B cpu_build -DGGML_CUDA=OFF -DGGML_RPC=OFF -DGGML_BLAS=OFF \\\n",
    "    -DGGML_SCHED_MAX_COPIES=1 -DLLAMA_CURL=OFF\n",
    "!cmake --build cpu_build --config Release -j $(nproc)\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build for gpu execution\n",
    "%cd llama.cpp\n",
    "!cmake -B cuda_build -DGGML_CUDA=ON -DGGML_RPC=OFF -DGGML_BLAS=OFF -DCMAKE_CUDA_ARCHITECTURES=\"80;86;89\" \\\n",
    "    -DGGML_SCHED_MAX_COPIES=1 -DLLAMA_CURL=OFF\n",
    "!cmake --build cuda_build --config Release -j $(nproc)\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ikawrakow/ik_llama.cpp --depth 1\n",
    "%cd ik_llama.cpp\n",
    "!cmake -B cuda_build -DGGML_CUDA=ON -DGGML_RPC=OFF -DGGML_BLAS=OFF -DGGML_SCHED_MAX_COPIES=1 -DLLAMA_CURL=OFF\n",
    "!cmake --build cuda_build --config Release -j $(nproc)\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 42 test\n",
    "\n",
    "- [TinyStories-656K](https://hf-mirror.com/mradermacher/TinyStories-656K-GGUF)\n",
    "\n",
    "<details>\n",
    "<summary>config</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"architectures\": [\n",
    "    \"LlamaForCausalLM\"\n",
    "  ],\n",
    "  \"attention_bias\": false,\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 1,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 128,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 384,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"mlp_bias\": false,\n",
    "  \"model_type\": \"llama\",\n",
    "  \"num_attention_heads\": 8,\n",
    "  \"num_hidden_layers\": 2,\n",
    "  \"num_key_value_heads\": 4,\n",
    "  \"pretraining_tp\": 1,\n",
    "  \"rms_norm_eps\": 1e-06,\n",
    "  \"rope_scaling\": null,\n",
    "  \"rope_theta\": 10000.0,\n",
    "  \"tie_word_embeddings\": true,\n",
    "  \"torch_dtype\": \"bfloat16\",\n",
    "  \"transformers_version\": \"4.41.2\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 2048\n",
    "}\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"models/TinyStories-656K.f16.gguf\"\n",
    "assert os.path.exists(model)\n",
    "!llama.cpp/cpu_build/bin/llama-cli -m {model} -p \"the answer to the ultimate question of life, the universe, and everything is 42. One day, they \" \\\n",
    "-t 1 -n 2 --temp 0 --top-k 0 --top-p 1.0 --min-p 0.0 2>/dev/null"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " the answer to the ultimate question of life, the universe, and everything is 42. One day, they saw a small "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hello world test\n",
    "\n",
    "- [SmolLM2-135M](https://hf-mirror.com/bartowski/SmolLM2-135M-Instruct-GGUF/tree/main)\n",
    "\n",
    "<details>\n",
    "<summary>Prompt format</summary>\n",
    "\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "# Q4_K_L == Q4_K_M, Q5_K_L == Q5_K_M\n",
    "model = \"models/SmolLM2-135M-Instruct-f16.gguf\"\n",
    "assert os.path.exists(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"Give the most simple C program to print 'hello world!'\" # Q4_K_S or under will be failed\n",
    "prompt = prefix + question + suffix\n",
    "!llama.cpp/cpu_build/bin/llama-cli -m {model} -p \"{prompt}\" -no-cnv -t 1 --temp 0 --top-k 0 --top-p 1.0 --min-p 0.0 2>/dev/null"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    printf(\"Hello, World!\\n\");\n",
    "    return 0;\n",
    "} [end of text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give the most simple C program to print 'Love world!'\" # not work for Q8_0/Q6_K, Q5_K_S is the lowest, Q4 or under will be failed\n",
    "# question = \"Give the most simple C program to print 'love world!'\" # Q4 or under will be failed\n",
    "prompt = prefix + question + suffix\n",
    "!llama.cpp/cpu_build/bin/llama-cli -m {model} -p \"{prompt}\" -no-cnv -t 1 --temp 0 --top-k 0 --top-p 1.0 --min-p 0.0 2>/dev/null"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    printf(\"Love world!\\n\");\n",
    "    return 0;\n",
    "} [end of text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Manga auther test\n",
    "\n",
    "- [Llama-3.2-1B](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF/tree/main)\n",
    "- [Official text_prompt_format](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md)\n",
    "\n",
    "<details>\n",
    "<summary>Prompt format</summary>\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 26 Jul 2024\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Q4_K_M/S 展现出了随着prefix和suffix微小变化的不稳定性\n",
    "# prefix = \"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "# prefix = \"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Dec 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "# prefix = \"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Oct 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "# suffix = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # Q4_K_M only corrent for 火影忍者的作者是谁？ pair with Jul 2024\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4_0/awq will be failed\n",
    "question = \"Who is the author of 'Demon Slayer'?\"\n",
    "answer = \"The author of the popular manga and anime series 'Demon Slayer: Kimetsu no Yaiba' is Koyoharu Gotoge. [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awq will be failed\n",
    "question = \"Who is the author of 'Chainsaw Man'?\"\n",
    "answer = \"The author of the manga and anime series 'Chainsaw Man' is Tatsuki Fujimoto. [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [awq] will be failed\n",
    "question = \"Who is the author of 'Detective Conan'?\"\n",
    "answer = \"The author of the popular manga and anime series 'Detective Conan' is Gosho Aoyama. [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under-4-bit will be failed\n",
    "question = \"Who is the author of Manga 'Slam Dunk'?\"\n",
    "answer = 'The author of the popular manga series \"Slam Dunk\" is Takehiko Inoue. [end of text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under-4-bit will be failed\n",
    "question = \"Who is the author of 'Berserk'?\"\n",
    "answer = 'The author of the manga and anime series \"Berserk\" is Kentaro Miura. [end of text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Q5_K_M/Q5_K_S**/[Q4_K_M/Q4_K_S]/IQ4_XS will be failed\n",
    "question = \"Naruto的作者是谁？\"\n",
    "answer = \"Naruto的作者是Masashi Kishimoto [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Q4_K_M/Q4_K_S]/IQ4_XS/awq/gptq will be failed\n",
    "question = \"火影忍者的作者是谁？\"\n",
    "answer = \"火影忍者是由Masashi Kishimoto所创作的日本动画和漫画。 [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Qwen3\" in model_name:\n",
    "    prefix = \"<|im_start|>user\\n\"\n",
    "    suffix = \" /no_think<|im_end|>\\n<|im_start|>assistant\"\n",
    "    # suffix = \" /think<|im_end|>\\n<|im_start|>assistant\"\n",
    "else: # Llama-3\n",
    "    prefix = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    suffix = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "prompt = prefix + question + suffix\n",
    "!llama.cpp/cpu_build/bin/llama-cli -m {model} -p \"{prompt}\" -c 2048 -n 512 --temp 0 --top-k 1 --seed 42 -ngl 100 -no-cnv 2>/dev/null\n",
    "print(\"\\n\"+answer)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# model_name = \"Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"Llama-3.3-70B-Instruct\"\n",
    "# model_name = \"Llama-3_3-Nemotron-Super-49B-v1_5\"\n",
    "# model_name = \"DeepSeek-R1-Distill-Llama-70B\"\n",
    "# model_name = \"DeepSeek-R1-Distill-Qwen-14B\"\n",
    "# model_name = \"DeepSeek-R1-Distill-Qwen-32B\"\n",
    "# model_name = \"Qwen3-0.6B\"\n",
    "# model_name = \"Qwen3-8B\"\n",
    "# model_name = \"Qwen3-14B\"\n",
    "model_name = \"Qwen3-32B\"\n",
    "# model_name = \"gemma-3-12b-it\"\n",
    "# model_name = \"gemma-3-27b-it\"\n",
    "# model_name = \"phi-4\"\n",
    "# model_name = \"Phi-4-reasoning-plus\"\n",
    "# model_name = \"Mistral-Small-3.2-24B-Instruct-2506\"\n",
    "# model_name = \"Magistral-Small-2509\"\n",
    "\n",
    "hf_model = f\"models/{model_name}\"\n",
    "assert os.path.exists(hf_model)\n",
    "model = f\"models/{model_name}.gguf\"\n",
    "bf16_model = f\"models/{model_name}-BF16.gguf\"\n",
    "quant_option = \"--token-embedding-type q6_K --output-tensor-type q6_K\"\n",
    "\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py --outtype bf16 {hf_model} --outfile {model}\n",
    "!ik_llama.cpp/cuda_build/bin/llama-quantize {quant_option} {model} {bf16_model} bf16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>imatrix modification for dumping activations</summary>\n",
    "\n",
    "```diff\n",
    "diff --git a/tools/imatrix/imatrix.cpp b/tools/imatrix/imatrix.cpp\n",
    "index cf07d99d..21678d82 100644\n",
    "--- a/tools/imatrix/imatrix.cpp\n",
    "+++ b/tools/imatrix/imatrix.cpp\n",
    "@@ -37,9 +37,17 @@ static const char * const LLM_KV_IMATRIX_DATASETS    = \"imatrix.datasets\";\n",
    " static const char * const LLM_KV_IMATRIX_CHUNK_COUNT = \"imatrix.chunk_count\";\n",
    " static const char * const LLM_KV_IMATRIX_CHUNK_SIZE  = \"imatrix.chunk_size\";\n",
    " \n",
    "+#define DUMP_ACTIVATION 0\n",
    "+#define DUMP_THRESHOLDS 1\n",
    " struct Stats {\n",
    "     std::vector<float>   values;\n",
    "     std::vector<int64_t> counts;\n",
    "+#if DUMP_ACTIVATION || DUMP_THRESHOLDS\n",
    "+    std::vector<float> activations;\n",
    "+#endif\n",
    "+#if DUMP_THRESHOLDS\n",
    "+    std::vector<float> top_k;\n",
    "+#endif\n",
    " };\n",
    " \n",
    " struct tensor_statistics {\n",
    "@@ -216,6 +224,26 @@ static void compute_cossim(std::vector<tensor_statistics> & tstats) {\n",
    "     }\n",
    " }\n",
    " \n",
    "+#if DUMP_THRESHOLDS\n",
    "+static void bucket_process(const std::vector<float>& arr, std::vector<float>& top_k) {\n",
    "+    const size_t CHUNK = 1024;\n",
    "+    size_t N = arr.size();\n",
    "+    size_t num_groups = N / CHUNK;\n",
    "+\n",
    "+    for (size_t g = 0; g < num_groups; g++) {\n",
    "+        const float* base = arr.data() + g * CHUNK;\n",
    "+\n",
    "+        float tmp[CHUNK];\n",
    "+        std::copy(base, base + CHUNK, tmp);\n",
    "+\n",
    "+        int pos_median = CHUNK/2;\n",
    "+        std::nth_element(tmp, tmp + pos_median, tmp + CHUNK);\n",
    "+        float median = tmp[pos_median];\n",
    "+        top_k.push_back(median);\n",
    "+    }\n",
    "+}\n",
    "+#endif\n",
    "+\n",
    " bool IMatrixCollector::collect_imatrix(struct ggml_tensor * t, bool ask, void * user_data) {\n",
    "     GGML_UNUSED(user_data);\n",
    " \n",
    "@@ -340,6 +368,12 @@ bool IMatrixCollector::collect_imatrix(struct ggml_tensor * t, bool ask, void *\n",
    "         if (e.values.empty()) {\n",
    "             e.values.resize(src1->ne[0] * n_mat, 0);\n",
    "             e.counts.resize(n_mat, 0);\n",
    "+#if DUMP_ACTIVATION\n",
    "+            e.activations.resize(src1->ne[0]*src1->ne[1]);\n",
    "+#elif DUMP_THRESHOLDS\n",
    "+            e.activations.resize(src1->ne[0]);\n",
    "+            e.top_k.clear();\n",
    "+#endif\n",
    "         }\n",
    "         else if (e.values.size() != (size_t)(src1->ne[0] * n_mat)) {\n",
    "             LOG_ERR(\"%s: inconsistent size for %s (%d vs %d)\\n\", __func__, wname.c_str(), (int)e.values.size(), (int)(src1->ne[0] * n_mat));\n",
    "@@ -349,6 +383,9 @@ bool IMatrixCollector::collect_imatrix(struct ggml_tensor * t, bool ask, void *\n",
    "             LOG_ERR(\"%s: inconsistent expert count for %s (%d vs %d)\\n\", __func__, wname.c_str(), (int)e.counts.size(), (int)n_mat);\n",
    "             exit(1); //GGML_ABORT(\"fatal error\");\n",
    "         }\n",
    "+#if DUMP_THRESHOLDS\n",
    "+        e.activations.assign(e.activations.size(), 0.0f);\n",
    "+#endif\n",
    "         LOG_DBGV(2, \"%s[%d]: %32s, %s, %5d x %5d x %5d, %d\\n\", __func__, m_last_chunk, wname.c_str(), ggml_op_name(t->op), (int)src1->ne[0], (int)src1->ne[1], (int)src1->ne[2], (int)src1->type);\n",
    "         for (int64_t i3 = 0; i3 < src1->ne[3]; ++i3) {\n",
    "             for (int64_t i2 = 0; i2 < src1->ne[2]; ++i2) {\n",
    "@@ -358,13 +395,29 @@ bool IMatrixCollector::collect_imatrix(struct ggml_tensor * t, bool ask, void *\n",
    "                 for (int64_t row = 0; row < src1->ne[1]; ++row) {\n",
    "                     const float * x = (const float *) (data + row * src1->nb[1] + i2 * src1->nb[2] + i3 * src1->ne[3]);\n",
    "                     e.counts[mat_id]++;\n",
    "+#if DUMP_ACTIVATION\n",
    "+                    auto activations = e.activations.data() + i11*src1->ne[0];\n",
    "+#elif DUMP_THRESHOLDS\n",
    "+                    auto activations = e.activations.data();\n",
    "+#endif\n",
    "                     for (int64_t j = 0; j < src1->ne[0]; ++j) {\n",
    "                         e.values[mat_start + j] += x[j] * x[j];\n",
    "+#if DUMP_ACTIVATION\n",
    "+                        activations[j] = x[j]*x[j];\n",
    "+#elif DUMP_THRESHOLDS\n",
    "+                        activations[j] += x[j]*x[j];\n",
    "+#endif\n",
    "                         if (!std::isfinite((float)e.values[j])) {\n",
    "                             LOG_ERR(\"%f detected in %s\\n\", (float)e.values[j], wname.c_str());\n",
    "                             exit(1);\n",
    "                         }\n",
    "                     }\n",
    "+#if DUMP_THRESHOLDS\n",
    "+                    if (row%5 == 4) {\n",
    "+                        bucket_process(e.activations, e.top_k);\n",
    "+                        e.activations.assign(e.activations.size(), 0.0f);\n",
    "+                    }\n",
    "+#endif\n",
    "                 }\n",
    "                 const int32_t n_chunk = e.counts[mat_id] / chunk_size;\n",
    "                 if (n_chunk > m_last_chunk) {\n",
    "@@ -466,6 +519,21 @@ void IMatrixCollector::save_imatrix_legacy(int32_t ncall) const {\n",
    "                 tmp[i] = (value / count) * static_cast<float>(ncall);\n",
    "             }\n",
    "             out.write((const char *) tmp.data(), nval * sizeof(float));\n",
    "+#if DUMP_ACTIVATION\n",
    "+            int nact = stat.activations.size();\n",
    "+            out.write((const char *) &nact, sizeof(nact));\n",
    "+            out.write((const char*)stat.activations.data(), nact*sizeof(float));\n",
    "+#elif DUMP_THRESHOLDS\n",
    "+            size_t index0 = std::ceil(0.1 * stat.top_k.size());\n",
    "+            std::vector<float> temp0 = stat.top_k;\n",
    "+            std::nth_element(temp0.begin(), temp0.begin() + index0, temp0.end());\n",
    "+            out.write((const char *)&temp0[index0], sizeof(float));\n",
    "+\n",
    "+            size_t index1 = std::ceil(0.9 * stat.top_k.size());\n",
    "+            std::vector<float> temp1 = stat.top_k;\n",
    "+            std::nth_element(temp1.begin(), temp1.begin() + index1, temp1.end());\n",
    "+            out.write((const char *)&temp1[index1], sizeof(float));\n",
    "+#endif\n",
    "         }\n",
    "     }\n",
    " \n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ~~Get the imatrix file~~\n",
    "# now, we used llama-imatrix to get median info of each layers about 20 minutes for Qwen3-32B BF16\n",
    "calib_file = f\"calibration_data/calibration_datav3.txt\"\n",
    "imatrix_file = f\"models/{model_name}.med\"\n",
    "# to use cuda backend, otherwise will be very very slow\n",
    "!llama.cpp/cuda_build/bin/llama-imatrix -m {bf16_model} -f {calib_file} --output-file {imatrix_file} -ngl 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# quant_type = \"exl3-2.0bpw\"\n",
    "# quant_type = \"exl3-2.25bpw\"\n",
    "# quant_type = \"IQ2_KT\"\n",
    "quant_type = \"IQ2_K\"\n",
    "# quant_type = \"IQ2_M\"\n",
    "\n",
    "#---------------------------\n",
    "# quant_type = \"IQ3_XXS\"\n",
    "# quant_type = \"IQ3_KT\"\n",
    "# quant_type = \"exl3-3.0bpw\"\n",
    "\n",
    "#---------------------------\n",
    "# quant_type = \"Q4_0\"\n",
    "# quant_type = \"IQ4_XS\"\n",
    "# quant_type = \"Q4_K_M\"\n",
    "\n",
    "#---------------------------\n",
    "# quant_type += \"-IQ2_KS\"\n",
    "# quant_type += \"-dequant\"\n",
    "\n",
    "model = f\"models/{model_name}-{quant_type}.gguf\"\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the quant model\n",
    "# quant_option += \" --pure\"\n",
    "# use cpu_build is also ok because only cpu is needed\n",
    "assert os.path.exists(bf16_model)\n",
    "# we download unsloth provided quanted model instead\n",
    "assert quant_type not in [\"IQ2_XXS\", \"IQ2_M\"]\n",
    "\n",
    "# imatrix_file = \"\"\n",
    "imatrix_file = f\"--imatrix models/imatrix_unsloth-{model_name}.dat\"\n",
    "assert os.path.exists(f\"models/imatrix_unsloth-{model_name}.dat\")\n",
    "# if quant old type, llama.cpp is prefered, instead of ik_\n",
    "# !llama.cpp/cuda_build/bin/llama-quantize {quant_option} {imatrix_file} {bf16_model} {model} {quant_type}\n",
    "!ik_llama.cpp/cuda_build/bin/llama-quantize {quant_option} {imatrix_file} {bf16_model} {model} {quant_type}\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use gguf_dump.py to check quant_model\n",
    "# the gguf_dump.py file is in (ik_)llama.cpp/scripts folder\n",
    "assert os.path.exists(model)\n",
    "gguf_dump_tool = \"llama.cpp/gguf-py/gguf/scripts/gguf_dump.py\"\n",
    "!python {gguf_dump_tool} --markdown {model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>perplexity results</summary>\n",
    "\n",
    "```bash\n",
    "wc -c < models/Llama-3.1-8B-Instruct-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-410.98*2)/(14134.97-410.98*2)*16}'\n",
    "wc -c < models/Qwen3-8B-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-486.86*2)/(14222.89-486.86*2)*16}'\n",
    "wc -c < models/gemma-3-12b-it-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-787.69)/(21310.61-787.69)*16}'\n",
    "\n",
    "wc -c < models/Llama-3_3-Nemotron-Super-49B-v1_5-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-821.953*2)/(92751.97-821.953*2)*16}'\n",
    "wc -c < models/Llama-3.3-70B-Instruct-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-821.95*2)/(132208.94-821.95*2)*16}'\n",
    "wc -c < models/Qwen3-14B-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-608.57*2)/(26418.76-608.57*2)*16}'\n",
    "wc -c < models/Qwen3-32B-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-608.57*2)/(60739.72-608.57*2)*16}'\n",
    "wc -c < models/gemma-3-27b-it-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-1102.77)/(49932.94-1102.77)*16}'\n",
    "wc -c < models/Phi-4-reasoning-plus-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-401.95*2)/(26805.49-401.95*2)*16}'\n",
    "wc -c < models/phi-4-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-401.95*2)/(26805.49-401.95*2)*16}'\n",
    "wc -c < models/Mistral-Small-3.2-24B-Instruct-2506-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-525.00*2)/(43451.58-525.00*2)*16}'\n",
    "wc -c < models/Magistral-Small-2509-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-525.00*2)/(43451.58-525.00*2)*16}'\n",
    "wc -c < models/GLM-Z1-32B-0414-IQ2_K.gguf | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-728.438*2)/(60022.62-728.438*2)*16}'\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "quant     | Lm70B  |DsLm70B| bpw| Lm49B  | bpw| Qw32B  | bpw| Gm27B  | bpw| Mis24B | Mag24B | bpw| Qw14B  | bpw| Phi14BR| bpw| phi14B | bpw\n",
    "---       | ---    |---    | ---| ---    | ---| ---    | ---| ---    | ---| ---    | ---    | ---| ---    | ---| ---    | ---| ---    | ---\n",
    "BF16      | 3.9195 | 6.4641| 16 | 8.9911 | 16 | 7.8999 | 16 | 8.4254 | 16 | 5.3768 | 5.5103 | 16 | 9.0137 | 16 | 7.2447 | 16 | 6.5365 | 16 \n",
    "|         |**E6H6**|       |    |**E6H6**|    |**E6H6**|    |**E6H6**|    |**E6H6**|        |    |**E6H6**|    |**E6H6**|    |**E6H6**|    \n",
    "IQ2_M     |        |       |    |        |    |        |    |        |    |        |        |    | 10.0184|2.75|  9.2491|2.70| 7.9992 |2.66\n",
    "IQ2_K     |        |       |    | 11.7787|2.45|  9.3806|2.49|  9.7112|2.49| 11.9114| 10.5590|2.44| 10.6293|2.49|  9.3127|2.46| 8.0229 |2.46\n",
    "IQ2_KT    | 6.6194 | 8.6518|2.30| 12.4296|2.29|  9.7530|2.31| 10.1012|2.28| 14.1785| 12.9880|2.25| 10.9093|2.32|  9.0545|2.42| 7.8496 |2.42\n",
    "exl3-2.25 | 6.1918 | 8.3365|2.25|  9.3429|2.25|  8.9215|2.25|  9.6186|2.25|  6.6008|  6.6564|2.25|        |    |        |    |        |    \n",
    "exl3-2.0  | 6.6928 | 8.9645|2.00|  9.8139|2.00|  9.2934|2.00| 10.8345|2.00|  6.8854|  7.0594|2.00|        |    |        |    |        |    \n",
    "|         |**E6H6**|       |    |**E6H6**|    |**E6H6**|    |**E6H6**|    |**E6H6**|        |    |**E6H6**|    |**E6H6**|    |**E6H6**|    \n",
    "Q4_K_M    |*4.0955*| 6.5746|4.79|*8.1214*|4.81| 7.9957 |4.79|*8.4811*|4.81|*5.4899*| 5.6145 |4.82| 9.2685 |4.79|*7.3020*|4.78| 6.6033 |4.89\n",
    "Q4_0      | 4.2788 | 6.6847|4.52|  8.3147|4.52| 8.0018 |4.52| 8.5661 |4.52| 5.5677 | 5.6938 |4.52| 9.1585 |4.52| 7.4037 |4.52| 6.6366 |4.51\n",
    "IQ4_XS    | 4.1613 | 6.7978|4.26|  8.1394|4.22| 7.9858 |4.26| 8.4654 |4.25| 5.5227 | 5.6457 |4.26| 9.1024 |4.27| 7.3601 |4.25| 6.6109 |4.25\n",
    "|           |**E6H6**|        | |        |    |**E6H6**|    |**E6H6**|    |**E6H6**|        |    |**E6H6**|    |**E6H6**|    |**E6H6**|     \n",
    "IQ2_M_KS    |        |        | |        |    |        |    |        |    |        |        |    | 9.0882 |    | 7.3299 |    | 6.6271 |    \n",
    "IQ2_K_KS    |        |        | |*8.2164*|    | 7.9724 |    | 8.4961 |    | 5.5148 | 5.6314 |    | 9.1922 |    | 7.3707 |    | 6.6193 |    \n",
    "IQ2_KT_KS   | 4.2772 | 6.7511 | | 12.0403|    | 8.0623 |    | 8.5156 |    | 5.5677 | 5.6964 |    | 9.2404 |    | 7.3630 |    | 6.6411 |\n",
    "exl3-2.25_KS| 4.1763 | 6.6362 | | 13.5946|    | 8.0491 |    | 8.5495 |    | 5.5165 | 5.6529 |    |        |    |        |    |        |    \n",
    "exl3-2.0_KS | 4.2786 | 6.8787 | | 12.5879|    | 8.0846 |    | 8.6236 |    | 5.5777 | 5.6954 |    |        |    |        |    |        |    \n",
    "\n",
    "> official first, then self-generate with unsloth imatrix dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# align with https://github.com/ikawrakow/ik_llama.cpp/discussions/63\n",
    "assert os.path.exists(model)\n",
    "# phi-4 need flash attention, therefore, we enable -fa flag\n",
    "!llama.cpp/cuda_build/bin/llama-perplexity -m {model} -f wikitext-2-raw/wiki.test.raw -t 1 -ngl 100 -fa #--chunks 8 -b 8 -c 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(model)\n",
    "!llama.cpp/cuda_build/bin/llama-bench -m {model} -p 512 -n 128 -t 1 -ngl 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(model)\n",
    "!llama.cpp/cpu_build/bin/llama-bench -m {model} -p 4,8,32,128,512 -n 128 -t 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## exllamav3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/turboderp-org/exllamav3 --depth 1\n",
    "assert os.path.exists(\"exllamav3\")\n",
    "%cd exllamav3\n",
    "!pip install -r requirements.txt\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bpw = 2.25\n",
    "quant_type = f\"exl3-{str(bpw)}bpw\"\n",
    "model = f\"models/{model_name}-{quant_type}\"\n",
    "# https://github.com/turboderp-org/exllamav3/blob/master/exllamav3/conversion/convert_model.py#L31\n",
    "%cd exllamav3\n",
    "!python convert.py -i {\"../\"+hf_model} -o {\"../\"+model} -w f\"/tmp/{model_name}-{quant_type}\" -b {str(bpw)} #-hb 4\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## safetensors\n",
    "\n",
    "<details>\n",
    "<summary>modification for llama and mistral series</summary>\n",
    "\n",
    "```diff\n",
    "diff --git a/exllamav3/util/rope.py b/exllamav3/util/rope.py\n",
    "index 38cf670..0668269 100644\n",
    "--- a/exllamav3/util/rope.py\n",
    "+++ b/exllamav3/util/rope.py\n",
    "@@ -307,6 +307,8 @@ class RoPE:\n",
    "         norm_constant_bias: float = 0.0,\n",
    "         inv_freq: torch.Tensor | None = None\n",
    "     ):\n",
    "+        q = q.reshape(*q.shape[:-1], -1, 2).transpose(-1, -2).flatten(-2)\n",
    "+        k = k.reshape(*k.shape[:-1], -1, 2).transpose(-1, -2).flatten(-2)\n",
    "         q = q.contiguous()\n",
    "         if k is not None: k = k.contiguous()\n",
    "         if positions is not None: positions = positions.contiguous()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# only llama and mistral series need to do this\n",
    "def inverse_permute(w, name, num_heads, num_kv_heads):\n",
    "    if 'q_proj' in name:\n",
    "        dim3 = num_heads\n",
    "    elif 'k_proj' in name:\n",
    "        dim3 = num_kv_heads\n",
    "    else:\n",
    "        return w\n",
    "    dim1, dim2 = w.shape\n",
    "    return w.view(dim3, 2, dim1 // dim3 // 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n",
    "\n",
    "with open(f\"{hf_model}/config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "test_path = Path(hf_model)\n",
    "safetensors_files = sorted([str(file) for file in list(test_path.glob(\"*.safetensors\"))])\n",
    "\n",
    "for model_path in safetensors_files:\n",
    "    with safe_open(model_path, framework=\"pt\") as f:\n",
    "        tensor_names = list(f.keys())\n",
    "        tensors_dict = {}\n",
    "        for name in tensor_names:\n",
    "            tensor = f.get_tensor(name)\n",
    "            if 'text_config' in config:\n",
    "                tensors_dict[name] = inverse_permute(tensor, name,\n",
    "                    config['text_config']['num_attention_heads'], config['text_config']['num_key_value_heads'])\n",
    "            else:\n",
    "                tensors_dict[name] = inverse_permute(tensor, name, config['num_attention_heads'], config['num_key_value_heads'])\n",
    "    save_file(tensors_dict, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "model_path = f\"{model}/model.safetensors\"\n",
    "# model_path = 'models/Qwen3-8B-exl3-2.5bpw/model.safetensors'\n",
    "\n",
    "with safe_open(model_path, framework=\"pt\") as f:\n",
    "    tensor_names = f.keys()\n",
    "    byte_cnt = 0\n",
    "    for name in tensor_names:\n",
    "        if \"model.layers\" in name:\n",
    "            tensor = f.get_tensor(name)\n",
    "            byte_cnt += 2*tensor.numel()\n",
    "byte_cnt/1024.0/1024.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "model_path = f\"{model}/model.safetensors\"\n",
    "\n",
    "with safe_open(model_path, framework=\"pt\") as f:\n",
    "    tensor_names = f.keys()\n",
    "    for name in tensor_names:\n",
    "        tensor = f.get_tensor(name)\n",
    "        print(f\"name: {name}\")\n",
    "        print(f\"shape: {tensor.shape}\")\n",
    "        print(f\"type: {tensor.dtype}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## residual/speculative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft_type = \"exl3-2.0bpw\"\n",
    "# draft_type = \"exl3-2.25bpw\"\n",
    "# draft_type = \"IQ2_KT\"\n",
    "draft_type = \"IQ2_K\"\n",
    "# draft_type = \"IQ2_M\"\n",
    "\n",
    "draft_model_prefix = f\"models/{model_name}-{draft_type}\"\n",
    "model = draft_model_prefix + \"-residual.gguf\"\n",
    "residual_model = draft_model_prefix + \"-residual-IQ2_KS.gguf\"\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Given residual weight corresponding to draft_type to quant it\n",
    "assert os.path.exists(model)\n",
    "# imatrix_file = \"\"\n",
    "imatrix_file = f\"--imatrix models/imatrix_unsloth-{model_name}.dat\"\n",
    "assert os.path.exists(f\"models/imatrix_unsloth-{model_name}.dat\")\n",
    "!ik_llama.cpp/cuda_build/bin/llama-quantize {imatrix_file} --pure {model} {residual_model} IQ2_KS\n",
    "!rm {model}\n",
    "residual_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_model = draft_model_prefix + \"-IQ2_KS.gguf\"\n",
    "imatrix_file = draft_model_prefix + \"-IQ2_KS.imatrix\"\n",
    "\n",
    "calib_file = f\"calibration_data/calibration_datav3.txt\"\n",
    "# modified ik_llama.cpp imatrix to dump activations\n",
    "# Attention: the corresponding load_gguf_imatrix func in playgguf.ipynb requires 32-bit int range,\n",
    "# therefore, the dumped activations imatrix file should smaller than 2GB\n",
    "# therefore, the batch size should be tried to fit the above constraints\n",
    "!ik_llama.cpp/cuda_build/bin/llama-imatrix -m {target_model} -f {calib_file} --output-file {imatrix_file} -ngl 100 -b 256 -c 256 --chunks 32 #-t 1\n",
    "target_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Mistral-Small-3.2-24B-Instruct-2506 prompt example</summary>\n",
    "\n",
    "```\n",
    "[SYSTEM_PROMPT]You are Mistral-Small-3.2-24B-Instruct-2506, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\n",
    "You power an AI assistant called Le Chat.\n",
    "Your knowledge base was last updated on 2023-10-01.\n",
    "The current date is 2025-10-01.\n",
    "\n",
    "When you're not sure about some information or when the user's request requires up-to-date or specific data, you must use the available tools to fetch the information. Do not hesitate to use tools whenever they can provide a more accurate or complete response. If no relevant tools are available, then clearly state that you don't have the information and avoid making up anything.\n",
    "If the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \"What are some good restaurants around me?\" => \"Where are you?\" or \"When is the next flight to Tokyo\" => \"Where do you travel from?\").\n",
    "You are always very attentive to dates, in particular you try to resolve dates (e.g. \"yesterday\" is 2023-09-30) and when asked about information at specific dates, you discard information that is at another date.\n",
    "You follow these instructions in all languages, and always respond to the user in the language they use or request.\n",
    "Next sections describe the capabilities that you have.\n",
    "\n",
    "# WEB BROWSING INSTRUCTIONS\n",
    "\n",
    "You cannot perform any web search or access internet to open URLs, links etc. If it seems like the user is expecting you to do so, you clarify the situation and ask the user to copy paste the text directly in the chat.\n",
    "\n",
    "# MULTI-MODAL INSTRUCTIONS\n",
    "\n",
    "You have the ability to read images, but you cannot generate images. You also cannot transcribe audio files or videos.\n",
    "You cannot read nor transcribe audio files or videos.\n",
    "\n",
    "# TOOL CALLING INSTRUCTIONS\n",
    "\n",
    "You may have access to tools that you can use to fetch information or perform actions. You must use these tools in the following situations:\n",
    "\n",
    "1. When the request requires up-to-date information.\n",
    "2. When the request requires specific data that you do not have in your knowledge base.\n",
    "3. When the request involves actions that you cannot perform without tools.\n",
    "\n",
    "Always prioritize using tools to provide the most accurate and helpful response. If tools are not available, inform the user that you cannot perform the requested action at the moment.[/SYSTEM_PROMPT][INST]Write 20 sentences about summer.[/INST]\n",
    "\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Magistral-Small-2509  prompt example</summary>\n",
    "\n",
    "```\n",
    "[SYSTEM_PROMPT]First draft your thinking process (inner monologue) until you arrive at a response. Format your response using Markdown, and use LaTeX for any mathematical equations. Write both your thoughts and the response in the same language as the input.\n",
    "\n",
    "Your thinking process must follow the template below:[THINK]Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate the response. Use the same language as the input.[/THINK]Here, provide a self-contained response.[/SYSTEM_PROMPT][INST]Write 20 sentences about summer.[/INST]\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_model = draft_model_prefix + \".gguf\"\n",
    "if not os.path.exists(target_model):\n",
    "    target_model = draft_model_prefix + \"-dequant.gguf\"\n",
    "draft_model  = draft_model_prefix + \"-residual-IQ2_KS.gguf\"\n",
    "\n",
    "if \"Qwen3\" in model_name:\n",
    "    prefix = \"<|im_start|>user\\n\"\n",
    "    suffix = \" /no_think<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    # suffix = \" /think<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "elif \"gemma-3\" in model_name:\n",
    "    prefix = \"<start_of_turn>user\\n\\n\"\n",
    "    suffix = \"<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "elif \"Phi-4\" in model_name:\n",
    "    # https://huggingface.co/microsoft/Phi-4-reasoning-plus\n",
    "    prefix = \"<|im_start|>system<|im_sep|>\\nYou are Phi, a language model trained by Microsoft to help users. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format:<think>{Thought section}</think>{Solution section}. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion. Now, try to solve the following question through the above guidelines:<|im_end|>\\n<|im_start|>user<|im_sep|>\\n\"\n",
    "    suffix = \"<|im_end|>\\n<|im_start|>assistant<|im_sep|>\\n\"\n",
    "elif \"phi-4\" in model_name:\n",
    "    # https://huggingface.co/microsoft/phi-4\n",
    "    prefix = \"<|im_start|>user<|im_sep|>\\n\"\n",
    "    suffix = \"<|im_end|>\\n<|im_start|>assistant<|im_sep|>\\n\"\n",
    "elif \"DeepSeek-R1-Distill\" in model_name:\n",
    "    prefix = \"<｜User｜>\"\n",
    "    suffix = \"<｜Assistant｜>\"\n",
    "elif \"GLM\" in model_name:\n",
    "    prefix = \"[gMASK]<sop><|system|>\\nYou are an AI assistant named ChatGLM. You are developed based on the GLM-4 language model trained by Zhipu AI, and your task is to provide appropriate responses and support for users' questions and requests.<|user|>\"\n",
    "    suffix = \"<|assistant|>\\n<think>\\n\"\n",
    "else: # Llama-3\n",
    "    prefix = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    suffix = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "# Mistral's prompt is too long, thus use file `-f`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LLama-70B | #Q1 | #Q2 | #Q3 | #Q4 | self-spec | #Q1 | #Q2 | #Q3 | #Q4\n",
    "--- | --- |--- | --- | --- | --- | --- |--- | --- | --- \n",
    "exl3-2.0+IQ2_KS  |         |         |         |         | | 81.190% | 83.333% | 78.947% | 92.520%\n",
    "exl3-2.25+IQ2_KS |         |         |         |         | | 75.652% | 77.049% | 84.856% | 93.595%\n",
    "IQ2_KT+IQ2_KS    |         |         |         |         | | 80.357% | 80.319% | 69.382% | 92.453%\n",
    "|\n",
    "1BUD4+IQ2_K+IQ2_KS| 64.007% | 61.508% | 40.652% | 97.222%\n",
    "\n",
    "\n",
    "DsLLama-70B| #Q1 | #Q2 | #Q3 | #Q4 | self-spec | #Q1 | #Q2 | #Q3 | #Q4\n",
    "--- | --- |--- | --- | --- | --- | --- |--- | --- | --- \n",
    "exl3-2.0+IQ2_KS  |         |         |         |         | | 72.656% | 71.094% | 82.080% | 86.111%\n",
    "exl3-2.25+IQ2_KS |         |         |         |         | | 73.101% | 74.224% | 77.826% | 85.845%\n",
    "IQ2_KT+IQ2_KS    |         |         |         |         | | 71.887% | 69.427% | 77.232% | 74.516%\n",
    "\n",
    "> LLama-49B failed with greedy decode\n",
    "\n",
    "Qw32B | #Q1 | #Q2 | #Q3 | #Q4 | **think** | #Q1 | #Q2 | #Q3 | #Q4\n",
    "--- | --- | --- |--- | --- | --- | --- | --- |--- | ---\n",
    "exl3-2.0+IQ2_KS     | 70.062% | 63.816% | 79.167% | 81.311% | | 62.279% | 73.799% | 70.989% | 70.616%\n",
    "exl3-2.25+IQ2_KS    | 71.067% | 75.781% | 78.540% | 81.010% | | 66.540% | 77.752% | 82.218% | 70.446%\n",
    "IQ2_KT+IQ2_KS       | 75.758% | 65.000% | 73.047% | 79.167% | | 67.115% | 71.795% | 74.516% | 69.301%\n",
    "IQ2_K+IQ2_KS        | 65.051% | 79.000% | 70.640% | 83.777% | | 66.459% | 75.508% | 77.333% | 69.649%\n",
    "DeepSeek-R1-Distill |         |         |         |         | | 73.636% | 78.398% | 79.268% | 85.086%\n",
    "|\n",
    "0.6BUD6+IQ2_M+IQ2_KS| 32.500% | 34.211% | 36.607% | 59.884% | | 33.689% | 36.534% | 42.102% | 41.173%\n",
    "1.7BUD4+IQ2_M+IQ2_KS| 69.612% | 46.795% | 46.615% | 69.612% | | 38.787% | 45.099% | 44.891% | 47.663%\n",
    "\n",
    "<details>\n",
    "<summary>original</summary>\n",
    "\n",
    "Qw32B | #Q1 | #Q2 | #Q3 | #Q4 | **think** | #Q1 | #Q2 | #Q3 | #Q4\n",
    "--- | --- | --- |--- | --- | --- | --- | --- |--- | ---\n",
    "IQ2_KT+IQ2_KS     | 63.081% | 64.535% | 70.175% | 77.320% | | 59.979% | 67.224% | 65.813% | 57.556%\n",
    "IQ2_K+IQ2_KS      | 62.240% | 68.056% | 66.106% | 80.469% | | 55.227% | 65.724% | 67.599% | 59.016%\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "Gm27B | #Q1 | #Q2 | #Q3 | #Q4 | self-spec | #Q1 | #Q2 | #Q3 | #Q4\n",
    "--- | --- | --- |--- | --- | --- | --- | --- |--- | ---\n",
    "exl3-2.0+IQ2_KS     |         |         |         |         | | 66.509% | 74.462% | 71.223% | 84.362%\n",
    "exl3-2.25+IQ2_KS    |         |         |         |         | | 70.495% | 72.152% | 77.551% | 85.776%\n",
    "IQ2_KT+IQ2_KS       | 65.094% | 55.275% | 68.990% | 82.668% | | 63.551% | 68.235% | 77.604% | 88.717%\n",
    "IQ2_K+IQ2_KS        | 61.875% | 57.609% | 64.423% | 81.771% | | 73.762% | 67.614% | 67.188% | 86.413%\n",
    "|\n",
    "4BUD+IQ2_M+IQ2_KS   | 49.603% | 47.000% | 45.859% | 80.556%\n",
    "1BUD+IQ2_M+IQ2_KS   | 45.149% | 36.224% | 35.789% | 73.654%\n",
    "\n",
    "Mis24B | #Q1 | #Q2 | #Q3 | #Q4 | self-spec | #Q1 | #Q2 | #Q3 | #Q4\n",
    "--- | --- | --- |--- | --- | --- | --- | --- |--- | ---\n",
    "exl3-2.0+IQ2_KS     |         |         |         |         | | 68.519% | 72.059% | 75.000% | 91.265%\n",
    "exl3-2.25+IQ2_KS    |         |         |         |         | | 72.603% | 75.000% | 78.289% | 92.262%\n",
    "IQ2_KT+IQ2_KS       | 61.446% | 59.091% | 55.747% | 83.911% | | 69.408% | 63.235% | 73.750% | 91.755%\n",
    "IQ2_K+IQ2_KS        | 61.765% | 51.724% | 62.153% | 83.158% | | 75.938% | 88.333% | 69.366% | 86.340%\n",
    "\n",
    "Mag24B | #Q1 | #Q2 | #Q3 | #Q4 | self-spec | #Q1 | #Q2 | #Q3 | #Q4\n",
    "--- | --- | --- |--- | --- | --- | --- | --- |--- | ---\n",
    "exl3-2.0+IQ2_KS     |         |         |         |         | | 81.325% | 74.550% | 77.872% | 84.255%\n",
    "exl3-2.25+IQ2_KS    |         |         |         |         | | 84.802% | 68.023% | 78.528% | 84.255%\n",
    "IQ2_KT+IQ2_KS       | 70.539% | 64.890% | 64.948% | 69.139% | | 78.313% | 64.236% | 75.293% | 85.086%\n",
    "IQ2_K+IQ2_KS        | 78.526% | 65.927% | 66.939% | 73.654% | | 87.731% | 72.527% | 79.370% | 80.556%\n",
    "\n",
    "Qw14B | #Q1 | #Q2 | #Q3 | #Q4 | think | #Q1 | #Q2 | #Q3 | #Q4\n",
    "--- | --- | --- |--- | --- | --- | --- | --- |--- | ---\n",
    "IQ2_KT+IQ2_KS       | 69.512% | 66.892% | 73.397% | 84.593% | | 65.857% | 75.704% | 74.131% | 79.796%\n",
    "IQ2_K+IQ2_KS        | 68.072% | 82.500% | 80.357% | 90.000% | | 69.581% | 68.562% | 74.805% | 79.898%\n",
    "DeepSeek-R1-Distill |         |         |         |         | | 67.688% | 73.732% | 84.848% | 73.180%\n",
    "UD-IQ2_M+IQ2_KS     | 69.767% | 84.167% | 85.417% | 88.043% | | 70.225% | 75.787% | 76.285% | 84.255%\n",
    "\n",
    "<details>\n",
    "<summary>original</summary>\n",
    "\n",
    "Qw14B | #Q1 | #Q2 | #Q3 | #Q4 | think | #Q1 | #Q2 | #Q3 | #Q4\n",
    "--- | --- | --- |--- | --- | --- | --- | --- |--- | ---\n",
    "IQ2_KT+IQ2_KS     | 58.889% | 47.857% | 62.360% | 86.765% | | 65.438% | 67.323% | 65.901% | 72.064%\n",
    "IQ2_K+IQ2_KS      | 59.267% | 69.565% | 70.066% | 89.412% | | 68.704% | 64.912% | 64.912% | 73.558%\n",
    "UD-IQ2_M+IQ2_KS   | 68.391% | 61.538% | 76.587% | 83.854% | | 61.529% | 72.064% | 67.025% | 79.268%\n",
    "\n",
    "</details>\n",
    "\n",
    "Phi14B | #Q1 | #Q2 | #Q3 | #Q4 | self-spec | #Q1 | #Q2 | #Q3 | #Q4\n",
    "--- | --- | --- |--- | --- | --- | --- | --- |--- | ---\n",
    "IQ2_KT+IQ2_KS   | 59.906% | 61.538% | 82.955% | 84.280% | | 72.500% | 61.250% | 92.647% | 91.250%\n",
    "IQ2_K+IQ2_KS    | 59.167% | 53.846% | 86.429% | 87.308% | | 68.333% | 67.391% | 89.286% | 91.929%\n",
    "UD-IQ2_M+IQ2_KS | 60.052% | 66.935% | 77.273% | 82.222% | | 75.000% | 76.852% | 82.031% | 90.833%\n",
    "\n",
    "> Phi14BR failed with greedy decode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = prefix+\"Write 20 sentences about summer.\"+suffix #Q1\n",
    "# prompt = prefix+\"Who was the first prime minister of Britain?\"+suffix #Q2\n",
    "# prompt = prefix+\"How many persons are needed to power a 800W toaster?\"+suffix #Q3\n",
    "# prompt = prefix+\"Write the Quicksort algorithm in TypeScript.\"+suffix #Q4\n",
    "\n",
    "# https://github.com/ggml-org/llama.cpp/discussions/10466#discussioncomment-11501175\n",
    "!llama.cpp/cuda_debug/bin/llama-speculative -m {target_model} -md {draft_model} \\\n",
    "-p \"{prompt}\" -c 2048 -cd 2048 -n 1024 --seed 42 --draft-max 4 --draft-min 4 \\\n",
    "--top-k 20 --temp 0.6 --top-p 0.95 --draft-p-min 0.0 -t 8 -fa --color -ngl 100 -ngld 100 #--sampling-seq k\n",
    "draft_model\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_model = draft_model_prefix + \"-IQ2_KS.gguf\"\n",
    "draft_model  = draft_model_prefix + \".gguf\"\n",
    "# target_model = f\"models/Llama-3.3-70B-Instruct-IQ2_K-IQ2_KS.gguf\"\n",
    "# draft_model  = f\"models/Llama-3.2-1B-Instruct-UD-Q4_K_XL.gguf\"\n",
    "\n",
    "if not os.path.exists(draft_model):\n",
    "    draft_model = draft_model_prefix + \"-dequant.gguf\"\n",
    "\n",
    "prompt = prefix+\"Write 20 sentences about summer.\"+suffix #Q1\n",
    "\n",
    "!llama.cpp/cuda_build/bin/llama-speculative-simple -m {target_model} -md {draft_model} \\\n",
    "-p \"{prompt}\" -c 2048 -cd 2048 -n 1024 --sampling-seq k --seed 42 --draft-max 4 --draft-min 4 \\\n",
    "--top-k 1 --draft-p-min 0.0 -t 8 -fa --color -ngl 100 -ngld 100\n",
    "draft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!llama.cpp/cuda_build/bin/llama-cli -m {target_model} -p \"{prompt}\" -c 2048 -n 512 --top-k 1 --seed 42 -ngl 100 -no-cnv 2>/dev/null\n",
    "target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!llama.cpp/cuda_build/bin/llama-cli -m {draft_model}  -p \"{prompt}\" -c 2048 -n 512 --top-k 1 --seed 42 -ngl 100 -no-cnv 2>/dev/null\n",
    "draft_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
