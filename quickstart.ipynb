{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for find out more details if needed\n",
    "%set_env LLAMA_LOG_VERBOSITY=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Quick build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build for cpu debug\n",
    "%cd llama.cpp\n",
    "!cmake -B cpu_debug -DGGML_CUDA=OFF -DGGML_RPC=OFF -DGGML_BLAS=OFF \\\n",
    "    -DGGML_SCHED_MAX_COPIES=1 -DLLAMA_CURL=OFF -DGGML_CPU_REPACK=OFF -DCMAKE_BUILD_TYPE=Debug\n",
    "!cmake --build cpu_debug --config Debug -j $(nproc)\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build for gpu debug\n",
    "%cd llama.cpp\n",
    "!cmake -B cuda_debug -DGGML_CUDA=ON -DGGML_RPC=OFF -DGGML_BLAS=OFF \\\n",
    "    -DGGML_SCHED_MAX_COPIES=1 -DLLAMA_CURL=OFF -DGGML_CPU_REPACK=OFF -DCMAKE_BUILD_TYPE=Debug\n",
    "!cmake --build cuda_debug --config Debug -j $(nproc)\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build for cpu execution\n",
    "%cd llama.cpp\n",
    "!cmake -B cpu_build -DGGML_CUDA=OFF -DGGML_RPC=OFF -DGGML_BLAS=OFF \\\n",
    "    -DGGML_SCHED_MAX_COPIES=1 -DLLAMA_CURL=OFF -DGGML_CPU_REPACK=OFF\n",
    "!cmake --build cpu_build --config Release -j $(nproc)\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build for gpu execution\n",
    "%cd llama.cpp\n",
    "!cmake -B cuda_build -DGGML_CUDA=ON -DGGML_RPC=OFF -DGGML_BLAS=OFF \\\n",
    "    -DGGML_SCHED_MAX_COPIES=1 -DLLAMA_CURL=OFF -DGGML_CPU_REPACK=OFF\n",
    "!cmake --build cuda_build --config Release -j $(nproc)\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ikawrakow/ik_llama.cpp --depth 1\n",
    "%cd ik_llama.cpp\n",
    "!cmake -B cuda_build -DGGML_CUDA=ON -DGGML_RPC=OFF -DGGML_BLAS=OFF -DGGML_SCHED_MAX_COPIES=1 -DLLAMA_CURL=OFF\n",
    "!cmake --build cuda_build --config Release -j $(nproc)\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 42 test\n",
    "\n",
    "- [TinyStories-656K](https://hf-mirror.com/mradermacher/TinyStories-656K-GGUF)\n",
    "\n",
    "<details>\n",
    "<summary>config</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"architectures\": [\n",
    "    \"LlamaForCausalLM\"\n",
    "  ],\n",
    "  \"attention_bias\": false,\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 1,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 128,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 384,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"mlp_bias\": false,\n",
    "  \"model_type\": \"llama\",\n",
    "  \"num_attention_heads\": 8,\n",
    "  \"num_hidden_layers\": 2,\n",
    "  \"num_key_value_heads\": 4,\n",
    "  \"pretraining_tp\": 1,\n",
    "  \"rms_norm_eps\": 1e-06,\n",
    "  \"rope_scaling\": null,\n",
    "  \"rope_theta\": 10000.0,\n",
    "  \"tie_word_embeddings\": true,\n",
    "  \"torch_dtype\": \"bfloat16\",\n",
    "  \"transformers_version\": \"4.41.2\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 2048\n",
    "}\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"models/TinyStories-656K.f16.gguf\"\n",
    "assert os.path.exists(model)\n",
    "!llama.cpp/cpu_build/bin/llama-cli -m {model} -p \"the answer to the ultimate question of life, the universe, and everything is 42. One day, they \" \\\n",
    "-t 1 -n 2 --temp 0 --top-k 0 --top-p 1.0 --min-p 0.0 2>/dev/null"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " the answer to the ultimate question of life, the universe, and everything is 42. One day, they saw a small "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hello world test\n",
    "\n",
    "- [SmolLM2-135M](https://hf-mirror.com/bartowski/SmolLM2-135M-Instruct-GGUF/tree/main)\n",
    "\n",
    "<details>\n",
    "<summary>Prompt format</summary>\n",
    "\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "# Q4_K_L == Q4_K_M, Q5_K_L == Q5_K_M\n",
    "model = \"models/SmolLM2-135M-Instruct-f16.gguf\"\n",
    "assert os.path.exists(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"Give the most simple C program to print 'hello world!'\" # Q4_K_S or under will be failed\n",
    "prompt = prefix + question + suffix\n",
    "!llama.cpp/cpu_build/bin/llama-cli -m {model} -p \"{prompt}\" -no-cnv -t 1 --temp 0 --top-k 0 --top-p 1.0 --min-p 0.0 2>/dev/null"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    printf(\"Hello, World!\\n\");\n",
    "    return 0;\n",
    "} [end of text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give the most simple C program to print 'Love world!'\" # not work for Q8_0/Q6_K, Q5_K_S is the lowest, Q4 or under will be failed\n",
    "# question = \"Give the most simple C program to print 'love world!'\" # Q4 or under will be failed\n",
    "prompt = prefix + question + suffix\n",
    "!llama.cpp/cpu_build/bin/llama-cli -m {model} -p \"{prompt}\" -no-cnv -t 1 --temp 0 --top-k 0 --top-p 1.0 --min-p 0.0 2>/dev/null"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    printf(\"Love world!\\n\");\n",
    "    return 0;\n",
    "} [end of text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Manga auther test\n",
    "\n",
    "- [Llama-3.2-1B](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF/tree/main)\n",
    "- [Official text_prompt_format](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md)\n",
    "\n",
    "<details>\n",
    "<summary>Prompt format</summary>\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 26 Jul 2024\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Q4_K_M/S 展现出了随着prefix和suffix微小变化的不稳定性\n",
    "# prefix = \"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "# prefix = \"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Dec 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "# prefix = \"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Oct 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "# suffix = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # Q4_K_M only corrent for 火影忍者的作者是谁？ pair with Jul 2024\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4_0/awq will be failed\n",
    "question = \"Who is the author of 'Demon Slayer'?\"\n",
    "answer = \"The author of the popular manga and anime series 'Demon Slayer: Kimetsu no Yaiba' is Koyoharu Gotoge. [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awq will be failed\n",
    "question = \"Who is the author of 'Chainsaw Man'?\"\n",
    "answer = \"The author of the manga and anime series 'Chainsaw Man' is Tatsuki Fujimoto. [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [awq] will be failed\n",
    "question = \"Who is the author of 'Detective Conan'?\"\n",
    "answer = \"The author of the popular manga and anime series 'Detective Conan' is Gosho Aoyama. [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under-4-bit will be failed\n",
    "question = \"Who is the author of Manga 'Slam Dunk'?\"\n",
    "answer = 'The author of the popular manga series \"Slam Dunk\" is Takehiko Inoue. [end of text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under-4-bit will be failed\n",
    "question = \"Who is the author of 'Berserk'?\"\n",
    "answer = 'The author of the manga and anime series \"Berserk\" is Kentaro Miura. [end of text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Q5_K_M/Q5_K_S**/[Q4_K_M/Q4_K_S]/IQ4_XS will be failed\n",
    "question = \"Naruto的作者是谁？\"\n",
    "answer = \"Naruto的作者是Masashi Kishimoto [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Q4_K_M/Q4_K_S]/IQ4_XS/awq/gptq will be failed\n",
    "question = \"火影忍者的作者是谁？\"\n",
    "answer = \"火影忍者是由Masashi Kishimoto所创作的日本动画和漫画。 [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Aug 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "suffix = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "prompt = prefix + question + suffix\n",
    "!llama.cpp/cpu_build/bin/llama-cli -m {model} -p \"{prompt}\" -c 2048 -n 512 --temp 0 --top-k 1 --seed 42 -ngl 100 2>/dev/null\n",
    "print(\"\\n\"+answer)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Llama-3.2-1B-Instruct\"\n",
    "model_name = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "hf_model = f\"models/{model_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf16_model = f\"models/{model_name}-BF16.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_hf_to_gguf_tool = f\"llama.cpp/convert_hf_to_gguf.py\"\n",
    "!python {convert_hf_to_gguf_tool} --outtype bf16 {hf_model} --outfile {bf16_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the imatrix file\n",
    "calib_file = f\"calibration_data/calibration_datav3.txt\"\n",
    "imatrix_file = f\"models/{model_name}.imatrix\"\n",
    "# to use cuda backend, otherwise will be very very slow\n",
    "!ik_llama.cpp/cuda_build/bin/llama-imatrix -m {bf16_model} -f {calib_file} --output-file {imatrix_file} -ngl 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "quant_type = \"IQ2_K\"\n",
    "# quant_type = \"IQ2_KL\"\n",
    "# quant_type = \"IQ2_S\"\n",
    "# quant_type = \"IQ2_M\"\n",
    "# quant_type = \"IQ2_KT\"\n",
    "# quant_type = \"exl3-2.0bpw\"\n",
    "# quant_type = \"exl3-2.25bpw\"\n",
    "# quant_type = \"exl3-2.5bpw\"\n",
    "\n",
    "# quant_type = \"Q4_0\"\n",
    "# quant_type = \"IQ4_XS\"\n",
    "# quant_type = \"IQ3_XXS\"\n",
    "# quant_type = \"IQ3_KT\"\n",
    "# quant_type = \"exl3-3.0bpw\"\n",
    "\n",
    "# quant_type += \"-IQ2_KS\"\n",
    "# quant_type += \"-dequant\"\n",
    "\n",
    "model = f\"models/{model_name}-{quant_type}.gguf\"\n",
    "assert os.path.exists(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the quant model\n",
    "imatrix_file = f\"--imatrix models/{model_name}.imatrix\"\n",
    "# imatrix_file = \"\"\n",
    "\n",
    "quant_option = \"--token-embedding-type q6_K --output-tensor-type q4_K\"\n",
    "# quant_option += \" --pure\"\n",
    "\n",
    "# use cpu_build is ok because only cpu is needed\n",
    "!ik_llama.cpp/cuda_build/bin/llama-quantize {quant_option} {imatrix_file} {bf16_model} {model} {quant_type}\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use gguf_dump.py to check quant_model\n",
    "# the gguf_dump.py file is in (ik_)llama.cpp/scripts folder\n",
    "gguf_dump_tool = \"llama.cpp/gguf-py/gguf/scripts/gguf_dump.py\"\n",
    "!python {gguf_dump_tool} --markdown {model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>perplexity results</summary>\n",
    "\n",
    "Attention: use ik_llama.cpp for PPL testing with `--output-tensor-type q6_K --token-embedding-type q6_K`\n",
    "\n",
    "```bash\n",
    "# wc -c < models/Llama-3.2-1B-Instruct-Q2_K.gguf  | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-205.49)/1856.2*16}'\n",
    "# wc -c < models/Llama-3.2-3B-Instruct-Q2_K.gguf  | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-308.23)/5376.6*16}'\n",
    "wc -c < models/Meta-Llama-3.1-8B-Instruct-Q2_K.gguf  | awk '{printf \"%.2f bpw\\n\", ($1/1024/1024-7.5-410.98*2)/13313.0*16}'\n",
    "```\n",
    "\n",
    "8B | ppl | +- | bpw\n",
    "--- | --- | --- | ---\n",
    "BF16**  | 7.3221 | 0.04673 | 16\n",
    "Q8_0**  | 7.3311 | 0.04679 | 8.50\n",
    "Q4_0*   | 7.6185 | 7.7364 | 4.50\n",
    "Q4_K_M* | 7.5071 | 7.5322 | 4.80\n",
    "IQ4_XS  | 7.5242 | 0.04821 | 4.27\n",
    "IQ4_KS  | 7.4781 | 0.04777 | 4.28\n",
    "IQ4_KSS | 7.5934 | 0.04848 | 4.04\n",
    "IQ3_XXS | 8.4682 | 0.05315 | 3.07\n",
    "IQ3_KT  | 8.1155 | 0.05161 | 3.18\n",
    "exl3-3.0bpw | 7.7937 | 0.04986 | 3.01\n",
    "| | |\n",
    "IQ2_KL | 9.3848 | 0.06148 | 2.72\n",
    "IQ2_K | 10.1837 | 0.06689 | 2.50\n",
    "IQ2_M | 9.5655 | 0.06230 | 2.70\n",
    "IQ2_S | 10.5209 | 0.06981 | 2.48\n",
    "IQ2_KL+IQ2_KS | 7.5117 | \n",
    "IQ2_K+IQ2_KS | 7.5172 | 7.5225\n",
    "IQ2_M+IQ2_KS | 7.4951 | 0.04796\n",
    "IQ2_S+IQ2_KS | 7.5634 | 7.5791\n",
    "| | |\n",
    "exl3-2.0bpw | 10.1817 | 0.06631 | 2.01\n",
    "exl3-2.25bpw | 9.3610 | 0.06127 | 2.26\n",
    "exl3-2.5bpw | 8.6902 | 0.05687 | 2.51\n",
    "exl3-2.0bpw+IQ2_KS | 7.5922 | 0.04846\n",
    "exl3-2.25bpw+IQ2_KS | 7.5636 | 0.04823\n",
    "exl3-2.5bpw+IQ2_KS | 7.4845 | 0.04792\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>perplexity results (legacy)</summary>\n",
    "\n",
    "model | ppl | +- | bpw/size\n",
    "--- | --- | --- | ---\n",
    "[gptq-g32](https://huggingface.co/ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v2.5?show_file_info=model.safetensors) | 14.8567 | 0.11085 | 4@weight+(16@scale+4@zero)/32+32@g_dx/out_dim\n",
    "[gptq-g128](https://hf-mirror.com/shuyuej/Llama-3.2-1B-Instruct-GPTQ?show_file_info=model.safetensors) | 17.8053 | 0.13670 | 4@weight+(16@scale+4@zero)/128+32@g_dx/out_dim\n",
    "[awq](https://huggingface.co/AMead10/Llama-3.2-1B-Instruct-AWQ?show_file_info=model.safetensors)  | 15.2694 | 0.11399 | 4@weight+(16@scale+4@zero)/128\n",
    "[IQ4_XS](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF?show_file_info=Llama-3.2-1B-Instruct-IQ4_XS.gguf) | 14.6856 | 0.10988 | 4.25 / 709MB\n",
    "[Q4_0](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF?show_file_info=Llama-3.2-1B-Instruct-Q4_0.gguf) | 15.3289 | 0.11394 | 738MB\n",
    "[Q4_K_S](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF?show_file_info=Llama-3.2-1B-Instruct-Q4_K_S.gguf) | 14.5792 | 0.10848 | 740MB\n",
    "[Q5_K_S](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF?show_file_info=Llama-3.2-1B-Instruct-Q5_K_S.gguf) | 14.0893 | 0.10462 | 852MB\n",
    "[Q5_K_M](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF?show_file_info=Llama-3.2-1B-Instruct-Q5_K_.gguf) | 14.0558 | 0.10458 | 870MB\n",
    "llmc-awq-omniq | 14.0102 | 0.10419 | 4@weight+16@scale/128?\n",
    "[w4a16g128asym](https://hf-mirror.com/numen-tech/Llama-3.2-1B-Instruct-w4a16g128asym) | 19.5997 | 0.14797 | 4@weight+16@scale/128\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# align with https://github.com/ikawrakow/ik_llama.cpp/discussions/63\n",
    "!ik_llama.cpp/cuda_build/bin/llama-perplexity -m {model} -f wikitext-2-raw/wiki.test.raw -t 1 -ngl 100 #--chunks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!llama.cpp/cuda_build/bin/llama-bench -m {model} -p 512 -n 128 -t 1 -ngl 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!llama.cpp/cpu_build/bin/llama-bench -m {model} -p 4,8,32,128,512 -n 128 -t 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## exllamav3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/turboderp-org/exllamav3 --depth 1\n",
    "assert os.path.exists(exllamav3)\n",
    "%cd exllamav3\n",
    "!pip install -r requirements.txt\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd exllamav3\n",
    "bpw = 2.0\n",
    "quant_type = f\"exl3-{str(bpw)}bpw\"\n",
    "model = f\"models/{model_name}-{quant_type}\"\n",
    "!python convert.py -i {\"../\"+hf_model} -o {\"../\"+model} -w f\"/tmp/{model_name}-{quant_type}\" -b {str(bpw)}\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## dump information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "model_path = f\"{model}/model.safetensors\"\n",
    "\n",
    "with safe_open(model_path, framework=\"pt\") as f:\n",
    "    tensor_names = f.keys()\n",
    "    byte_cnt = 0\n",
    "    for name in tensor_names:\n",
    "        if \"model.layers\" in name:\n",
    "            tensor = f.get_tensor(name)\n",
    "            byte_cnt += 2*tensor.numel()\n",
    "byte_cnt/1024.0/1024.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "model_path = f\"{hf_model}/model.safetensors\"\n",
    "\n",
    "with safe_open(model_path, framework=\"pt\") as f:\n",
    "    tensor_names = f.keys()\n",
    "    for name in tensor_names:\n",
    "        tensor = f.get_tensor(name)\n",
    "        print(f\"name: {name}\")\n",
    "        print(f\"shape: {tensor.shape}\")\n",
    "        print(f\"type: {tensor.dtype}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with safe_open(model_path, framework=\"pt\") as f:\n",
    "  # tensor = f.get_tensor(\"model.norm.weight\") # 'output_norm.weight'\n",
    "  tensor = f.get_tensor(\"model.layers.0.input_layernorm.weight\")\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Consider residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_type = \"IQ2_K\"\n",
    "# draft_type = \"IQ2_KL\"\n",
    "# draft_type = \"IQ2_S\"\n",
    "# draft_type = \"IQ2_M\"\n",
    "# draft_type = \"exl3-2.0bpw\"\n",
    "# draft_type = \"exl3-2.25bpw\"\n",
    "# draft_type = \"exl3-2.5bpw\"\n",
    "draft_model_prefix = f\"models/{model_name}-{draft_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Given residual weight corresponding to draft_type to quant it\n",
    "imatrix_file = f\"--imatrix models/{model_name}.imatrix\"\n",
    "quant_option = \"--token-embedding-type q6_K\"\n",
    "residual_model = draft_model_prefix + \"-residual.gguf\"\n",
    "residual_qmodel = draft_model_prefix + \"-residual-IQ2_KS.gguf\"\n",
    "# we don't care output-tensor and token-embedding, just don't want to quant them exactly\n",
    "!ik_llama.cpp/cuda_build/bin/llama-quantize {imatrix_file} --pure {quant_option} {residual_model} {residual_qmodel} IQ2_KS\n",
    "residual_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>imatrix modification for dumping activations</summary>\n",
    "\n",
    "```diff\n",
    "diff --git a/examples/imatrix/imatrix.cpp b/examples/imatrix/imatrix.cpp\n",
    "index 2e03a4a0..2f10ac30 100644\n",
    "--- a/examples/imatrix/imatrix.cpp\n",
    "+++ b/examples/imatrix/imatrix.cpp\n",
    "@@ -39,6 +39,7 @@ static void print_usage(int argc, char ** argv, const gpt_params & params) {\n",
    " \n",
    " struct Stats {\n",
    "     std::vector<float> values;\n",
    "+    std::vector<float> activations;\n",
    "     std::vector<int> counts;\n",
    "     int ncall = 0;\n",
    "     int n_as = 1;\n",
    "@@ -341,6 +342,7 @@ bool IMatrixCollector::collect_imatrix(struct ggml_tensor * t, bool ask, void *\n",
    "             // Hence, the storage we need is src0->ne[0]*src0->ne[2].\n",
    "             e.values.resize(src0->ne[0]*src0->ne[2], 0);\n",
    "             e.counts.resize(src0->ne[0]*src0->ne[2], 0);\n",
    "+            e.activations.resize(src1->ne[0]*src1->ne[1]);\n",
    "         }\n",
    "         else if (e.values.size() != (size_t)(src0->ne[0]*src0->ne[2])) {\n",
    "             fprintf(stderr, \"Oops: inconsistent size for %s (%d vs %d)\\n\", wname.c_str(), (int)e.values.size(), (int)src1->ne[0]);\n",
    "@@ -357,8 +359,10 @@ bool IMatrixCollector::collect_imatrix(struct ggml_tensor * t, bool ask, void *\n",
    "             auto counts = e.counts.data() + i02*src0->ne[0];\n",
    "             for (int i11 = 0; i11 < (int)src1->ne[1]; ++i11) {\n",
    "                 const float * x = (const float *)((const char *)data + i11*src1->nb[1] + i12*src1->nb[2]);\n",
    "+                auto activations = e.activations.data() + i11*src1->ne[0];\n",
    "                 for (int j = 0; j < (int)src1->ne[0]; ++j) {\n",
    "                     values[j] += x[j]*x[j];\n",
    "+                    activations[j] = x[j]*x[j];\n",
    "                     counts[j]++;\n",
    "                     if (!std::isfinite(values[j])) {\n",
    "                         fprintf(stderr, \"%f detected in %s\\n\", e.values[j], wname.c_str());\n",
    "@@ -482,6 +486,9 @@ void IMatrixCollector::save_imatrix(int ncall) const {\n",
    "                 tmp[i] = (stat.values[i] / static_cast<float>(stat.counts[i])) * static_cast<float>(stat.ncall);\n",
    "             }\n",
    "             out.write((const char*)tmp.data(), nval*sizeof(float));\n",
    "+            int nact = stat.activations.size();\n",
    "+            out.write((const char *) &nact, sizeof(nact));\n",
    "+            out.write((const char*)stat.activations.data(), nact*sizeof(float));\n",
    "         }\n",
    "     }\n",
    " \n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_model = draft_model_prefix + \"-IQ2_KS.gguf\"\n",
    "imatrix_file = draft_model_prefix + \"-IQ2_KS.imatrix\"\n",
    "\n",
    "calib_file = f\"calibration_data/calibration_datav3.txt\"\n",
    "# modified ik_llama.cpp imatrix to dump activations\n",
    "# Attention: the corresponding load_gguf_imatrix func in playgguf.ipynb requires 32-bit int range,\n",
    "# therefore, the dumped activations imatrix file should smaller than 2GB\n",
    "# therefore, the batch size should be tried to fit the above constraints\n",
    "!ik_llama.cpp/cuda_build/bin/llama-imatrix -m {target_model} -f {calib_file} --output-file {imatrix_file} -ngl 100 -b 256 -c 256 --chunks 32 #-t 1\n",
    "target_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>1B, but we don't care so much</summary>\n",
    "\n",
    "1B | #Q1 | #Q2 | #Q3 | #Q4 |\n",
    "--- | --- | --- |--- | ---\n",
    "IQ2_KL+IQ2_KS | 60.307% | 48.333% | 61.364% | 75.000%\n",
    "IQ2_K+IQ2_KS | 52.560% | 39.919% | 57.558% | 74.242%\n",
    "IQ2_M+IQ2_KS | 66.786% | 43.929% | 66.026% | 62.500%\n",
    "IQ2_S+IQ2_KS | 52.841% | 34.375% | 52.632% | 57.895%\n",
    "exl3-2.5bpw+IQ2_KS | 62.597% | 50.000% | 69.952% | 77.941%\n",
    "exl3-2.25bpw+IQ2_KS | 56.818% | 48.120% | 65.278% | 61.111%\n",
    "exl3-2.0bpw+IQ2_KS | 49.144% | 51.230% | 85.991%  | 69.444%\n",
    "\n",
    "</details>\n",
    "\n",
    "> self speculative\n",
    "\n",
    "8B | #Q1 | #Q2 | #Q3 | #Q4 |\n",
    "--- | --- | --- |--- | ---\n",
    "IQ2_M+IQ2_KS | 73.288% | 73.881% | 72.917% | 92.593%\n",
    "IQ2_S+IQ2_KS | 69.505% | 58.562% | 76.339% | 92.593%\n",
    "IQ2_KL+IQ2_KS | \n",
    "IQ2_K+IQ2_KS | 68.000% | 70.902% | 80.882% | 75.000%\n",
    "exl3-2.5bpw+IQ2_KS | 74.671% | 74.643% | 52.679% | 75.000%\n",
    "exl3-2.25bpw+IQ2_KS | 73.739% | 61.397% | 66.667% | 79.688%\n",
    "exl3-2.0bpw+IQ2_KS | 53.313% | 55.340% | 65.789% | 79.167%\n",
    "\n",
    "> orignal\n",
    "\n",
    "8B | #Q1 | #Q2 | #Q3 | #Q4 |\n",
    "--- | --- | --- |--- | ---\n",
    "IQ2_M+IQ2_KS | 77.632%| 71.121% | 85.714% | 70.968%\n",
    "IQ2_S+IQ2_KS | 76.087% | 61.594% | 60.473% | 82.500%\n",
    "IQ2_KL+IQ2_KS | 74.786% | 80.500% | 72.093% | 91.667%\n",
    "IQ2_K+IQ2_KS | 63.261% | 62.805% | 63.362% | 78.333%\n",
    "exl3-2.5bpw+IQ2_KS | 74.671% | 74.643% | 52.679% | 75.000%\n",
    "exl3-2.25bpw+IQ2_KS | 73.739% | 61.397% | 66.667% | 79.688%\n",
    "exl3-2.0bpw+IQ2_KS | 53.313% | 55.340% | 65.789% | 79.167%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target_model = f\"models/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "# draft_model  = f\"models/Llama-3.2-1B-Instruct-Q8_0.gguf\"\n",
    "\n",
    "target_model = draft_model_prefix + \".gguf\"\n",
    "draft_model  = draft_model_prefix + \"-residual-IQ2_KS.gguf\"\n",
    "\n",
    "prefix = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "suffix = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "prompt = prefix+\"Write 20 sentences about summer.\"+suffix #Q1\n",
    "# prompt = prefix+\"Who was the first prime minister of Britain?\"+suffix #Q2\n",
    "# prompt = prefix+\"How many persons are needed to power a 800W toaster?\"+suffix #Q3\n",
    "# prompt = prefix+\"Write the Quicksort algorithm in TypeScript.\"+suffix #Q4\n",
    "\n",
    "# https://github.com/ggml-org/llama.cpp/discussions/10466#discussioncomment-11501175\n",
    "!llama.cpp/cpu_build/bin/llama-speculative -m {target_model} -md {draft_model} \\\n",
    "-p \"{prompt}\" -c 2048 -n 512 --temp 0 --top-k 1 --seed 42 --draft-max 4 --draft-min 0 --draft-p-min 0.0 -t 8\n",
    "draft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!llama.cpp/cuda_build/bin/llama-cli -m {target_model} -p \"{prompt}\" -c 2048 -n 512 --temp 0 --top-k 1 --seed 42 -ngl 100 -no-cnv 2>/dev/null\n",
    "target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!llama.cpp/cuda_build/bin/llama-cli -m {draft_model}  -p \"{prompt}\" -c 2048 -n 512 --temp 0 --top-k 1 --seed 42 -ngl 100 -no-cnv 2>/dev/null\n",
    "draft_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
