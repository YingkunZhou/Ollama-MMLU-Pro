{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env LLAMA_LOG_VERBOSITY=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 42 test\n",
    "\n",
    "- [TinyStories-656K](https://hf-mirror.com/mradermacher/TinyStories-656K-GGUF)\n",
    "\n",
    "<details>\n",
    "<summary>config</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"architectures\": [\n",
    "    \"LlamaForCausalLM\"\n",
    "  ],\n",
    "  \"attention_bias\": false,\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 1,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 128,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 384,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"mlp_bias\": false,\n",
    "  \"model_type\": \"llama\",\n",
    "  \"num_attention_heads\": 8,\n",
    "  \"num_hidden_layers\": 2,\n",
    "  \"num_key_value_heads\": 4,\n",
    "  \"pretraining_tp\": 1,\n",
    "  \"rms_norm_eps\": 1e-06,\n",
    "  \"rope_scaling\": null,\n",
    "  \"rope_theta\": 10000.0,\n",
    "  \"tie_word_embeddings\": true,\n",
    "  \"torch_dtype\": \"bfloat16\",\n",
    "  \"transformers_version\": \"4.41.2\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 2048\n",
    "}\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"models/TinyStories-656K.f16.gguf\"\n",
    "assert os.path.exists(model)\n",
    "!llama.cpp/bin/llama-cli -m {model} -p \"the answer to the ultimate question of life, the universe, and everything is 42. One day, they \" \\\n",
    "-t 1 -n 2 --temp 0 --top-k 0 --top-p 1.0 --min-p 0.0 2>/dev/null"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " the answer to the ultimate question of life, the universe, and everything is 42. One day, they saw a small "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hello world test\n",
    "\n",
    "- [SmolLM2-135M](https://hf-mirror.com/bartowski/SmolLM2-135M-Instruct-GGUF/tree/main)\n",
    "\n",
    "<details>\n",
    "<summary>Prompt format</summary>\n",
    "\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "# Q4_K_L == Q4_K_M, Q5_K_L == Q5_K_M\n",
    "model = \"models/SmolLM2-135M-Instruct-f16.gguf\"\n",
    "assert os.path.exists(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"Give the most simple C program to print 'hello world!'\" # Q4_K_S or under will be failed\n",
    "prompt = prefix + question + suffix\n",
    "!llama.cpp/bin/llama-cli -m {model} -p \"{prompt}\" -no-cnv -t 1 --temp 0 --top-k 0 --top-p 1.0 --min-p 0.0 2>/dev/null"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    printf(\"Hello, World!\\n\");\n",
    "    return 0;\n",
    "} [end of text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give the most simple C program to print 'Love world!'\" # not work for Q8_0/Q6_K, Q5_K_S is the lowest, Q4 or under will be failed\n",
    "# question = \"Give the most simple C program to print 'love world!'\" # Q4 or under will be failed\n",
    "prompt = prefix + question + suffix\n",
    "!llama.cpp/bin/llama-cli -m {model} -p \"{prompt}\" -no-cnv -t 1 --temp 0 --top-k 0 --top-p 1.0 --min-p 0.0 2>/dev/null"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    printf(\"Love world!\\n\");\n",
    "    return 0;\n",
    "} [end of text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Manga auther test\n",
    "\n",
    "- [Llama-3.2-1B](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF/tree/main)\n",
    "- [Official text_prompt_format](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md)\n",
    "\n",
    "<details>\n",
    "<summary>Prompt format</summary>\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 26 Jul 2024\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# model=\"Llama-3.2-1B-Instruct-AWQ.gguf\"\n",
    "# model = \"Llama-3.2-1B-Instruct-auto_awq-int4-gs128-asym.gguf\"\n",
    "# model = \"Llama-3.2-1B-Instruct-auto_awq-int4-gs128-sym.gguf\"\n",
    "# model = \"Llama-3.2-1B-Instruct-llmc-awq.gguf\"\n",
    "# model = \"Llama-3.2-1B-Instruct-IQ3_M.gguf\"\n",
    "# model = \"Llama-3.2-1B-Instruct-IQ3_M-cn.gguf\"\n",
    "\n",
    "\n",
    "# model = \"Llama-3.2-1B-Instruct-GPTQ-INT4.gguf\"\n",
    "# model=\"Llama-3.2-1B-Instruct-GPTQ-g128.gguf\" # 除了柯南，愣是一个问题都没有回答对\n",
    "# model=\"Llama-3.2-1B-Instruct-GPTQ-g32.gguf\"\n",
    "\n",
    "model=\"models/Llama-3.2-1B-Instruct-IQ2_XXS.gguf\"\n",
    "# model=\"Llama-3.2-1B-Instruct-IQ2_XXS.gguf\"\n",
    "### hugging-quants/Llama-3.2-1B-Instruct-Q4_K_M-GGUF\n",
    "# model=\"models/llama-3.2-1b-instruct-q4_k_m.gguf\" # 这个模型是没有imatrix的\n",
    "### SanctumAI/Llama-3.2-1B-Instruct-GGUF\n",
    "# model=\"models/llama-3.2-1b-instruct.Q5_K_M.gguf\" # 这个模型是没有imatrix的\n",
    "### second-state/Llama-3.2-1B-Instruct-GGUF 不带imatrix的这个也可以考虑\n",
    "\n",
    "# model = \"Llama-3.2-1B-Instruct-w4a16g128asym.gguf\"\n",
    "# model = \"Llama-3.2-1B-Instruct-llmc-awq-omniq.gguf\"\n",
    "# model = \"Llama-3.2-1B-Instruct-q4f16_1-MLC.gguf\"\n",
    "# model = \"Llama-3.2-1B-Instruct-llmc-hqq.gguf\"\n",
    "\n",
    "# model = \"Llama-3.2-1B-Instruct-nbits4-GSNone-Axis0-HQQ.gguf\"\n",
    "# model = \"Llama-3.2-1B-Instruct-nbits4-GS64-Axis1-HQQ.gguf\"\n",
    "\n",
    "assert os.path.exists(model)\n",
    "# Q4_K_M/S 展现出了随着prefix和suffix微小变化的不稳定性\n",
    "# prefix = \"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "# prefix = \"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Dec 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "# prefix = \"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Oct 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "prefix = \"<|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Aug 2024\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "# suffix = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" # Q4_K_M only corrent for 火影忍者的作者是谁？ pair with Jul 2024\n",
    "suffix = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" # Q4_K_M only corrent for 火影忍者的作者是谁？ pair with 26 Aug\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perplexity results\n",
    "\n",
    "model | ppl | +- | bpw/size\n",
    "--- | --- | --- | ---\n",
    "[gptq-g32](https://huggingface.co/ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v2.5?show_file_info=model.safetensors) | 14.8567 | 0.11085 | 4@weight+(16@scale+4@zero)/32+32@g_dx/out_dim\n",
    "[gptq-g128](https://hf-mirror.com/shuyuej/Llama-3.2-1B-Instruct-GPTQ?show_file_info=model.safetensors) | 17.8053 | 0.13670 | 4@weight+(16@scale+4@zero)/128+32@g_dx/out_dim\n",
    "[awq](https://huggingface.co/AMead10/Llama-3.2-1B-Instruct-AWQ?show_file_info=model.safetensors)  | 15.2694 | 0.11399 | 4@weight+(16@scale+4@zero)/128\n",
    "[IQ4_XS](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF?show_file_info=Llama-3.2-1B-Instruct-IQ4_XS.gguf) | 14.6856 | 0.10988 | 4.25 / 709MB\n",
    "[Q4_0](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF?show_file_info=Llama-3.2-1B-Instruct-Q4_0.gguf) | 15.3289 | 0.11394 | 738MB\n",
    "[Q4_K_S](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF?show_file_info=Llama-3.2-1B-Instruct-Q4_K_S.gguf) | 14.5792 | 0.10848 | 740MB\n",
    "[Q5_K_S](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF?show_file_info=Llama-3.2-1B-Instruct-Q5_K_S.gguf) | 14.0893 | 0.10462 | 852MB\n",
    "[Q5_K_M](https://hf-mirror.com/bartowski/Llama-3.2-1B-Instruct-GGUF?show_file_info=Llama-3.2-1B-Instruct-Q5_K_.gguf) | 14.0558 | 0.10458 | 870MB\n",
    "llmc-awq-omniq | 14.0102 | 0.10419 | 4@weight+16@scale/128?\n",
    "[w4a16g128asym](https://hf-mirror.com/numen-tech/Llama-3.2-1B-Instruct-w4a16g128asym) | 19.5997 | 0.14797 | 4@weight+16@scale/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!llama.cpp/bin/llama-perplexity -m {model} -f wikitext-2-raw/wiki.test.raw --n-gpu-layers 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Q5_K_M/Q5_K_S**/[Q4_K_M/Q4_K_S]/IQ4_XS will be failed\n",
    "question = \"Naruto的作者是谁？\"\n",
    "answer = \"Naruto的作者是Masashi Kishimoto [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Q4_K_M/Q4_K_S]/IQ4_XS/awq/gptq will be failed\n",
    "question = \"火影忍者的作者是谁？\"\n",
    "answer = \"火影忍者是由Masashi Kishimoto所创作的日本动画和漫画。 [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prefix + question + suffix\n",
    "!llama.cpp/bin/llama-cli -m {model} -p \"{prompt}\" -no-cnv -t 1 --temp 0 --top-k 0 --top-p 1.0 --min-p 0.0 2>/dev/null\n",
    "print(answer)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4_0/awq will be failed\n",
    "question = \"Who is the author of 'Demon Slayer'?\"\n",
    "answer = \"The author of the popular manga and anime series 'Demon Slayer: Kimetsu no Yaiba' is Koyoharu Gotoge. [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awq will be failed\n",
    "question = \"Who is the author of 'Chainsaw Man'?\"\n",
    "answer = \"The author of the manga and anime series 'Chainsaw Man' is Tatsuki Fujimoto. [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [awq] will be failed\n",
    "question = \"Who is the author of 'Detective Conan'?\"\n",
    "answer = \"The author of the popular manga and anime series 'Detective Conan' is Gosho Aoyama. [end of text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under-4-bit will be failed\n",
    "question = \"Who is the author of Manga 'Slam Dunk'?\"\n",
    "answer = 'The author of the popular manga series \"Slam Dunk\" is Takehiko Inoue. [end of text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under-4-bit will be failed\n",
    "question = \"Who is the author of 'Berserk'?\"\n",
    "answer = 'The author of the manga and anime series \"Berserk\" is Kentaro Miura. [end of text]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Original model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "model_path = \"model.safetensors\"\n",
    "\n",
    "with safe_open(model_path, framework=\"pt\") as f:\n",
    "    tensor_names = f.keys()\n",
    "    for name in tensor_names:\n",
    "        tensor = f.get_tensor(name)\n",
    "        print(f\"name: {name}\")\n",
    "        print(f\"shape: {tensor.shape}\")\n",
    "        print(f\"type: {tensor.dtype}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with safe_open(model_path, framework=\"pt\") as f:\n",
    "  # tensor = f.get_tensor(\"model.norm.weight\") # 'output_norm.weight'\n",
    "  tensor = f.get_tensor(\"model.layers.0.input_layernorm.weight\")\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# imatrix and quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_cn = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_cn = \"-cn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Llama-3.2-1B-Instruct\"\n",
    "unquant_model = f\"models/{model_name}-f16.gguf\"\n",
    "calib_file = f\"calibration_data/calibration_datav3{with_cn}.txt\"\n",
    "imatrix_file = f\"{model_name}{with_cn}.imatrix\"\n",
    "\n",
    "!llama.cpp/bin/llama-imatrix -m {unquant_model} -f {calib_file} --output-file {imatrix_file} #-t 1 --chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_level = \"IQ2_XXS\"\n",
    "quant_model = f\"{model_name}-{quant_level}{with_cn}.gguf\"\n",
    "!llama.cpp/bin/llama-quantize --imatrix {imatrix_file} {unquant_model} {quant_model} {quant_level}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
