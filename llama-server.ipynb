{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env LD_LIBRARY_PATH=llama.cpp/ggml/src:llama.cpp/src\n",
    "import threading\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Llama-3.2-1B-Instruct\"\n",
    "# model = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "# model = \"Qwen2.5-1.5B-Instruct\"\n",
    "# model = \"gemma-2-2b-it\"\n",
    "# model = \"Phi-3.5-mini-instruct\"\n",
    "# model = \"Phi-3-medium-128k-instruct\"\n",
    "# model = \"Mistral-Nemo-Instruct-2407\"\n",
    "# model = \"Mistral-Small-Instruct-2409\"\n",
    "# size = \"IQ4_XS\"\n",
    "# size = \"Q5_K_M\"\n",
    "size = \"Q8_0\"\n",
    "path = 'eval_results/' + model.replace('.', '-') + '-' + size + '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMLU-Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkill -9 llama-server\n",
    "subprocess.run(['pkill', '-9', 'llama-server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-server -m models/gemma2/gemma-2-2b-it-Q4_K_S.gguf --ctx-size 6144 --n-gpu-layers 99 --flash-attn --parallel 2\n",
    "## It seems that there is a noticeable benefit when the degree of parallelism is 2,\n",
    "## and there is also some benefit when it is 4,\n",
    "## but beyond 4, the benefits are not significant.\n",
    "slot_ctx = 3072\n",
    "parallel = 4\n",
    "ngl = '99'\n",
    "# the default host and port\n",
    "host = '127.0.0.1'\n",
    "port = '8080'\n",
    "\n",
    "server = threading.Thread(target=lambda: subprocess.run([\n",
    "    'llama.cpp/bin/llama-server',\n",
    "    '-m', \"models/\" + model + \"-\" + size + \".gguf\",\n",
    "    '--ctx-size', str(parallel * slot_ctx),\n",
    "    '--parallel', str(parallel),\n",
    "    '--host', host,\n",
    "    '--port', port,\n",
    "    '--n-gpu-layers', ngl,\n",
    "    '--flash-attn'], check=True), daemon=True)\n",
    "\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_openai.py --url http://localhost:8080/v1 --model gguf --dataset TIGER-Lab/MMLU-Pro --category 'computer science' --parallel 2 --style multi_chat_zeroshot\n",
    "# in 4 minutes 18 seconds for Llama-3.2-1B-Instruct-Q8_0 on RTX 3080\n",
    "style = 'multi_chat_zeroshot'\n",
    "category = 'computer science'\n",
    "subprocess.run([\n",
    "    'python', 'run_openai.py',\n",
    "    '--url', 'http://' + host + ':' + port + '/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'TIGER-Lab/MMLU-Pro',\n",
    "    '--parallel', str(parallel),\n",
    "    '--category', category,\n",
    "    '--style', style], check=True)\n",
    "\n",
    "subprocess.run(['mv', path + category + '_result.json', path + category + '_result.full.json'])\n",
    "subprocess.run(['mv', path + category + '_summary.json', path + category + '_summary.full.json'])\n",
    "subprocess.run(['mv', path + 'report.txt', path + 'cs-report.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_openai.py --url http://localhost:8080/v1 --model gguf --dataset TIGER-Lab/MMLU-Pro --category 'engineering' --parallel 2 --style multi_chat_zeroshot\n",
    "# in 21 minutes 43 seconds for Llama-3.2-1B-Instruct-Q8_0 on RTX 3080\n",
    "style = 'multi_chat_zeroshot'\n",
    "category = 'engineering'\n",
    "subprocess.run([\n",
    "    'python', 'run_openai.py',\n",
    "    '--url', 'http://' + host + ':' + port + '/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'TIGER-Lab/MMLU-Pro',\n",
    "    '--parallel', str(parallel),\n",
    "    '--category', category,\n",
    "    '--style', style], check=True)\n",
    "\n",
    "subprocess.run(['mv', path + category + '_result.json', path + category + '_result.full.json'])\n",
    "subprocess.run(['mv', path + category + '_summary.json', path + category + '_summary.full.json'])\n",
    "subprocess.run(['mv', path + 'report.txt', path + 'engi-report.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_openai.py --url http://localhost:8080/v1 --model gguf --dataset sam-paech/mmlu-pro-nomath-sml --parallel 2 --style multi_chat_zeroshot\n",
    "# in 16 minutes 52 seconds for Llama-3.2-1B-Instruct-Q8_0 on RTX 3080\n",
    "style = 'multi_chat_zeroshot'\n",
    "subprocess.run([\n",
    "    'python', 'run_openai.py',\n",
    "    '--url', 'http://' + host + ':' + port + '/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'sam-paech/mmlu-pro-nomath-sml',\n",
    "    '--parallel', str(parallel),\n",
    "    '--style', style], check=True)\n",
    "\n",
    "subprocess.run(['mv', path + 'report.txt', path + 'mmlu-pro-nomath-sml-report.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openai/MMMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkill -9 llama-server\n",
    "subprocess.run(['pkill', '-9', 'llama-server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_ctx = 2900\n",
    "parallel = 3\n",
    "ngl = '99'\n",
    "# the default host and port\n",
    "host = '127.0.0.1'\n",
    "port = '8080'\n",
    "\n",
    "server = threading.Thread(target=lambda: subprocess.run([\n",
    "    'llama.cpp/bin/llama-server',\n",
    "    '-m', \"models/\" + model + \"-\" + size + \".gguf\",\n",
    "    '--ctx-size', str(parallel * slot_ctx),\n",
    "    '--parallel', str(parallel),\n",
    "    '--host', host,\n",
    "    '--port', port,\n",
    "    '--n-gpu-layers', ngl,\n",
    "    '--flash-attn'], check=True), daemon=True)\n",
    "\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_openai-mmmlu.py --url http://localhost:8080/v1 --model gguf --dataset openai/MMMLU --category ES_LA\n",
    "mmmlu_es = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai-mmmlu.py',\n",
    "    '--url', 'http://' + host + ':' + port + '/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'openai/MMMLU',\n",
    "    '--category', 'ES_LA'], check=True), daemon=True)\n",
    "\n",
    "mmmlu_es.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_openai-mmmlu.py --url http://localhost:8080/v1 --model gguf --dataset openai/MMMLU --category FR_FR\n",
    "mmmlu_fr = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai-mmmlu.py',\n",
    "    '--url', 'http://' + host + ':' + port + '/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'openai/MMMLU',\n",
    "    '--category', 'FR_FR'], check=True), daemon=True)\n",
    "\n",
    "mmmlu_fr.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_openai-mmmlu.py --url http://localhost:8080/v1 --model gguf --dataset openai/MMMLU --category ZH_CN\n",
    "mmmlu_zh = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai-mmmlu.py',\n",
    "    '--url', 'http://' + host + ':' + port + '/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'openai/MMMLU',\n",
    "    '--category', 'ZH_CN'], check=True), daemon=True)\n",
    "\n",
    "mmmlu_zh.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in 9 minutes 13 seconds for Llama-3.2-1B-Instruct-Q8_0 on RTX 3080\n",
    "subprocess.run(['mv', path + 'report.txt', path + 'mmmlu-report.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### ATTENTION: NOT include in our customized benchmark for saving time\n",
    "# python run_openai-mmmlu.py --url http://localhost:8080/v1 --model gguf --dataset openai/MMMLU --category DE_DE\n",
    "mmmlu_de = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai-mmmlu.py',\n",
    "    '--url', 'http://' + host + ':' + port + '/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'openai/MMMLU',\n",
    "    '--category', 'DE_DE'], check=True), daemon=True)\n",
    "\n",
    "mmmlu_de.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### ATTENTION: NOT include in our customized benchmark for saving time\n",
    "# python run_openai-mmmlu.py --url http://localhost:8080/v1 --model gguf --dataset openai/MMMLU --category PT_BR\n",
    "mmmlu_pt = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai-mmmlu.py',\n",
    "    '--url', 'http://' + host + ':' + port + '/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'openai/MMMLU',\n",
    "    '--category', 'PT_BR'], check=True), daemon=True)\n",
    "\n",
    "mmmlu_pt.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkill -9 llama-server\n",
    "subprocess.run(['pkill', '-9', 'llama-server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about 2 minutes for Llama-3.2-1B-Instruct-Q8_0 on RTX 3080\n",
    "ngl = '99'\n",
    "with open(path + 'perplexity.log', 'w+') as outfile:\n",
    "    subprocess.run([\n",
    "    'llama.cpp/bin/llama-perplexity',\n",
    "    '-m', \"models/\" + model + \"-\" + size + \".gguf\",\n",
    "    '-f', 'wikitext-2-raw/wiki.test.raw',\n",
    "    '--n-gpu-layers', ngl], stdout = outfile, stderr=subprocess.STDOUT)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
