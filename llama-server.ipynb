{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env LD_LIBRARY_PATH=llama.cpp/ggml/src:llama.cpp/src\n",
    "import threading\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-server -m models/gemma2/gemma-2-2b-it-Q4_K_S.gguf --ctx-size 8192 --n-gpu-layers 99 --flash-attn --parallel 2\n",
    "\n",
    "series = \"qwen2.5\"\n",
    "model = \"Qwen2.5-32B-Instruct-IQ3_XS\"\n",
    "model_path = \"models/\" + series + \"/\" + model + \".gguf\"\n",
    "ctx_size = '8192'\n",
    "ngl = '99'\n",
    "parallel = '2'\n",
    "\n",
    "stop_event = threading.Event()\n",
    "server = threading.Thread(target=lambda: subprocess.run([\n",
    "    'llama.cpp/bin/llama-server',\n",
    "    '-m', model_path,\n",
    "    '--ctx-size', ctx_size,\n",
    "    '--n-gpu-layers', ngl,\n",
    "    '--parallel', parallel,\n",
    "    '--flash-attn'], check=True), daemon=True)\n",
    "\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkill -9 llama-server\n",
    "subprocess.run(['pkill', '-9', 'llama-server'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
