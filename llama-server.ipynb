{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env LD_LIBRARY_PATH=llama.cpp/ggml/src:llama.cpp/src\n",
    "import threading\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkill -9 llama-server\n",
    "subprocess.run(['pkill', '-9', 'llama-server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Llama-3.2-1B-Instruct\"\n",
    "# model = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "# model = \"Qwen2.5-1.5B-Instruct\"\n",
    "# model = \"gemma-2-2b-it\"\n",
    "# model = \"Phi-3.5-mini-instruct\"\n",
    "# model = \"Phi-3-medium-128k-instruct\"\n",
    "# model = \"Mistral-Nemo-Instruct-2407\"\n",
    "# model = \"Mistral-Small-Instruct-2409\"\n",
    "# size = \"IQ4_XS\"\n",
    "# size = \"Q5_K_M\"\n",
    "size = \"Q8_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMLU-Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-server -m models/gemma2/gemma-2-2b-it-Q4_K_S.gguf --ctx-size 6144 --n-gpu-layers 99 --flash-attn --parallel 2\n",
    "ctx_size = '6144' # 2*3072\n",
    "parallel = '2'\n",
    "ngl = '99'\n",
    "\n",
    "server = threading.Thread(target=lambda: subprocess.run([\n",
    "    'llama.cpp/bin/llama-server',\n",
    "    '-m', \"models/\" + model + \"-\" + size + \".gguf\",\n",
    "    '--ctx-size', ctx_size,\n",
    "    '--n-gpu-layers', ngl,\n",
    "    '--parallel', parallel,\n",
    "    '--flash-attn'], check=True), daemon=True)\n",
    "\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_openai.py --url http://localhost:8080/v1 --model gguf --dataset sam-paech/mmlu-pro-nomath-sml --parallel 2 --style multi_chat_zeroshot\n",
    "style = 'multi_chat_zeroshot'\n",
    "mmlu_pro_nomath = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai.py',\n",
    "    '--url', 'http://localhost:8080/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'sam-paech/mmlu-pro-nomath-sml',\n",
    "    '--parallel', parallel,\n",
    "    '--style', style], check=True), daemon=True)\n",
    "\n",
    "mmlu_pro_nomath.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_openai.py --url http://localhost:8080/v1 --model gguf --dataset TIGER-Lab/MMLU-Pro --category 'computer science' --parallel 2 --style multi_chat_zeroshot\n",
    "# python run_openai.py --url http://localhost:8080/v1 --model gguf --dataset TIGER-Lab/MMLU-Pro --category 'engineering' --parallel 2 --style multi_chat_zeroshot\n",
    "style = 'multi_chat_zeroshot'\n",
    "category = \"computer science\"\n",
    "# category = \"engineering\"\n",
    "mmlu_pro_category = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai.py',\n",
    "    '--url', 'http://localhost:8080/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'TIGER-Lab/MMLU-Pro',\n",
    "    '--parallel', parallel,\n",
    "    '--category', category,\n",
    "    '--style', style], check=True), daemon=True)\n",
    "\n",
    "mmlu_pro_category.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openai/MMMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_size = '8700' # 3*2900\n",
    "parallel = '3'\n",
    "ngl = '99'\n",
    "\n",
    "server = threading.Thread(target=lambda: subprocess.run([\n",
    "    'llama.cpp/bin/llama-server',\n",
    "    '-m', \"models/\" + model + \"-\" + size + \".gguf\",\n",
    "    '--ctx-size', ctx_size,\n",
    "    '--n-gpu-layers', ngl,\n",
    "    '--parallel', parallel,\n",
    "    '--flash-attn'], check=True), daemon=True)\n",
    "\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_openai-mmmlu.py --url http://localhost:8080/v1 --model gguf --dataset openai/MMMLU --category ES_LA\n",
    "mmmlu_es = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai-mmmlu.py',\n",
    "    '--url', 'http://localhost:8080/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'openai/MMMLU',\n",
    "    '--category', 'ES_LA'], check=True), daemon=True)\n",
    "\n",
    "mmmlu_es.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_openai-mmmlu.py --url http://localhost:8080/v1 --model gguf --dataset openai/MMMLU --category FR_FR\n",
    "mmmlu_fr = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai-mmmlu.py',\n",
    "    '--url', 'http://localhost:8080/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'openai/MMMLU',\n",
    "    '--category', 'FR_FR'], check=True), daemon=True)\n",
    "\n",
    "mmmlu_fr.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_openai-mmmlu.py --url http://localhost:8080/v1 --model gguf --dataset openai/MMMLU --category ZH_CN\n",
    "mmmlu_zh = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai-mmmlu.py',\n",
    "    '--url', 'http://localhost:8080/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'openai/MMMLU',\n",
    "    '--category', 'ZH_CN'], check=True), daemon=True)\n",
    "\n",
    "mmmlu_zh.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ATTENTION: NOT include in our customized benchmark for saving time\n",
    "# python run_openai-mmmlu.py --url http://localhost:8080/v1 --model gguf --dataset openai/MMMLU --category DE_DE\n",
    "mmmlu_de = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai-mmmlu.py',\n",
    "    '--url', 'http://localhost:8080/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'openai/MMMLU',\n",
    "    '--category', 'DE_DE'], check=True), daemon=True)\n",
    "\n",
    "mmmlu_de.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ATTENTION: NOT include in our customized benchmark for saving time\n",
    "# python run_openai-mmmlu.py --url http://localhost:8080/v1 --model gguf --dataset openai/MMMLU --category PT_BR\n",
    "mmmlu_pt = threading.Thread(target=lambda: subprocess.run([\n",
    "    'python', 'run_openai-mmmlu.py',\n",
    "    '--url', 'http://localhost:8080/v1',\n",
    "    '--model', model + \"-\" + size,\n",
    "    '--dataset', 'openai/MMMLU',\n",
    "    '--category', 'PT_BR'], check=True), daemon=True)\n",
    "\n",
    "mmmlu_pt.start()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
