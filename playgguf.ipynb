{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGkzELPkr_HI"
   },
   "outputs": [],
   "source": [
    "# local use the latest by pip install .\n",
    "# the indent of local jupyter and remote colab is different\n",
    "!pip install gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oMLA8TDsKAo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gguf\n",
    "from gguf import GGUFReader\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from safetensors import safe_open\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax1xdre7Uyiy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dequant\n",
    "\n",
    "<details>\n",
    "<summary>permute and inverse_permute demo</summary>\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py#L222\n",
    "\n",
    "```python\n",
    "def permute(w, n_heads, dim1, dim2):\n",
    "    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n",
    "\n",
    "def inverse_permute(w, n_heads, dim1, dim2):\n",
    "    w = w.view(n_heads, 2, dim1 // n_heads // 2, dim2)\n",
    "    w = w.transpose(1, 2)\n",
    "    w = w.reshape(dim1, dim2)\n",
    "    return w\n",
    "\n",
    "n_heads = 2\n",
    "dim1 = 12\n",
    "dim2 = 12\n",
    "w= torch.arange(dim1 * dim2).view(dim1, dim2)\n",
    "permuted_w = permute(w, n_heads, dim1, dim2)\n",
    "print(w)\n",
    "print(permuted_w)\n",
    "w = inverse_permute(permuted_w, n_heads, dim1, dim2)\n",
    "print(w)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awq and gptq share the same pytorch name \n",
    "name_map = {\n",
    "    'attn_q': 'self_attn.q',\n",
    "    'attn_k': 'self_attn.k',\n",
    "    'attn_v': 'self_attn.v',\n",
    "    'attn_output': 'self_attn.o',\n",
    "    'ffn_down': 'mlp.down',\n",
    "    'ffn_gate': 'mlp.gate',\n",
    "    'ffn_up': 'mlp.up',\n",
    "    'attn_norm': 'input_layernorm',\n",
    "    'ffn_norm': 'post_attention_layernorm'\n",
    "}\n",
    "\n",
    "def pt_get_tensor(reader, prefix, name):\n",
    "    if name == 'token_embd.weight':\n",
    "        return reader.get_tensor('model.embed_tokens.weight')\n",
    "    for k in ['attn_norm', 'ffn_norm']:\n",
    "        if k in name:\n",
    "            return reader.get_tensor(prefix+name_map[k]+'.weight')\n",
    "    return None\n",
    "\n",
    "# permute for sliced rotary\n",
    "def permute(w, n_heads, dim1, dim2):\n",
    "    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n",
    "\n",
    "# inverse permute for sliced rotary\n",
    "def inverse_permute(name, w):\n",
    "    if 'attn_q' in name:\n",
    "        dim3 = n_heads\n",
    "    elif 'attn_k' in name:\n",
    "        dim3 = n_kv_heads\n",
    "    else:\n",
    "        return w\n",
    "        \n",
    "    dim1, dim2 = w.shape\n",
    "    return w.view(dim3, 2, dim1 // dim3 // 2, dim2).transpose(1, 2).reshape(dim1, dim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mit-han-lab/llm-awq/blob/main/awq/quantize/quantizer.py\n",
    "\n",
    "<details>\n",
    "<summary>awq quant logic</summary>\n",
    "\n",
    "[Question about the zero point](https://github.com/mit-han-lab/llm-awq/issues/116)\n",
    "\n",
    "I noticed that only negative minimum values are preserved as zero points with the code.\n",
    "\n",
    "```python\n",
    "    if zero_point:\n",
    "        max_val = w.amax(dim=1, keepdim=True)\n",
    "        min_val = w.amin(dim=1, keepdim=True)\n",
    "        max_int = 2**n_bit - 1\n",
    "        min_int = 0\n",
    "        scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "        zeros = (-torch.round(min_val / scales)).clamp_(min_int, max_int)\n",
    "```\n",
    "\n",
    "Then, why not preserve all the minimum values including the positive values?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOTBjsiAUKO2"
   },
   "outputs": [],
   "source": [
    "# https://medium.com/@crclq2018/awq-how-its-code-works-1ea92fb80bd2\n",
    "unpack_order = torch.tensor([[0, 4, 1, 5, 2, 6, 3, 7]], dtype=torch.int32)\n",
    "bit_shift = unpack_order*4\n",
    "bit_mask = torch.tensor([[0xf]]) << bit_shift\n",
    "# tmp = torch.tensor([[0x86427531]]).repeat(1, 8)\n",
    "# (tmp & bit_mask)>>bit_shift\n",
    "\n",
    "def awq_load_tensor(reader, name):\n",
    "    layer = name.split('.')[1] # f'blk.{layer}.xxx'\n",
    "    prefix = f'model.layers.{layer}.'\n",
    "    \n",
    "    tensor = pt_get_tensor(reader, prefix, name)\n",
    "    if tensor is not None:\n",
    "        return tensor\n",
    "    \n",
    "    for k in name_map:\n",
    "        if k in name:\n",
    "            pt_name = prefix+name_map[k]+ '_proj.'\n",
    "            break\n",
    "\n",
    "    weight = reader.get_tensor(pt_name+'qweight')\n",
    "    scales = reader.get_tensor(pt_name+'scales')\n",
    "    zeros  = reader.get_tensor(pt_name+'qzeros')\n",
    "\n",
    "    n_row, n_col = weight.shape\n",
    "    bmask = bit_mask.repeat(1, n_col).repeat(n_row, 1)\n",
    "    bshift = bit_shift.repeat(1, n_col).repeat(n_row, 1)\n",
    "    # dequantize\n",
    "    weight = torch.repeat_interleave(weight, repeats=8, dim=1)\n",
    "    scales = torch.repeat_interleave(scales, repeats=128, dim=0)\n",
    "    zeros  = torch.repeat_interleave(zeros, repeats=8, dim=1)\n",
    "    zeros  = torch.repeat_interleave(zeros, repeats=128, dim=0)\n",
    "    tensor = scales * (((weight & bmask) >> bshift) - ((zeros & bmask) >> bshift))\n",
    "\n",
    "    return inverse_permute(name, tensor.float().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/qwopqwop200/GPTQ-for-LLaMa/blob/triton/gptq.py\n",
    "\n",
    "https://github.com/qwopqwop200/GPTQ-for-LLaMa/blob/triton/quant/quantizer.py\n",
    "\n",
    "<details>\n",
    "<summary>gptq quant logic</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# if actorder:\n",
    "H = torch.tensor([[8,100,100,100], [100,7,100,100], [100,100,5,100], [100,100,100,9]])\n",
    "g_idx = torch.tensor([i//2 for i in range(4)])\n",
    "perm = torch.argsort(torch.diag(H), descending=True)\n",
    "# [3, 0, 1, 2]\n",
    "invperm = torch.argsort(perm)\n",
    "# [1, 2, 3, 0]\n",
    "g_idx = g_idx[invperm]\n",
    "# [0, 1, 1, 0]\n",
    " \n",
    "if self.maxq < 0:\n",
    "    self.scale = xmax\n",
    "    self.zero = xmin\n",
    "else:\n",
    "    self.scale = (xmax - xmin) / self.maxq\n",
    "    if self.sym:\n",
    "        self.zero = torch.full_like(self.scale, (self.maxq + 1) / 2)\n",
    "    else:\n",
    "        self.zero = torch.round(-xmin / self.scale)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/AutoGPTQ/AutoGPTQ/blob/main/auto_gptq/nn_modules/qlinear/qlinear_cuda.py\n",
    "wf = torch.tensor(list(range(0, 32, 4)), dtype=torch.int32).unsqueeze(0)\n",
    "\n",
    "def gptq_load_tensor(reader, name):\n",
    "    layer = name.split('.')[1] # f'blk.{layer}.xxx'\n",
    "    prefix = f'model.layers.{layer}.'\n",
    "\n",
    "    tensor = pt_get_tensor(reader, prefix, name)\n",
    "    if tensor is not None:\n",
    "        return tensor\n",
    "\n",
    "    for k in name_map:\n",
    "        if k in name:\n",
    "            pt_name = prefix+name_map[k]+ '_proj.'\n",
    "            break\n",
    "\n",
    "    qzeros  = reader.get_tensor(pt_name+'qzeros')\n",
    "    qweight = reader.get_tensor(pt_name+'qweight')\n",
    "    g_idx   = reader.get_tensor(pt_name+'g_idx')\n",
    "    scales  = reader.get_tensor(pt_name+'scales')\n",
    "\n",
    "    zeros = torch.bitwise_right_shift(torch.unsqueeze(qzeros, 2).expand(-1, -1, 8), wf.unsqueeze(0)).to(torch.int8)\n",
    "    zeros = torch.bitwise_and(zeros, 0xf)\n",
    "    zeros = zeros + 1\n",
    "    zeros = zeros.reshape(scales.shape)\n",
    "\n",
    "    weight = torch.bitwise_right_shift(torch.unsqueeze(qweight, 1).expand(-1, 8, -1), wf.unsqueeze(-1)).to(torch.int8)\n",
    "    weight = torch.bitwise_and(weight, 0xf)\n",
    "    weight = weight.reshape(weight.shape[0] * weight.shape[1], weight.shape[2])\n",
    "\n",
    "    tensor = scales[g_idx.long()] * (weight - zeros[g_idx.long()])\n",
    "\n",
    "    return inverse_permute(name, tensor.float().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSCnDDg2sdOq",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEkFzlnrsm04"
   },
   "outputs": [],
   "source": [
    "gguf_file = \"models/TinyStories-656K.Q4_K_S.gguf\"\n",
    "huggingface_repo = \"https://huggingface.co/mradermacher/TinyStories-656K-GGUF/resolve/main/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D72VKzJYA5N4"
   },
   "outputs": [],
   "source": [
    "gguf_file = \"models/SmolLM2-135M-Instruct-Q8_0.gguf\"\n",
    "huggingface_repo = \"https://huggingface.co/bartowski/SmolLM2-135M-Instruct-GGUF/resolve/main/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bPAqxyQj_3d"
   },
   "outputs": [],
   "source": [
    "gguf_file = \"models/Llama-3.2-1B-Instruct-IQ4_XS.gguf\"\n",
    "huggingface_repo = \"https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bPAqxyQj_3d"
   },
   "outputs": [],
   "source": [
    "gguf_file = \"models/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "huggingface_repo = \"https://huggingface.co/bartowski/Llama-3.1-8B-Instruct-GGUF/resolve/main/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(gguf_file):\n",
    "    print(\"file not found, download from internet...\")\n",
    "    subprocess.run([\"wget\", \"-O\", gguf_file, huggingface_repo+gguf_file[7:]])\n",
    "\n",
    "gguf_reader = GGUFReader(gguf_file, 'r')\n",
    "\n",
    "metadata = {}\n",
    "for key, field in gguf_reader.fields.items():\n",
    "    metadata[key] = field.parts[field.data[0]][0]\n",
    "\n",
    "vocab_size  = metadata['llama.vocab_size']\n",
    "hidden_size = metadata['llama.embedding_length']\n",
    "n_blocks    = metadata['llama.block_count']\n",
    "n_heads     = metadata['llama.attention.head_count']\n",
    "n_kv_heads  = metadata['llama.attention.head_count_kv']\n",
    "rope_theta  = metadata['llama.rope.freq_base']\n",
    "norm_eps    = metadata['llama.attention.layer_norm_rms_epsilon']\n",
    "n_dims      = metadata['llama.rope.dimension_count']\n",
    "n_tensors   = metadata['GGUF.tensor_count']\n",
    "n_layer     = 0\n",
    "\n",
    "tensor_idx = {}\n",
    "for i in range(n_tensors):\n",
    "    tensor_idx[gguf_reader.get_tensor(i).name] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DqZJ9ulT60W",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AWQ/GPTQ\n",
    "\n",
    "<details>\n",
    "<summary>analysis</summary>\n",
    "\n",
    "```python\n",
    "tensor_names = awq_reader.keys()\n",
    "for name in tensor_names:\n",
    "    tensor = awq_reader.get_tensor(name)\n",
    "    print(f\"name: {name}\")\n",
    "    print(f\"shape: {tensor.shape}\")\n",
    "    print(f\"type: {tensor.dtype}\")\n",
    "    # print(f\"tensor:\\n{tensor}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 讲个鬼故事，gguf和awq/gptq读出来的tensor是互为转置的\n",
    "# 而且gguf的tensor排列是和原版的meta发布的模型权重次序是一致的\n",
    "# awq/gptq则和huggingface中的保持一致，因为awq/gptq也是作为huggingface生态的一部分\n",
    "print(gguf_reader.get_tensor(tensor_idx[f'blk.{0}.ffn_down.weight']).shape)\n",
    "print(awq_reader.get_tensor(f'model.layers.{0}.mlp.down_proj.qweight').shape)\n",
    "print(gptq_reader.get_tensor(f'model.layers.{0}.mlp.down_proj.qweight').shape)\n",
    "\n",
    "print(gguf_load_tensor(gguf_reader, 'output_norm.weight'))\n",
    "print(awq_reader.get_tensor('model.norm.weight'))\n",
    "print(gptq_reader.get_tensor('model.norm.weight'))\n",
    "\n",
    "layer=0\n",
    "print(gguf_load_tensor(gguf_reader, f'blk.{layer}.attn_k.weight').shape)\n",
    "print(awq_load_tensor(awq_reader, f'blk.{layer}.attn_k.weight').shape)\n",
    "print(gptq_load_tensor(gptq_reader, f'blk.{layer}.attn_k.weight').shape)\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Llama-3.2-1B-Instruct-AWQ](https://huggingface.co/AMead10/Llama-3.2-1B-Instruct-AWQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWfldrTQTzqV"
   },
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/AMead10/Llama-3.2-1B-Instruct-AWQ/resolve/main/model.safetensors\"\n",
    "model_path = \"models/Llama-3.2-1B-Instruct-AWQ.safetensors\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"file not found, download from internet...\")\n",
    "    subprocess.run([\"wget\", \"-O\", model_path, url])\n",
    "\n",
    "awq_reader = safe_open(model_path, framework=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Llama-3.2-1B-Instruct-GPTQ](https://huggingface.co/ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://huggingface.co/ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v2.5/resolve/main/model.safetensors\"\n",
    "# model_path = \"models/Llama-3.2-1B-Instruct-GPTQ-g32.safetensors\"\n",
    "url = \"https://huggingface.co/shuyuej/Llama-3.2-1B-Instruct-GPTQ/resolve/main/model.safetensors\"\n",
    "model_path = \"models/Llama-3.2-1B-Instruct-GPTQ-g128.safetensors\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"file not found, download from internet...\")\n",
    "    subprocess.run([\"wget\", \"-O\", model_path, url])\n",
    "\n",
    "gptq_reader = safe_open(model_path, framework=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 10\n",
    "print(gguf_reader.get_tensor(tensor_idx['token_embd.weight']).data[t])\n",
    "print(awq_reader.get_tensor('model.embed_tokens.weight')[t])\n",
    "print(awq_reader.get_tensor('lm_head.weight')[t])\n",
    "print(gptq_reader.get_tensor('model.embed_tokens.weight')[t])\n",
    "# print(gptq_reader.get_tensor('lm_head.weight')[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Running demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env http_proxy=http://192.168.3.36:7890\n",
    "%env https_proxy=http://192.168.3.36:7890\n",
    "# !pip install -U transformers peft accelerate optimum auto-gptq autoawq\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# question = \"火影忍者的作者是谁？\" # Q5_K_S/Q4_K_S/IQ4_XS/Q3_K_XL will be failed\n",
    "# question = \"Naruto的作者是谁？\" # Q5/IQ4_XS/Q3_K_XL will be failed\n",
    "question = \"Who is the author of 'Chainsaw Man'?\" # Q4_0/IQ3_M/Q3_K_XL will be failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"AMead10/Llama-3.2-1B-Instruct-AWQ\"\n",
    "# model_path = \"Almheiri/Llama-3.2-1B-Instruct-GPTQ-INT4\"\n",
    "model_path = \"ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v2.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=\"auto\", device_map=\"cuda\")\n",
    "\n",
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"\\n\\nYou are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "input_tensor = tokenizer.apply_chat_template(prompt, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids=input_tensor.to(model.device), max_new_tokens=512, do_sample=False)\n",
    "# result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Write a new GGUF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import Any, Sequence, NamedTuple\n",
    "from gguf import GGUFWriter\n",
    "from gguf.constants import GGMLQuantizationType\n",
    "\n",
    "class MetadataDetails(NamedTuple):\n",
    "    type: gguf.GGUFValueType\n",
    "    value: Any\n",
    "    description: str = ''\n",
    "\n",
    "def get_field_data(reader: gguf.GGUFReader, key: str) -> Any:\n",
    "    field = reader.get_field(key)\n",
    "    # seems that remote colab cannot work: AttributeError: 'ReaderField' object has no attribute 'contents'\n",
    "    return field.contents() if field else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gguf = \"Llama-3.2-1B-Instruct-awq.gguf\"\n",
    "reader, load_tensor = awq_reader, awq_load_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gguf = \"Llama-3.2-1B-Instruct-gptq-g128.gguf\"\n",
    "# output_gguf = \"Llama-3.2-1B-Instruct-gptq-g32.gguf\"\n",
    "# output_gguf = \"Llama-3.2-1B-Instruct-gptq.gguf\"\n",
    "reader, load_tensor = gptq_reader, gptq_load_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = get_field_data(gguf_reader, gguf.Keys.General.ARCHITECTURE)\n",
    "writer = gguf.GGUFWriter(output_gguf, arch=arch, endianess=gguf_reader.endianess)\n",
    "alignment = get_field_data(gguf_reader, gguf.Keys.General.ALIGNMENT)\n",
    "assert alignment is None\n",
    "\n",
    "for field in gguf_reader.fields.values():\n",
    "    # Suppress virtual fields and fields written by GGUFWriter\n",
    "    if field.name == gguf.Keys.General.ARCHITECTURE or field.name.startswith('GGUF.'):\n",
    "        continue\n",
    "    val = MetadataDetails(field.types[0], field.contents())\n",
    "    if val.value is not None:\n",
    "        writer.add_key_value(field.name, val.value, val.type)\n",
    "\n",
    "total_bytes = 0\n",
    "\n",
    "for tensor in gguf_reader.tensors:\n",
    "    if 'blk' in tensor.name[:3] and '_norm' not in tensor.name:\n",
    "        data = load_tensor(reader, tensor.name)\n",
    "        dim1, dim2 = data.shape\n",
    "        nbytes = dim1 * dim2 * 2\n",
    "        writer.add_tensor_info(tensor.name, (dim1, dim2), np.float16(1.0).dtype, nbytes, GGMLQuantizationType.F16)\n",
    "        total_bytes += nbytes\n",
    "    else:\n",
    "        total_bytes += tensor.n_bytes\n",
    "        writer.add_tensor_info(tensor.name, tensor.data.shape, tensor.data.dtype, tensor.data.nbytes, tensor.tensor_type)\n",
    "\n",
    "bar = tqdm(desc=\"Writing\", total=total_bytes, unit=\"byte\", unit_scale=True)\n",
    "writer.write_header_to_file()\n",
    "writer.write_kv_data_to_file()\n",
    "writer.write_ti_data_to_file()\n",
    "\n",
    "for tensor in gguf_reader.tensors:\n",
    "    if 'blk' in tensor.name[:3]:\n",
    "        data = load_tensor(reader, tensor.name)\n",
    "        data_type = torch.float32 if 'attn_norm' in tensor.name or 'ffn_norm' in tensor.name else torch.float16\n",
    "        writer.write_tensor_data(data.to(data_type).numpy())\n",
    "    else:\n",
    "        writer.write_tensor_data(tensor.data)\n",
    "    bar.update(tensor.n_bytes)\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO9YNwR9z+FmRFrsyE75WMB",
   "collapsed_sections": [
    "NSCnDDg2sdOq",
    "1K9jQcbDAtvR",
    "rr8GpObAuSeT",
    "vX5Y7_OBvNrQ",
    "NWrffNMRvmwI",
    "1Z35x9C-0w5s"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
